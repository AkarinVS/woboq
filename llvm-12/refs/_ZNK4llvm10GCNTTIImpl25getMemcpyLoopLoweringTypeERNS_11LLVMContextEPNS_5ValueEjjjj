<dec f='llvm/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.h' l='140' type='llvm::Type * llvm::GCNTTIImpl::getMemcpyLoopLoweringType(llvm::LLVMContext &amp; Context, llvm::Value * Length, unsigned int SrcAddrSpace, unsigned int DestAddrSpace, unsigned int SrcAlign, unsigned int DestAlign) const'/>
<def f='llvm/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp' l='395' ll='421' type='llvm::Type * llvm::GCNTTIImpl::getMemcpyLoopLoweringType(llvm::LLVMContext &amp; Context, llvm::Value * Length, unsigned int SrcAddrSpace, unsigned int DestAddrSpace, unsigned int SrcAlign, unsigned int DestAlign) const'/>
<doc f='llvm/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp' l='388'>// FIXME: Really we would like to issue multiple 128-bit loads and stores per
// iteration. Should we report a larger size and let it legalize?
//
// FIXME: Should we use narrower types for local/region, or account for when
// unaligned access is legal?
//
// FIXME: This could use fine tuning and microbenchmarks.</doc>
