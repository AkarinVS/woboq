<def f='llvm/llvm/include/llvm/Analysis/InlineAdvisor.h' l='39' ll='43'/>
<use f='llvm/llvm/include/llvm/Analysis/InlineAdvisor.h' l='231' c='_ZN4llvm21InlineAdvisorAnalysis6Result9tryCreateENS_12InlineParamsENS_19InliningAdvisorModeENS_9StringRefE'/>
<use f='llvm/llvm/include/llvm/Transforms/IPO/Inliner.h' l='124' c='_ZN4llvm24ModuleInlinerWrapperPassC1ENS_12InlineParamsEbbNS_19InliningAdvisorModeEj'/>
<use f='llvm/llvm/include/llvm/Transforms/IPO/Inliner.h' l='141'/>
<doc f='llvm/llvm/include/llvm/Analysis/InlineAdvisor.h' l='26'>/// There are 3 scenarios we can use the InlineAdvisor:
/// - Default - use manual heuristics.
///
/// - Release mode, the expected mode for production, day to day deployments.
/// In this mode, when building the compiler, we also compile a pre-trained ML
/// model to native code, and link it as a static library. This mode has low
/// overhead and no additional dependencies for the compiler runtime.
///
/// - Development mode, for training new models.
/// In this mode, we trade off runtime performance for flexibility. This mode
/// requires the full C Tensorflow API library, and evaluates models
/// dynamically. This mode also permits generating training logs, for offline
/// training.</doc>
<use f='llvm/llvm/lib/Analysis/InlineAdvisor.cpp' l='157' c='_ZN4llvm21InlineAdvisorAnalysis6Result9tryCreateENS_12InlineParamsENS_19InliningAdvisorModeENS_9StringRefE'/>
<use f='llvm/llvm/lib/Passes/PassBuilder.cpp' l='244'/>
<use f='llvm/llvm/lib/Transforms/IPO/Inliner.cpp' l='998' c='_ZN4llvm24ModuleInlinerWrapperPassC1ENS_12InlineParamsEbbNS_19InliningAdvisorModeEj'/>
