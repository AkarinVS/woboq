<dec f='llvm/llvm/tools/llvm-mca/Views/SchedulerStatistics.h' l='76' type='void llvm::mca::SchedulerStatistics::onEvent(const llvm::mca::HWInstructionEvent &amp; Event)'/>
<inh f='llvm/llvm/include/llvm/MCA/HWEventListener.h' l='164' c='_ZN4llvm3mca15HWEventListener7onEventERKNS0_18HWInstructionEventE'/>
<def f='llvm/llvm/tools/llvm-mca/Views/SchedulerStatistics.cpp' l='44' ll='72' type='void llvm::mca::SchedulerStatistics::onEvent(const llvm::mca::HWInstructionEvent &amp; Event)'/>
<doc f='llvm/llvm/tools/llvm-mca/Views/SchedulerStatistics.cpp' l='33'>// FIXME: This implementation works under the assumption that load/store queue
// entries are reserved at &apos;instruction dispatched&apos; stage, and released at
// &apos;instruction executed&apos; stage. This currently matches the behavior of LSUnit.
//
// The current design minimizes the number of events generated by the
// Dispatch/Execute stages, at the cost of doing extra bookkeeping in method
// `onEvent`. However, it introduces a subtle dependency between this view and
// how the LSUnit works.
//
// In future we should add a new &quot;memory queue&quot; event type, so that we stop
// making assumptions on how LSUnit internally works (See PR39828).</doc>
