<!doctype html>
<html>
<head>
<meta name="viewport" content="width=device-width, initial-scale=1.0"><title>DevelopmentModeInlineAdvisor.cpp source code [llvm/llvm/lib/Analysis/DevelopmentModeInlineAdvisor.cpp] - Woboq Code Browser</title>
<link rel="stylesheet" href="../../../.././data/qtcreator.css" title="QtCreator"/>
<link rel="alternate stylesheet" href="../../../.././data/kdevelop.css" title="KDevelop"/>
<script type="text/javascript" src="../../../.././data/jquery/jquery.min.js"></script>
<script type="text/javascript" src="../../../.././data/jquery/jquery-ui.min.js"></script>
<script>var file = 'llvm/llvm/lib/Analysis/DevelopmentModeInlineAdvisor.cpp'; var root_path = '../../../..'; var data_path = '../../../.././data'; var ecma_script_api_version = 2;</script>
<script src='../../../.././data/codebrowser.js'></script>
</head>
<body><div id='header'><h1 id='breadcrumb'><span>Browse the source code of </span><a href='../../..'>llvm</a>/<a href='../..'>llvm</a>/<a href='..'>lib</a>/<a href='./'>Analysis</a>/<a href='DevelopmentModeInlineAdvisor.cpp.html'>DevelopmentModeInlineAdvisor.cpp</a></h1></div>
<hr/><div id='content'><table class="code">
<tr><th id="1">1</th><td><i>//===- DevelopmentModeInlineAdvisor.cpp - runtime-loadable model runner  --===//</i></td></tr>
<tr><th id="2">2</th><td><i>//</i></td></tr>
<tr><th id="3">3</th><td><i>//                     The LLVM Compiler Infrastructure</i></td></tr>
<tr><th id="4">4</th><td><i>//</i></td></tr>
<tr><th id="5">5</th><td><i>// This file is distributed under the University of Illinois Open Source</i></td></tr>
<tr><th id="6">6</th><td><i>// License. See LICENSE.TXT for details.</i></td></tr>
<tr><th id="7">7</th><td><i>//</i></td></tr>
<tr><th id="8">8</th><td><i>//===----------------------------------------------------------------------===//</i></td></tr>
<tr><th id="9">9</th><td><i>//</i></td></tr>
<tr><th id="10">10</th><td><i>// This file implements a model runner using Tensorflow C APIs, allowing the</i></td></tr>
<tr><th id="11">11</th><td><i>// loading of a model from a command line option.</i></td></tr>
<tr><th id="12">12</th><td><i>//</i></td></tr>
<tr><th id="13">13</th><td><i>//===----------------------------------------------------------------------===//</i></td></tr>
<tr><th id="14">14</th><td><u>#include <a href="../../../build/include/llvm/Config/config.h.html">"llvm/Config/config.h"</a></u></td></tr>
<tr><th id="15">15</th><td><u>#<span data-ppcond="15">if</span> defined(<span class="macro" data-ref="_M/LLVM_HAVE_TF_API">LLVM_HAVE_TF_API</span>)</u></td></tr>
<tr><th id="16">16</th><td></td></tr>
<tr><th id="17">17</th><td><u>#include "llvm/Analysis/CallGraph.h"</u></td></tr>
<tr><th id="18">18</th><td><u>#include "llvm/Analysis/InlineSizeEstimatorAnalysis.h"</u></td></tr>
<tr><th id="19">19</th><td><u>#include "llvm/Analysis/MLInlineAdvisor.h"</u></td></tr>
<tr><th id="20">20</th><td><u>#include "llvm/Analysis/Utils/TFUtils.h"</u></td></tr>
<tr><th id="21">21</th><td><u>#include "llvm/IR/LLVMContext.h"</u></td></tr>
<tr><th id="22">22</th><td><u>#include "llvm/Support/CommandLine.h"</u></td></tr>
<tr><th id="23">23</th><td><u>#include "llvm/Support/ManagedStatic.h"</u></td></tr>
<tr><th id="24">24</th><td></td></tr>
<tr><th id="25">25</th><td><u>#include &lt;vector&gt;</u></td></tr>
<tr><th id="26">26</th><td></td></tr>
<tr><th id="27">27</th><td><b>using</b> <b>namespace</b> llvm;</td></tr>
<tr><th id="28">28</th><td></td></tr>
<tr><th id="29">29</th><td><em>static</em> cl::opt&lt;std::string&gt; TrainingLog(</td></tr>
<tr><th id="30">30</th><td>    <q>"training-log"</q>, cl::Hidden,</td></tr>
<tr><th id="31">31</th><td>    cl::desc(<q>"Path where the development - mode inlining log is saved."</q>));</td></tr>
<tr><th id="32">32</th><td></td></tr>
<tr><th id="33">33</th><td><em>static</em> cl::opt&lt;std::string&gt; TFModelUnderTrainingPath(</td></tr>
<tr><th id="34">34</th><td>    <q>"ml-inliner-model-under-training"</q>, cl::Hidden,</td></tr>
<tr><th id="35">35</th><td>    cl::desc(<q>R"(Path to SavedModel from the previous training iteration.</q></td></tr>
<tr><th id="36">36</th><td><q>The directory is also expected to contain a JSON specification of the </q></td></tr>
<tr><th id="37">37</th><td><q>outputs expected to be logged, where the first entry must be the </q></td></tr>
<tr><th id="38">38</th><td><q>inlining decision. The file containing the specification should be </q></td></tr>
<tr><th id="39">39</th><td><q>called output_spec.json. The expected JSON value is an array of </q></td></tr>
<tr><th id="40">40</th><td><q>dictionaries. Each dictionary should have 2 keys: </q></td></tr>
<tr><th id="41">41</th><td><q></q></td></tr>
<tr><th id="42">42</th><td><q>- "tensor_spec, followed by the TensorSpec description of the</q></td></tr>
<tr><th id="43">43</th><td><q>output; and </q></td></tr>
<tr><th id="44">44</th><td><q>- "logging_name", a string indicating the name to use when</q></td></tr>
<tr><th id="45">45</th><td><q>logging the output values. </q></td></tr>
<tr><th id="46">46</th><td><q></q></td></tr>
<tr><th id="47">47</th><td><q>Example:</q></td></tr>
<tr><th id="48">48</th><td><q>[</q></td></tr>
<tr><th id="49">49</th><td><q>  {</q></td></tr>
<tr><th id="50">50</th><td><q>    "logging_name" : "some_name", </q></td></tr>
<tr><th id="51">51</th><td><q>    "tensor_spec" : { </q></td></tr>
<tr><th id="52">52</th><td><q>      "name" : "model_name", </q></td></tr>
<tr><th id="53">53</th><td><q>      "port" : 0,</q></td></tr>
<tr><th id="54">54</th><td><q>      "shape" : [2, 3],</q></td></tr>
<tr><th id="55">55</th><td><q>      "type" : "float"</q></td></tr>
<tr><th id="56">56</th><td><q>      }</q></td></tr>
<tr><th id="57">57</th><td><q>  }</q></td></tr>
<tr><th id="58">58</th><td><q>]</q></td></tr>
<tr><th id="59">59</th><td><q></q></td></tr>
<tr><th id="60">60</th><td><q>The first value must always correspond to the decision.)"</q>));</td></tr>
<tr><th id="61">61</th><td></td></tr>
<tr><th id="62">62</th><td><em>static</em> cl::opt&lt;std::string&gt; TFOutputSpecOverride(</td></tr>
<tr><th id="63">63</th><td>    <q>"ml-inliner-output-spec-override"</q>, cl::Hidden,</td></tr>
<tr><th id="64">64</th><td>    cl::desc(<q>"Override the path to the output spec json file. See "</q></td></tr>
<tr><th id="65">65</th><td>             <q>"-ml-inliner-model-under-training documentation for the "</q></td></tr>
<tr><th id="66">66</th><td>             <q>"specification of that file."</q>));</td></tr>
<tr><th id="67">67</th><td></td></tr>
<tr><th id="68">68</th><td><em>static</em> cl::opt&lt;std::string&gt; TFFeedPrefix(<q>"ml-inliner-trained-model-feed-prefix"</q>,</td></tr>
<tr><th id="69">69</th><td>                                         cl::Hidden, cl::init(<q>"action_"</q>),</td></tr>
<tr><th id="70">70</th><td>                                         cl::desc(<q>"Prefix for feature names."</q>));</td></tr>
<tr><th id="71">71</th><td></td></tr>
<tr><th id="72">72</th><td><b>namespace</b> {</td></tr>
<tr><th id="73">73</th><td><i class="doc">/// An InlineEvent, used by TrainingLogger.</i></td></tr>
<tr><th id="74">74</th><td><b>struct</b> InlineEvent {</td></tr>
<tr><th id="75">75</th><td>  <i class="doc">/// What the default policy's decision would have been.</i></td></tr>
<tr><th id="76">76</th><td>  int64_t DefaultDecision = <var>0</var>;</td></tr>
<tr><th id="77">77</th><td></td></tr>
<tr><th id="78">78</th><td>  <i class="doc">/// What we advised. When training off the default policy, this is the same as</i></td></tr>
<tr><th id="79">79</th><td><i class="doc">  /// DefaultDecision.</i></td></tr>
<tr><th id="80">80</th><td>  int64_t AdvisedDecision = <var>0</var>;</td></tr>
<tr><th id="81">81</th><td></td></tr>
<tr><th id="82">82</th><td>  <i class="doc">/// What actually happened. This would be 'false' in the case of an inline</i></td></tr>
<tr><th id="83">83</th><td><i class="doc">  /// error, even if AdvisedDecision were true, otherwise it agrees with</i></td></tr>
<tr><th id="84">84</th><td><i class="doc">  /// AdvisedDecision.</i></td></tr>
<tr><th id="85">85</th><td>  <em>bool</em> Effect = <b>false</b>;</td></tr>
<tr><th id="86">86</th><td></td></tr>
<tr><th id="87">87</th><td>  <i class="doc">/// What the change in size was: size_after - size_before</i></td></tr>
<tr><th id="88">88</th><td>  int64_t Reward = <var>0</var>;</td></tr>
<tr><th id="89">89</th><td>};</td></tr>
<tr><th id="90">90</th><td></td></tr>
<tr><th id="91">91</th><td><i class="doc">/// Collect data we may use for training a model, and write it as a textual</i></td></tr>
<tr><th id="92">92</th><td><i class="doc">/// Tensorflow SequenceExample</i></td></tr>
<tr><th id="93">93</th><td><i class="doc">/// (<a href="https://www.tensorflow.org/api_docs/python/tf/train/SequenceExample">https://www.tensorflow.org/api_docs/python/tf/train/SequenceExample</a>)</i></td></tr>
<tr><th id="94">94</th><td><i class="doc">/// protobuf (<a href="https://developers.google.com/protocol-buffers">https://developers.google.com/protocol-buffers</a>).</i></td></tr>
<tr><th id="95">95</th><td><i class="doc">/// Because this is a protobuf, we cannot just stream the events as they come.</i></td></tr>
<tr><th id="96">96</th><td><i class="doc">/// Internally, TrainingLogger stores data in column-major format, because that</i></td></tr>
<tr><th id="97">97</th><td><i class="doc">/// lines up with how TF SequenceExample represents it.</i></td></tr>
<tr><th id="98">98</th><td><b>class</b> ModelUnderTrainingRunner;</td></tr>
<tr><th id="99">99</th><td><b>class</b> TrainingLogger final {</td></tr>
<tr><th id="100">100</th><td><b>public</b>:</td></tr>
<tr><th id="101">101</th><td>  TrainingLogger(StringRef LogFileName, <em>const</em> ModelUnderTrainingRunner *MUTR);</td></tr>
<tr><th id="102">102</th><td></td></tr>
<tr><th id="103">103</th><td>  <i class="doc">/// Log one inlining event.</i></td></tr>
<tr><th id="104">104</th><td>  <em>void</em> logInlineEvent(<em>const</em> InlineEvent &amp;Event,</td></tr>
<tr><th id="105">105</th><td>                      <em>const</em> MLModelRunner &amp;ModelRunner);</td></tr>
<tr><th id="106">106</th><td></td></tr>
<tr><th id="107">107</th><td>  <i class="doc">/// Print the stored tensors.</i></td></tr>
<tr><th id="108">108</th><td>  <em>void</em> print();</td></tr>
<tr><th id="109">109</th><td></td></tr>
<tr><th id="110">110</th><td><b>private</b>:</td></tr>
<tr><th id="111">111</th><td>  StringRef LogFileName;</td></tr>
<tr><th id="112">112</th><td>  <em>const</em> ModelUnderTrainingRunner *<em>const</em> MUTR;</td></tr>
<tr><th id="113">113</th><td>  std::unique_ptr&lt;Logger&gt; L;</td></tr>
<tr><th id="114">114</th><td>  std::vector&lt;<em>bool</em>&gt; Effects;</td></tr>
<tr><th id="115">115</th><td>  <i class="doc">/// There's at least one output. We'll set this to a different value if MUTR</i></td></tr>
<tr><th id="116">116</th><td><i class="doc">  /// is avaliable.</i></td></tr>
<tr><th id="117">117</th><td>  size_t OutputCount = <var>1</var>;</td></tr>
<tr><th id="118">118</th><td>  <i class="doc">/// Set these 2 clearly OOB, to make sure we set them later.</i></td></tr>
<tr><th id="119">119</th><td>  size_t DefaultDecisionPos = std::numeric_limits&lt;size_t&gt;::max();</td></tr>
<tr><th id="120">120</th><td>  size_t DecisionPos = std::numeric_limits&lt;size_t&gt;::max();</td></tr>
<tr><th id="121">121</th><td>};</td></tr>
<tr><th id="122">122</th><td></td></tr>
<tr><th id="123">123</th><td><i class="doc">/// An extension of the MLInlineAdvisor for the 'development' mode, targeting</i></td></tr>
<tr><th id="124">124</th><td><i class="doc">/// the offline training scenario. Note that training happens outside of the</i></td></tr>
<tr><th id="125">125</th><td><i class="doc">/// compiler, this facility is concerned with producing training data ("logs").</i></td></tr>
<tr><th id="126">126</th><td><i class="doc">/// This InlineAdvisor can operate in the following modes:</i></td></tr>
<tr><th id="127">127</th><td><i class="doc">///</i></td></tr>
<tr><th id="128">128</th><td><i class="doc">/// 1) collect logs for the default policy. This is useful for bootstrapping</i></td></tr>
<tr><th id="129">129</th><td><i class="doc">/// training, which will be considerably faster by starting from a reasonable</i></td></tr>
<tr><th id="130">130</th><td><i class="doc">/// policy.</i></td></tr>
<tr><th id="131">131</th><td><i class="doc">///</i></td></tr>
<tr><th id="132">132</th><td><i class="doc">/// 2) collect logs for the ML policy, using a model from a previous</i></td></tr>
<tr><th id="133">133</th><td><i class="doc">/// training. Potentially, that model uses internally some small random</i></td></tr>
<tr><th id="134">134</th><td><i class="doc">/// perturbation of its weights, to induce exploration (setting this up is the</i></td></tr>
<tr><th id="135">135</th><td><i class="doc">/// responsibility of the training algorithm). The logs would then be used to</i></td></tr>
<tr><th id="136">136</th><td><i class="doc">/// retrain and improve on this model.</i></td></tr>
<tr><th id="137">137</th><td><i class="doc">///</i></td></tr>
<tr><th id="138">138</th><td><i class="doc">/// 3) use the provided model, with no logging. This is useful for end to end</i></td></tr>
<tr><th id="139">139</th><td><i class="doc">/// validation - the model, in this case, is a release candidate and shouldn't</i></td></tr>
<tr><th id="140">140</th><td><i class="doc">/// have random perturbations. It is a convenience feature: rather than needing</i></td></tr>
<tr><th id="141">141</th><td><i class="doc">/// to take the release candidate model and compile it in 'release' mode,</i></td></tr>
<tr><th id="142">142</th><td><i class="doc">/// validate it, then potentially discard it, it's easier to just pass the model</i></td></tr>
<tr><th id="143">143</th><td><i class="doc">/// to the compiler, albeit compilation would be slower, as a one-off. Once the</i></td></tr>
<tr><th id="144">144</th><td><i class="doc">/// model behaves satisfactorily, it can be compiled AOT, for efficiency, in</i></td></tr>
<tr><th id="145">145</th><td><i class="doc">/// release mode. The expectation is that a well-trained model provides a good</i></td></tr>
<tr><th id="146">146</th><td><i class="doc">/// policy over a sufficiently diverse codebase, over many changes (i.e.</i></td></tr>
<tr><th id="147">147</th><td><i class="doc">/// training happens seldom).</i></td></tr>
<tr><th id="148">148</th><td><b>class</b> DevelopmentModeMLInlineAdvisor : <b>public</b> MLInlineAdvisor {</td></tr>
<tr><th id="149">149</th><td><b>public</b>:</td></tr>
<tr><th id="150">150</th><td>  DevelopmentModeMLInlineAdvisor(</td></tr>
<tr><th id="151">151</th><td>      Module &amp;M, ModuleAnalysisManager &amp;MAM,</td></tr>
<tr><th id="152">152</th><td>      std::unique_ptr&lt;MLModelRunner&gt; ModelRunner,</td></tr>
<tr><th id="153">153</th><td>      std::function&lt;<em>bool</em>(CallBase &amp;)&gt; GetDefaultAdvice, <em>bool</em> IsDoingInference,</td></tr>
<tr><th id="154">154</th><td>      std::unique_ptr&lt;TrainingLogger&gt; Logger);</td></tr>
<tr><th id="155">155</th><td></td></tr>
<tr><th id="156">156</th><td>  size_t getTotalSizeEstimate();</td></tr>
<tr><th id="157">157</th><td></td></tr>
<tr><th id="158">158</th><td>  <b>virtual</b> ~DevelopmentModeMLInlineAdvisor();</td></tr>
<tr><th id="159">159</th><td>  <em>void</em> updateNativeSizeEstimate(int64_t Change) {</td></tr>
<tr><th id="160">160</th><td>    *CurrentNativeSize += Change;</td></tr>
<tr><th id="161">161</th><td>  }</td></tr>
<tr><th id="162">162</th><td>  <em>void</em> resetNativeSize(Function *F) {</td></tr>
<tr><th id="163">163</th><td>    FAM.invalidate&lt;InlineSizeEstimatorAnalysis&gt;(*F);</td></tr>
<tr><th id="164">164</th><td>  }</td></tr>
<tr><th id="165">165</th><td></td></tr>
<tr><th id="166">166</th><td>  std::unique_ptr&lt;MLInlineAdvice&gt;</td></tr>
<tr><th id="167">167</th><td>  getAdviceFromModel(CallBase &amp;CB, OptimizationRemarkEmitter &amp;ORE) override;</td></tr>
<tr><th id="168">168</th><td></td></tr>
<tr><th id="169">169</th><td>  Optional&lt;size_t&gt; getNativeSizeEstimate(<em>const</em> Function &amp;F) <em>const</em>;</td></tr>
<tr><th id="170">170</th><td></td></tr>
<tr><th id="171">171</th><td><b>private</b>:</td></tr>
<tr><th id="172">172</th><td>  <em>bool</em> isLogging() <em>const</em> { <b>return</b> !!Logger; }</td></tr>
<tr><th id="173">173</th><td>  std::unique_ptr&lt;MLInlineAdvice&gt; getMandatoryAdviceImpl(CallBase &amp;CB) override;</td></tr>
<tr><th id="174">174</th><td></td></tr>
<tr><th id="175">175</th><td>  std::function&lt;<em>bool</em>(CallBase &amp;)&gt; GetDefaultAdvice;</td></tr>
<tr><th id="176">176</th><td>  <em>const</em> <em>bool</em> IsDoingInference;</td></tr>
<tr><th id="177">177</th><td>  std::unique_ptr&lt;TrainingLogger&gt; Logger;</td></tr>
<tr><th id="178">178</th><td></td></tr>
<tr><th id="179">179</th><td>  <em>const</em> Optional&lt;int32_t&gt; InitialNativeSize;</td></tr>
<tr><th id="180">180</th><td>  Optional&lt;int32_t&gt; CurrentNativeSize;</td></tr>
<tr><th id="181">181</th><td>};</td></tr>
<tr><th id="182">182</th><td></td></tr>
<tr><th id="183">183</th><td><i class="doc">/// A variant of MLInlineAdvice that tracks all non-trivial inlining</i></td></tr>
<tr><th id="184">184</th><td><i class="doc">/// decisions, for training/logging.</i></td></tr>
<tr><th id="185">185</th><td><b>class</b> LoggingMLInlineAdvice : <b>public</b> MLInlineAdvice {</td></tr>
<tr><th id="186">186</th><td><b>public</b>:</td></tr>
<tr><th id="187">187</th><td>  LoggingMLInlineAdvice(DevelopmentModeMLInlineAdvisor *Advisor, CallBase &amp;CB,</td></tr>
<tr><th id="188">188</th><td>                        OptimizationRemarkEmitter &amp;ORE, <em>bool</em> Recommendation,</td></tr>
<tr><th id="189">189</th><td>                        TrainingLogger &amp;Logger,</td></tr>
<tr><th id="190">190</th><td>                        Optional&lt;size_t&gt; CallerSizeEstimateBefore,</td></tr>
<tr><th id="191">191</th><td>                        Optional&lt;size_t&gt; CalleeSizeEstimateBefore,</td></tr>
<tr><th id="192">192</th><td>                        <em>bool</em> DefaultDecision, <em>bool</em> Mandatory = <b>false</b>)</td></tr>
<tr><th id="193">193</th><td>      : MLInlineAdvice(Advisor, CB, ORE, Recommendation), Logger(Logger),</td></tr>
<tr><th id="194">194</th><td>        CallerSizeEstimateBefore(CallerSizeEstimateBefore),</td></tr>
<tr><th id="195">195</th><td>        CalleeSizeEstimateBefore(CalleeSizeEstimateBefore),</td></tr>
<tr><th id="196">196</th><td>        DefaultDecision(DefaultDecision), Mandatory(Mandatory) {}</td></tr>
<tr><th id="197">197</th><td></td></tr>
<tr><th id="198">198</th><td>  <b>virtual</b> ~LoggingMLInlineAdvice() = <b>default</b>;</td></tr>
<tr><th id="199">199</th><td></td></tr>
<tr><th id="200">200</th><td><b>private</b>:</td></tr>
<tr><th id="201">201</th><td>  DevelopmentModeMLInlineAdvisor *getAdvisor() <em>const</em> {</td></tr>
<tr><th id="202">202</th><td>    <b>return</b> <b>static_cast</b>&lt;DevelopmentModeMLInlineAdvisor *&gt;(Advisor);</td></tr>
<tr><th id="203">203</th><td>  }</td></tr>
<tr><th id="204">204</th><td>  <em>void</em> recordInliningImpl() override {</td></tr>
<tr><th id="205">205</th><td>    MLInlineAdvice::recordInliningImpl();</td></tr>
<tr><th id="206">206</th><td>    getAdvisor()-&gt;resetNativeSize(Caller);</td></tr>
<tr><th id="207">207</th><td>    <em>int</em> Reward = std::numeric_limits&lt;<em>int</em>&gt;::max();</td></tr>
<tr><th id="208">208</th><td>    <b>if</b> (InlineSizeEstimatorAnalysis::isEvaluatorRequested() &amp;&amp;</td></tr>
<tr><th id="209">209</th><td>        !getAdvisor()-&gt;isForcedToStop()) {</td></tr>
<tr><th id="210">210</th><td>      <em>int</em> NativeSizeAfter = *getAdvisor()-&gt;getNativeSizeEstimate(*Caller) +</td></tr>
<tr><th id="211">211</th><td>                            *CalleeSizeEstimateBefore;</td></tr>
<tr><th id="212">212</th><td>      Reward = NativeSizeAfter -</td></tr>
<tr><th id="213">213</th><td>               (*CallerSizeEstimateBefore + *CalleeSizeEstimateBefore);</td></tr>
<tr><th id="214">214</th><td>      getAdvisor()-&gt;updateNativeSizeEstimate(Reward);</td></tr>
<tr><th id="215">215</th><td>    }</td></tr>
<tr><th id="216">216</th><td>    log(Reward, <i>/*Success=*/</i><b>true</b>);</td></tr>
<tr><th id="217">217</th><td>  }</td></tr>
<tr><th id="218">218</th><td></td></tr>
<tr><th id="219">219</th><td>  <em>void</em> recordInliningWithCalleeDeletedImpl() override {</td></tr>
<tr><th id="220">220</th><td>    MLInlineAdvice::recordInliningWithCalleeDeletedImpl();</td></tr>
<tr><th id="221">221</th><td>    getAdvisor()-&gt;resetNativeSize(Caller);</td></tr>
<tr><th id="222">222</th><td>    <b>if</b> (InlineSizeEstimatorAnalysis::isEvaluatorRequested() &amp;&amp;</td></tr>
<tr><th id="223">223</th><td>        !getAdvisor()-&gt;isForcedToStop()) {</td></tr>
<tr><th id="224">224</th><td>      <em>int</em> NativeSizeAfter = *getAdvisor()-&gt;getNativeSizeEstimate(*Caller);</td></tr>
<tr><th id="225">225</th><td>      <em>int</em> Reward = NativeSizeAfter -</td></tr>
<tr><th id="226">226</th><td>                   (*CallerSizeEstimateBefore + *CalleeSizeEstimateBefore);</td></tr>
<tr><th id="227">227</th><td>      getAdvisor()-&gt;updateNativeSizeEstimate(Reward);</td></tr>
<tr><th id="228">228</th><td>      log(Reward, <i>/*Success=*/</i><b>true</b>);</td></tr>
<tr><th id="229">229</th><td>    }</td></tr>
<tr><th id="230">230</th><td>  }</td></tr>
<tr><th id="231">231</th><td></td></tr>
<tr><th id="232">232</th><td>  <em>void</em> recordUnsuccessfulInliningImpl(<em>const</em> InlineResult &amp;Result) override {</td></tr>
<tr><th id="233">233</th><td>    MLInlineAdvice::recordUnsuccessfulInliningImpl(Result);</td></tr>
<tr><th id="234">234</th><td>    log(NoReward, <i>/*Success=*/</i><b>false</b>);</td></tr>
<tr><th id="235">235</th><td>  }</td></tr>
<tr><th id="236">236</th><td></td></tr>
<tr><th id="237">237</th><td>  <em>void</em> recordUnattemptedInliningImpl() override {</td></tr>
<tr><th id="238">238</th><td>    MLInlineAdvice::recordUnattemptedInliningImpl();</td></tr>
<tr><th id="239">239</th><td>    log(NoReward, <i>/*Success=*/</i><b>false</b>);</td></tr>
<tr><th id="240">240</th><td>  }</td></tr>
<tr><th id="241">241</th><td></td></tr>
<tr><th id="242">242</th><td>  <em>void</em> log(int64_t Reward, <em>bool</em> Success) {</td></tr>
<tr><th id="243">243</th><td>    <b>if</b> (Mandatory)</td></tr>
<tr><th id="244">244</th><td>      <b>return</b>;</td></tr>
<tr><th id="245">245</th><td>    InlineEvent Event;</td></tr>
<tr><th id="246">246</th><td>    Event.AdvisedDecision = isInliningRecommended();</td></tr>
<tr><th id="247">247</th><td>    Event.DefaultDecision = DefaultDecision;</td></tr>
<tr><th id="248">248</th><td>    Event.Effect = Success;</td></tr>
<tr><th id="249">249</th><td>    Event.Reward = Reward;</td></tr>
<tr><th id="250">250</th><td>    Logger.logInlineEvent(Event, getAdvisor()-&gt;getModelRunner());</td></tr>
<tr><th id="251">251</th><td>  }</td></tr>
<tr><th id="252">252</th><td></td></tr>
<tr><th id="253">253</th><td>  <em>static</em> <em>const</em> int64_t NoReward = <var>0</var>;</td></tr>
<tr><th id="254">254</th><td>  TrainingLogger &amp;Logger;</td></tr>
<tr><th id="255">255</th><td>  <em>const</em> Optional&lt;size_t&gt; CallerSizeEstimateBefore;</td></tr>
<tr><th id="256">256</th><td>  <em>const</em> Optional&lt;size_t&gt; CalleeSizeEstimateBefore;</td></tr>
<tr><th id="257">257</th><td>  <em>const</em> int64_t DefaultDecision;</td></tr>
<tr><th id="258">258</th><td>  <em>const</em> int64_t Mandatory;</td></tr>
<tr><th id="259">259</th><td>};</td></tr>
<tr><th id="260">260</th><td></td></tr>
<tr><th id="261">261</th><td><i class="doc">/// A pseudo model runner. We use it to store feature values when collecting</i></td></tr>
<tr><th id="262">262</th><td><i class="doc">/// logs for the default policy, but never ask it to 'run'.</i></td></tr>
<tr><th id="263">263</th><td><b>class</b> NoInferenceModelRunner : <b>public</b> MLModelRunner {</td></tr>
<tr><th id="264">264</th><td><b>public</b>:</td></tr>
<tr><th id="265">265</th><td>  NoInferenceModelRunner(LLVMContext &amp;Ctx)</td></tr>
<tr><th id="266">266</th><td>      : MLModelRunner(Ctx), Features(NumberOfFeatures) {}</td></tr>
<tr><th id="267">267</th><td>  <em>void</em> setFeature(FeatureIndex Index, int64_t Value) override {</td></tr>
<tr><th id="268">268</th><td>    Features[<b>static_cast</b>&lt;<em>int</em>&gt;(Index)] = Value;</td></tr>
<tr><th id="269">269</th><td>  }</td></tr>
<tr><th id="270">270</th><td></td></tr>
<tr><th id="271">271</th><td>  int64_t getFeature(<em>int</em> Index) <em>const</em> override { <b>return</b> Features[Index]; }</td></tr>
<tr><th id="272">272</th><td>  <em>bool</em> run() override {</td></tr>
<tr><th id="273">273</th><td>    llvm_unreachable(<q>"We shouldn't call run on this model runner."</q>);</td></tr>
<tr><th id="274">274</th><td>  }</td></tr>
<tr><th id="275">275</th><td></td></tr>
<tr><th id="276">276</th><td><b>private</b>:</td></tr>
<tr><th id="277">277</th><td>  InlineFeatures Features;</td></tr>
<tr><th id="278">278</th><td>};</td></tr>
<tr><th id="279">279</th><td></td></tr>
<tr><th id="280">280</th><td><i class="doc">/// ModelUnderTrainingRunner - training mode implementation. It uses TF C APIs</i></td></tr>
<tr><th id="281">281</th><td><i class="doc">/// to dynamically load and evaluate a TF SavedModel</i></td></tr>
<tr><th id="282">282</th><td><i class="doc">/// (<a href="https://www.tensorflow.org/guide/saved_model">https://www.tensorflow.org/guide/saved_model</a>). Runtime performance is</i></td></tr>
<tr><th id="283">283</th><td><i class="doc">/// sacrificed for ease of use while training.</i></td></tr>
<tr><th id="284">284</th><td><b>class</b> ModelUnderTrainingRunner final : <b>public</b> MLModelRunner {</td></tr>
<tr><th id="285">285</th><td><b>public</b>:</td></tr>
<tr><th id="286">286</th><td>  ModelUnderTrainingRunner(LLVMContext &amp;Ctx, <em>const</em> std::string &amp;ModelPath);</td></tr>
<tr><th id="287">287</th><td></td></tr>
<tr><th id="288">288</th><td>  <em>bool</em> run() override;</td></tr>
<tr><th id="289">289</th><td></td></tr>
<tr><th id="290">290</th><td>  <i>// Disallows copy and assign.</i></td></tr>
<tr><th id="291">291</th><td>  ModelUnderTrainingRunner(<em>const</em> ModelUnderTrainingRunner &amp;) = <b>delete</b>;</td></tr>
<tr><th id="292">292</th><td>  ModelUnderTrainingRunner &amp;</td></tr>
<tr><th id="293">293</th><td>  <b>operator</b>=(<em>const</em> ModelUnderTrainingRunner &amp;) = <b>delete</b>;</td></tr>
<tr><th id="294">294</th><td></td></tr>
<tr><th id="295">295</th><td>  <em>void</em> setFeature(FeatureIndex Index, int64_t Value) override;</td></tr>
<tr><th id="296">296</th><td>  int64_t getFeature(<em>int</em> Index) <em>const</em> override;</td></tr>
<tr><th id="297">297</th><td>  <em>bool</em> isValid() <em>const</em> { <b>return</b> !!Evaluator; }</td></tr>
<tr><th id="298">298</th><td></td></tr>
<tr><th id="299">299</th><td>  <em>const</em> std::vector&lt;LoggedFeatureSpec&gt; &amp;outputLoggedFeatureSpecs() <em>const</em> {</td></tr>
<tr><th id="300">300</th><td>    <b>return</b> OutputSpecs;</td></tr>
<tr><th id="301">301</th><td>  }</td></tr>
<tr><th id="302">302</th><td></td></tr>
<tr><th id="303">303</th><td>  <em>const</em> Optional&lt;TFModelEvaluator::EvaluationResult&gt; &amp;</td></tr>
<tr><th id="304">304</th><td>  lastEvaluationResult() <em>const</em> {</td></tr>
<tr><th id="305">305</th><td>    <b>return</b> LastEvaluationResult;</td></tr>
<tr><th id="306">306</th><td>  }</td></tr>
<tr><th id="307">307</th><td></td></tr>
<tr><th id="308">308</th><td><b>private</b>:</td></tr>
<tr><th id="309">309</th><td>  std::unique_ptr&lt;TFModelEvaluator&gt; Evaluator;</td></tr>
<tr><th id="310">310</th><td>  std::vector&lt;LoggedFeatureSpec&gt; OutputSpecs;</td></tr>
<tr><th id="311">311</th><td>  Optional&lt;TFModelEvaluator::EvaluationResult&gt; LastEvaluationResult;</td></tr>
<tr><th id="312">312</th><td></td></tr>
<tr><th id="313">313</th><td>  <i>// The training framework needs some additional features.</i></td></tr>
<tr><th id="314">314</th><td>  <em>const</em> std::vector&lt;TensorSpec&gt; TrainingOnlyFeatures{</td></tr>
<tr><th id="315">315</th><td>      TensorSpec::createSpec&lt;int64_t&gt;(TFFeedPrefix + <q>"inlining_default"</q>, {<var>1</var>}),</td></tr>
<tr><th id="316">316</th><td>      TensorSpec::createSpec&lt;<em>float</em>&gt;(TFFeedPrefix + <q>"discount"</q>, {<var>1</var>}),</td></tr>
<tr><th id="317">317</th><td>      TensorSpec::createSpec&lt;<em>float</em>&gt;(TFFeedPrefix + <q>"reward"</q>, {<var>1</var>}),</td></tr>
<tr><th id="318">318</th><td>      TensorSpec::createSpec&lt;int32_t&gt;(TFFeedPrefix + <q>"step_type"</q>, {<var>1</var>})};</td></tr>
<tr><th id="319">319</th><td>};</td></tr>
<tr><th id="320">320</th><td>} <i>// namespace</i></td></tr>
<tr><th id="321">321</th><td></td></tr>
<tr><th id="322">322</th><td>TrainingLogger::TrainingLogger(StringRef LogFileName,</td></tr>
<tr><th id="323">323</th><td>                               <em>const</em> ModelUnderTrainingRunner *MUTR)</td></tr>
<tr><th id="324">324</th><td>    : LogFileName(LogFileName), MUTR(MUTR) {</td></tr>
<tr><th id="325">325</th><td>  <i>// The first output is the inlining decision.</i></td></tr>
<tr><th id="326">326</th><td>  <b>if</b> (MUTR)</td></tr>
<tr><th id="327">327</th><td>    OutputCount = MUTR-&gt;outputLoggedFeatureSpecs().size();</td></tr>
<tr><th id="328">328</th><td>  std::vector&lt;LoggedFeatureSpec&gt; FT;</td></tr>
<tr><th id="329">329</th><td></td></tr>
<tr><th id="330">330</th><td>  <b>for</b> (size_t I = <var>0</var>; I &lt; NumberOfFeatures; ++I)</td></tr>
<tr><th id="331">331</th><td>    FT.push_back(</td></tr>
<tr><th id="332">332</th><td>        {TensorSpec::createSpec&lt;int64_t&gt;(FeatureNameMap.at(I), {<var>1</var>}), None});</td></tr>
<tr><th id="333">333</th><td>  <b>if</b> (MUTR &amp;&amp; MUTR-&gt;outputLoggedFeatureSpecs().size() &gt; <var>1</var>)</td></tr>
<tr><th id="334">334</th><td>    append_range(FT, drop_begin(MUTR-&gt;outputLoggedFeatureSpecs()));</td></tr>
<tr><th id="335">335</th><td></td></tr>
<tr><th id="336">336</th><td>  DefaultDecisionPos = FT.size();</td></tr>
<tr><th id="337">337</th><td>  FT.push_back(</td></tr>
<tr><th id="338">338</th><td>      {TensorSpec::createSpec&lt;int64_t&gt;(DefaultDecisionName, {<var>1</var>}), None});</td></tr>
<tr><th id="339">339</th><td></td></tr>
<tr><th id="340">340</th><td>  DecisionPos = FT.size();</td></tr>
<tr><th id="341">341</th><td>  FT.push_back({TensorSpec::createSpec&lt;int64_t&gt;(DecisionName, {<var>1</var>}), None});</td></tr>
<tr><th id="342">342</th><td></td></tr>
<tr><th id="343">343</th><td>  L = std::make_unique&lt;Logger&gt;(</td></tr>
<tr><th id="344">344</th><td>      FT, TensorSpec::createSpec&lt;int64_t&gt;(RewardName, {<var>1</var>}),</td></tr>
<tr><th id="345">345</th><td>      InlineSizeEstimatorAnalysis::isEvaluatorRequested());</td></tr>
<tr><th id="346">346</th><td>}</td></tr>
<tr><th id="347">347</th><td></td></tr>
<tr><th id="348">348</th><td><i class="doc">/// Log one inlining event.</i></td></tr>
<tr><th id="349">349</th><td><em>void</em> TrainingLogger::logInlineEvent(<em>const</em> InlineEvent &amp;Event,</td></tr>
<tr><th id="350">350</th><td>                                    <em>const</em> MLModelRunner &amp;ModelRunner) {</td></tr>
<tr><th id="351">351</th><td>  size_t CurrentFeature = <var>0</var>;</td></tr>
<tr><th id="352">352</th><td>  <b>for</b> (; CurrentFeature &lt; NumberOfFeatures; ++CurrentFeature) {</td></tr>
<tr><th id="353">353</th><td>    int64_t F = ModelRunner.getFeature(CurrentFeature);</td></tr>
<tr><th id="354">354</th><td>    L-&gt;logTensorValue(CurrentFeature, &amp;F);</td></tr>
<tr><th id="355">355</th><td>  }</td></tr>
<tr><th id="356">356</th><td></td></tr>
<tr><th id="357">357</th><td>  <b>for</b> (size_t I = <var>1</var>; I &lt; OutputCount; ++I) {</td></tr>
<tr><th id="358">358</th><td>    <em>const</em> <em>auto</em> &amp;Result = *MUTR-&gt;lastEvaluationResult();</td></tr>
<tr><th id="359">359</th><td>    <em>auto</em> &amp;Spec = MUTR-&gt;outputLoggedFeatureSpecs()[I].Spec;</td></tr>
<tr><th id="360">360</th><td>    <em>const</em> <em>char</em> *RawData =</td></tr>
<tr><th id="361">361</th><td>        <b>reinterpret_cast</b>&lt;<em>const</em> <em>char</em> *&gt;(Result.getUntypedTensorValue(I));</td></tr>
<tr><th id="362">362</th><td>    L-&gt;logTensorValue(CurrentFeature, RawData,</td></tr>
<tr><th id="363">363</th><td>                      Spec.getElementCount() * Spec.getElementByteSize());</td></tr>
<tr><th id="364">364</th><td>    ++CurrentFeature;</td></tr>
<tr><th id="365">365</th><td>  }</td></tr>
<tr><th id="366">366</th><td></td></tr>
<tr><th id="367">367</th><td>  assert(CurrentFeature == DefaultDecisionPos);</td></tr>
<tr><th id="368">368</th><td>  L-&gt;logTensorValue(DefaultDecisionPos, &amp;Event.DefaultDecision);</td></tr>
<tr><th id="369">369</th><td>  L-&gt;logTensorValue(DecisionPos, &amp;Event.AdvisedDecision);</td></tr>
<tr><th id="370">370</th><td>  <b>if</b> (InlineSizeEstimatorAnalysis::isEvaluatorRequested())</td></tr>
<tr><th id="371">371</th><td>    L-&gt;logReward(Event.Reward);</td></tr>
<tr><th id="372">372</th><td></td></tr>
<tr><th id="373">373</th><td>  <i>// For debugging / later use</i></td></tr>
<tr><th id="374">374</th><td>  Effects.push_back(Event.Effect);</td></tr>
<tr><th id="375">375</th><td>}</td></tr>
<tr><th id="376">376</th><td></td></tr>
<tr><th id="377">377</th><td><em>void</em> TrainingLogger::print() {</td></tr>
<tr><th id="378">378</th><td>  std::error_code EC;</td></tr>
<tr><th id="379">379</th><td>  raw_fd_ostream OutFile(LogFileName, EC);</td></tr>
<tr><th id="380">380</th><td>  L-&gt;print(OutFile);</td></tr>
<tr><th id="381">381</th><td>}</td></tr>
<tr><th id="382">382</th><td></td></tr>
<tr><th id="383">383</th><td>DevelopmentModeMLInlineAdvisor::DevelopmentModeMLInlineAdvisor(</td></tr>
<tr><th id="384">384</th><td>    Module &amp;M, ModuleAnalysisManager &amp;MAM,</td></tr>
<tr><th id="385">385</th><td>    std::unique_ptr&lt;MLModelRunner&gt; ModelRunner,</td></tr>
<tr><th id="386">386</th><td>    std::function&lt;<em>bool</em>(CallBase &amp;)&gt; GetDefaultAdvice, <em>bool</em> IsDoingInference,</td></tr>
<tr><th id="387">387</th><td>    std::unique_ptr&lt;TrainingLogger&gt; Logger)</td></tr>
<tr><th id="388">388</th><td>    : MLInlineAdvisor(M, MAM, std::move(ModelRunner)),</td></tr>
<tr><th id="389">389</th><td>      GetDefaultAdvice(GetDefaultAdvice), IsDoingInference(IsDoingInference),</td></tr>
<tr><th id="390">390</th><td>      Logger(std::move(Logger)),</td></tr>
<tr><th id="391">391</th><td>      InitialNativeSize(isLogging() ? getTotalSizeEstimate() : <var>0</var>),</td></tr>
<tr><th id="392">392</th><td>      CurrentNativeSize(InitialNativeSize) {</td></tr>
<tr><th id="393">393</th><td>  <i>// We cannot have the case of neither inference nor logging.</i></td></tr>
<tr><th id="394">394</th><td>  assert(IsDoingInference || isLogging());</td></tr>
<tr><th id="395">395</th><td>}</td></tr>
<tr><th id="396">396</th><td></td></tr>
<tr><th id="397">397</th><td>DevelopmentModeMLInlineAdvisor::~DevelopmentModeMLInlineAdvisor() {</td></tr>
<tr><th id="398">398</th><td>  <b>if</b> (isLogging())</td></tr>
<tr><th id="399">399</th><td>    Logger-&gt;print();</td></tr>
<tr><th id="400">400</th><td>}</td></tr>
<tr><th id="401">401</th><td></td></tr>
<tr><th id="402">402</th><td>Optional&lt;size_t&gt;</td></tr>
<tr><th id="403">403</th><td>DevelopmentModeMLInlineAdvisor::getNativeSizeEstimate(<em>const</em> Function &amp;F) <em>const</em> {</td></tr>
<tr><th id="404">404</th><td>  <b>if</b> (!InlineSizeEstimatorAnalysis::isEvaluatorRequested())</td></tr>
<tr><th id="405">405</th><td>    <b>return</b> None;</td></tr>
<tr><th id="406">406</th><td>  <em>auto</em> &amp;R =</td></tr>
<tr><th id="407">407</th><td>      FAM.getResult&lt;InlineSizeEstimatorAnalysis&gt;(<b>const_cast</b>&lt;Function &amp;&gt;(F));</td></tr>
<tr><th id="408">408</th><td>  <b>if</b> (!R) {</td></tr>
<tr><th id="409">409</th><td>    F.getParent()-&gt;getContext().emitError(</td></tr>
<tr><th id="410">410</th><td>        <q>"Native size estimator is not present."</q>);</td></tr>
<tr><th id="411">411</th><td>    <b>return</b> <var>0</var>;</td></tr>
<tr><th id="412">412</th><td>  }</td></tr>
<tr><th id="413">413</th><td>  <b>return</b> *R;</td></tr>
<tr><th id="414">414</th><td>}</td></tr>
<tr><th id="415">415</th><td></td></tr>
<tr><th id="416">416</th><td>std::unique_ptr&lt;MLInlineAdvice&gt;</td></tr>
<tr><th id="417">417</th><td>DevelopmentModeMLInlineAdvisor::getMandatoryAdviceImpl(CallBase &amp;CB) {</td></tr>
<tr><th id="418">418</th><td>  <b>return</b> std::make_unique&lt;LoggingMLInlineAdvice&gt;(</td></tr>
<tr><th id="419">419</th><td>      <i>/*Advisor=*/</i><b>this</b>,</td></tr>
<tr><th id="420">420</th><td>      <i>/*CB=*/</i>CB, <i>/*ORE=*/</i>getCallerORE(CB), <i>/*Recommendation=*/</i><b>true</b>,</td></tr>
<tr><th id="421">421</th><td>      <i>/*Logger=*/</i>*Logger,</td></tr>
<tr><th id="422">422</th><td>      <i>/*CallerSizeEstimateBefore=*/</i>getNativeSizeEstimate(*CB.getCaller()),</td></tr>
<tr><th id="423">423</th><td>      <i>/*CalleeSizeEstimateBefore=*/</i></td></tr>
<tr><th id="424">424</th><td>      getNativeSizeEstimate(*CB.getCalledFunction()),</td></tr>
<tr><th id="425">425</th><td>      <i>/*DefaultDecision=*/</i><b>true</b>, <i>/*Mandatory*/</i> <b>true</b>);</td></tr>
<tr><th id="426">426</th><td>}</td></tr>
<tr><th id="427">427</th><td></td></tr>
<tr><th id="428">428</th><td>std::unique_ptr&lt;MLInlineAdvice&gt;</td></tr>
<tr><th id="429">429</th><td>DevelopmentModeMLInlineAdvisor::getAdviceFromModel(</td></tr>
<tr><th id="430">430</th><td>    CallBase &amp;CB, OptimizationRemarkEmitter &amp;ORE) {</td></tr>
<tr><th id="431">431</th><td>  <b>if</b> (IsDoingInference &amp;&amp; !isLogging())</td></tr>
<tr><th id="432">432</th><td>    <b>return</b> MLInlineAdvisor::getAdviceFromModel(CB, ORE);</td></tr>
<tr><th id="433">433</th><td></td></tr>
<tr><th id="434">434</th><td>  <em>bool</em> DefaultAdvice = GetDefaultAdvice(CB);</td></tr>
<tr><th id="435">435</th><td>  <em>auto</em> Recommendation = IsDoingInference ? ModelRunner-&gt;run() : DefaultAdvice;</td></tr>
<tr><th id="436">436</th><td>  <b>return</b> std::make_unique&lt;LoggingMLInlineAdvice&gt;(</td></tr>
<tr><th id="437">437</th><td>      <i>/*Advisor=*/</i><b>this</b>,</td></tr>
<tr><th id="438">438</th><td>      <i>/*CB=*/</i>CB, <i>/*ORE=*/</i>ORE, <i>/*Recommendation=*/</i>Recommendation,</td></tr>
<tr><th id="439">439</th><td>      <i>/*Logger=*/</i>*Logger,</td></tr>
<tr><th id="440">440</th><td>      <i>/*CallerSizeEstimateBefore=*/</i>getNativeSizeEstimate(*CB.getCaller()),</td></tr>
<tr><th id="441">441</th><td>      <i>/*CalleeSizeEstimateBefore=*/</i></td></tr>
<tr><th id="442">442</th><td>      getNativeSizeEstimate(*CB.getCalledFunction()),</td></tr>
<tr><th id="443">443</th><td>      <i>/*DefaultDecision=*/</i>DefaultAdvice);</td></tr>
<tr><th id="444">444</th><td>}</td></tr>
<tr><th id="445">445</th><td></td></tr>
<tr><th id="446">446</th><td>size_t DevelopmentModeMLInlineAdvisor::getTotalSizeEstimate() {</td></tr>
<tr><th id="447">447</th><td>  <b>if</b> (!InlineSizeEstimatorAnalysis::isEvaluatorRequested())</td></tr>
<tr><th id="448">448</th><td>    <b>return</b> <var>0</var>;</td></tr>
<tr><th id="449">449</th><td>  size_t Ret = <var>0</var>;</td></tr>
<tr><th id="450">450</th><td>  <b>for</b> (<em>auto</em> &amp;F : M) {</td></tr>
<tr><th id="451">451</th><td>    <b>if</b> (F.isDeclaration())</td></tr>
<tr><th id="452">452</th><td>      <b>continue</b>;</td></tr>
<tr><th id="453">453</th><td>    <b>if</b> (isFunctionDeleted(&amp;F))</td></tr>
<tr><th id="454">454</th><td>      <b>continue</b>;</td></tr>
<tr><th id="455">455</th><td>    Ret += *getNativeSizeEstimate(F);</td></tr>
<tr><th id="456">456</th><td>  }</td></tr>
<tr><th id="457">457</th><td>  <b>return</b> Ret;</td></tr>
<tr><th id="458">458</th><td>}</td></tr>
<tr><th id="459">459</th><td></td></tr>
<tr><th id="460">460</th><td>ModelUnderTrainingRunner::ModelUnderTrainingRunner(LLVMContext &amp;Ctx,</td></tr>
<tr><th id="461">461</th><td>                                                   <em>const</em> std::string &amp;ModelPath)</td></tr>
<tr><th id="462">462</th><td>    : MLModelRunner(Ctx) {</td></tr>
<tr><th id="463">463</th><td>  std::vector&lt;TensorSpec&gt; InputSpecs;</td></tr>
<tr><th id="464">464</th><td>  <b>for</b> (size_t I = <var>0</var>; I &lt; NumberOfFeatures; ++I)</td></tr>
<tr><th id="465">465</th><td>    InputSpecs.push_back(</td></tr>
<tr><th id="466">466</th><td>        TensorSpec::createSpec&lt;int64_t&gt;(TFFeedPrefix + FeatureNameMap[I], {<var>1</var>}));</td></tr>
<tr><th id="467">467</th><td>  append_range(InputSpecs, TrainingOnlyFeatures);</td></tr>
<tr><th id="468">468</th><td>  <b>if</b> (<em>auto</em> MaybeOutSpecs =</td></tr>
<tr><th id="469">469</th><td>          loadOutputSpecs(Ctx, DecisionName, ModelPath, TFOutputSpecOverride))</td></tr>
<tr><th id="470">470</th><td>    OutputSpecs = std::move(*MaybeOutSpecs);</td></tr>
<tr><th id="471">471</th><td>  <b>else</b></td></tr>
<tr><th id="472">472</th><td>    <b>return</b>;</td></tr>
<tr><th id="473">473</th><td></td></tr>
<tr><th id="474">474</th><td>  Evaluator = std::make_unique&lt;TFModelEvaluator&gt;(</td></tr>
<tr><th id="475">475</th><td>      ModelPath, InputSpecs, [&amp;](size_t I) { <b>return</b> OutputSpecs[I].Spec; },</td></tr>
<tr><th id="476">476</th><td>      OutputSpecs.size());</td></tr>
<tr><th id="477">477</th><td>  <b>if</b> (!Evaluator || !Evaluator-&gt;isValid()) {</td></tr>
<tr><th id="478">478</th><td>    Ctx.emitError(<q>"Failed to create inliner saved model evaluator"</q>);</td></tr>
<tr><th id="479">479</th><td>    Evaluator.reset();</td></tr>
<tr><th id="480">480</th><td>    <b>return</b>;</td></tr>
<tr><th id="481">481</th><td>  }</td></tr>
<tr><th id="482">482</th><td>}</td></tr>
<tr><th id="483">483</th><td></td></tr>
<tr><th id="484">484</th><td><em>bool</em> ModelUnderTrainingRunner::run() {</td></tr>
<tr><th id="485">485</th><td>  LastEvaluationResult = Evaluator-&gt;evaluate();</td></tr>
<tr><th id="486">486</th><td>  <b>if</b> (!LastEvaluationResult.hasValue()) {</td></tr>
<tr><th id="487">487</th><td>    Ctx.emitError(<q>"Error evaluating model."</q>);</td></tr>
<tr><th id="488">488</th><td>    <b>return</b> <b>false</b>;</td></tr>
<tr><th id="489">489</th><td>  }</td></tr>
<tr><th id="490">490</th><td>  int64_t Decision = *LastEvaluationResult-&gt;getTensorValue&lt;int64_t&gt;(<var>0</var>);</td></tr>
<tr><th id="491">491</th><td>  <b>return</b> <b>static_cast</b>&lt;<em>bool</em>&gt;(Decision);</td></tr>
<tr><th id="492">492</th><td>}</td></tr>
<tr><th id="493">493</th><td></td></tr>
<tr><th id="494">494</th><td>int64_t ModelUnderTrainingRunner::getFeature(<em>int</em> Index) <em>const</em> {</td></tr>
<tr><th id="495">495</th><td>  <b>return</b> *Evaluator-&gt;getInput&lt;int64_t&gt;(Index);</td></tr>
<tr><th id="496">496</th><td>}</td></tr>
<tr><th id="497">497</th><td></td></tr>
<tr><th id="498">498</th><td><em>void</em> ModelUnderTrainingRunner::setFeature(FeatureIndex Index, int64_t Value) {</td></tr>
<tr><th id="499">499</th><td>  size_t NumericIndex = <b>static_cast</b>&lt;size_t&gt;(Index);</td></tr>
<tr><th id="500">500</th><td>  *(Evaluator-&gt;getInput&lt;int64_t&gt;(NumericIndex)) = Value;</td></tr>
<tr><th id="501">501</th><td>}</td></tr>
<tr><th id="502">502</th><td></td></tr>
<tr><th id="503">503</th><td>std::unique_ptr&lt;InlineAdvisor&gt; llvm::getDevelopmentModeAdvisor(</td></tr>
<tr><th id="504">504</th><td>    Module &amp;M, ModuleAnalysisManager &amp;MAM,</td></tr>
<tr><th id="505">505</th><td>    std::function&lt;<em>bool</em>(CallBase &amp;)&gt; GetDefaultAdvice) {</td></tr>
<tr><th id="506">506</th><td>  <em>auto</em> &amp;Ctx = M.getContext();</td></tr>
<tr><th id="507">507</th><td>  std::unique_ptr&lt;MLModelRunner&gt; Runner;</td></tr>
<tr><th id="508">508</th><td>  ModelUnderTrainingRunner *MUTRPtr = <b>nullptr</b>;</td></tr>
<tr><th id="509">509</th><td>  <em>bool</em> IsDoingInference = <b>false</b>;</td></tr>
<tr><th id="510">510</th><td>  <b>if</b> (TFModelUnderTrainingPath.empty())</td></tr>
<tr><th id="511">511</th><td>    Runner.reset(<b>new</b> NoInferenceModelRunner(Ctx));</td></tr>
<tr><th id="512">512</th><td>  <b>else</b> {</td></tr>
<tr><th id="513">513</th><td>    <em>auto</em> MUTR = std::make_unique&lt;ModelUnderTrainingRunner&gt;(</td></tr>
<tr><th id="514">514</th><td>        Ctx, TFModelUnderTrainingPath);</td></tr>
<tr><th id="515">515</th><td>    <b>if</b> (!MUTR || !MUTR-&gt;isValid()) {</td></tr>
<tr><th id="516">516</th><td>      Ctx.emitError(<q>"Could not load the policy model from the provided path"</q>);</td></tr>
<tr><th id="517">517</th><td>      <b>return</b> <b>nullptr</b>;</td></tr>
<tr><th id="518">518</th><td>    }</td></tr>
<tr><th id="519">519</th><td>    IsDoingInference = <b>true</b>;</td></tr>
<tr><th id="520">520</th><td>    MUTRPtr = MUTR.get();</td></tr>
<tr><th id="521">521</th><td>    Runner = std::move(MUTR);</td></tr>
<tr><th id="522">522</th><td>  }</td></tr>
<tr><th id="523">523</th><td>  std::unique_ptr&lt;TrainingLogger&gt; Logger;</td></tr>
<tr><th id="524">524</th><td>  <b>if</b> (!TrainingLog.empty())</td></tr>
<tr><th id="525">525</th><td>    Logger = std::make_unique&lt;TrainingLogger&gt;(TrainingLog, MUTRPtr);</td></tr>
<tr><th id="526">526</th><td></td></tr>
<tr><th id="527">527</th><td>  <b>return</b> std::make_unique&lt;DevelopmentModeMLInlineAdvisor&gt;(</td></tr>
<tr><th id="528">528</th><td>      M, MAM, std::move(Runner), GetDefaultAdvice, IsDoingInference,</td></tr>
<tr><th id="529">529</th><td>      std::move(Logger));</td></tr>
<tr><th id="530">530</th><td>}</td></tr>
<tr><th id="531">531</th><td><u>#<span data-ppcond="15">endif</span> // defined(LLVM_HAVE_TF_API)</u></td></tr>
<tr><th id="532">532</th><td></td></tr>
</table><hr/><p id='footer'>
Generated on <em>2021-Jul-01</em> from project llvm revision <em>12</em>