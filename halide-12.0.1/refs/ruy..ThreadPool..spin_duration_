<use f='halide/build-apps/ruy/ruy/thread_pool.h' l='81' u='w' c='_ZN3ruy10ThreadPool21set_spin_millisecondsEf'/>
<use f='halide/build-apps/ruy/ruy/thread_pool.h' l='85' u='r' c='_ZNK3ruy10ThreadPool17spin_millisecondsEv'/>
<dec f='halide/build-apps/ruy/ruy/thread_pool.h' l='122' type='ruy::Duration'/>
<offset>960</offset>
<doc f='halide/build-apps/ruy/ruy/thread_pool.h' l='108'>// This value was empirically derived with some microbenchmark, we don&apos;t have
  // high confidence in it.
  //
  // That this value means that we may be sleeping substantially longer
  // than a scheduler timeslice&apos;s duration is not necessarily surprising. The
  // idea is to pick up quickly new work after having finished the previous
  // workload. When it&apos;s new work within the same GEMM as the previous work, the
  // time interval that we might be busy-waiting is very small, so for that
  // purpose it would be more than enough to sleep for 1 ms.
  // That is all what we would observe on a GEMM benchmark. However, in a real
  // application, after having finished a GEMM, we might do unrelated work for
  // a little while, then start on a new GEMM. In that case the wait interval
  // may be a little longer. There may also not be another GEMM for a long time,
  // in which case we&apos;ll end up passively waiting below.</doc>
<use f='halide/build-apps/ruy/ruy/thread_pool.cc' l='196' u='r' c='_ZN3ruy10ThreadPool11ExecuteImplEiiPNS_4TaskE'/>
<use f='halide/build-apps/ruy/ruy/thread_pool.cc' l='211' u='r' c='_ZN3ruy10ThreadPool13CreateThreadsEi'/>
<use f='halide/build-apps/ruy/ruy/thread_pool.cc' l='213' u='r' c='_ZN3ruy10ThreadPool13CreateThreadsEi'/>
