<dec f='halide/build-apps/_deps/tflite-src/tensorflow/lite/core/api/flatbuffer_conversions.h' l='63' type='TfLiteStatus tflite::ParseOpData(const tflite::Operator * op, tflite::BuiltinOperator op_type, tflite::ErrorReporter * error_reporter, tflite::BuiltinDataAllocator * allocator, void ** builtin_data)'/>
<def f='halide/build-apps/_deps/tflite-src/tensorflow/lite/core/api/flatbuffer_conversions.cc' l='2126' ll='2157' type='TfLiteStatus tflite::ParseOpData(const tflite::Operator * op, tflite::BuiltinOperator op_type, tflite::ErrorReporter * error_reporter, tflite::BuiltinDataAllocator * allocator, void ** builtin_data)'/>
<doc f='halide/build-apps/_deps/tflite-src/tensorflow/lite/core/api/flatbuffer_conversions.h' l='54'>// Parse the appropriate data out of the op.
//
// This handles builtin data explicitly as there are flatbuffer schemas.
// If it returns kTfLiteOk, it passes the data out with `builtin_data`. The
// calling function has to pass in an allocator object, and this allocator
// will be called to reserve space for the output data. If the calling
// function&apos;s allocator reserves memory on the heap, then it&apos;s the calling
// function&apos;s responsibility to free it.
// If it returns kTfLiteError, `builtin_data` will be `nullptr`.</doc>
<use f='halide/build-apps/_deps/tflite-src/tensorflow/lite/interpreter_builder.cc' l='343' u='c' c='_ZN6tflite18InterpreterBuilder10ParseNodesEPKN11flatbuffers6VectorINS1_6OffsetINS_8OperatorEEEEEPNS_8SubgraphE'/>
