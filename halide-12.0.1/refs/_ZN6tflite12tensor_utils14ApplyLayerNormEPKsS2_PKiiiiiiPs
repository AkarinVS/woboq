<dec f='halide/build-apps/_deps/tflite-src/tensorflow/lite/kernels/internal/tensor_utils_common.h' l='203' type='void tflite::tensor_utils::ApplyLayerNorm(const int16_t * input, const int16_t * layer_norm_weights, const int32_t * bias, int32_t layer_norm_scale_a, int32_t layer_norm_scale_b, int32_t variance_limit, int n_batch, int n_input, int16_t * output)'/>
<doc f='halide/build-apps/_deps/tflite-src/tensorflow/lite/kernels/internal/tensor_utils_common.h' l='191'>// Apply Layer Normalization (https://arxiv.org/abs/1607.06450) to a Quantized
// vector.
// Parameters:
//     - input: batch vector of size n_batch * n_input; 16 bit.
//     - layer_norm_weights:  the quantized layer normalization weights.
//     - bias: the bias for the layer normalization.
//     - layer_norm_scale_a: multiplier for scale factor.
//     - layer_norm_scale_b: shift for scale factor.
//     - variance_limit: the guard to make sure the inverse does not overflow.
//     - n_batch: the number of batches.
//     - n_input: the size for input and output.
//     - output:  the 16 bit output</doc>
<def f='halide/build-apps/_deps/tflite-src/tensorflow/lite/kernels/internal/reference/portable_tensor_utils.h' l='174' ll='181' type='void tflite::tensor_utils::ApplyLayerNorm(const int16_t * input, const int16_t * layer_norm_weights, const int32_t * bias, int32_t layer_norm_scale_a, int32_t layer_norm_scale_b, int32_t variance_limit, int n_batch, int n_input, int16_t * output)'/>
<use f='halide/build-apps/_deps/tflite-src/tensorflow/lite/kernels/lstm_eval.cc' l='540' u='c' c='_ZN6tflite3ops7builtin9lstm_eval12_GLOBAL__N_130CalculateLstmGateInteger8x8_16EPKaS5_PKiiiS5_S5_S7_iiPKsS9_iiS9_S7_iiiiiii21TfLiteFusedActivationPsPNS14049076'/>
