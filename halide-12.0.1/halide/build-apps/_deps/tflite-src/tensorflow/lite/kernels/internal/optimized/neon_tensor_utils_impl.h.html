<!doctype html>
<html>
<head>
<meta name="viewport" content="width=device-width, initial-scale=1.0"><title>neon_tensor_utils_impl.h source code [halide/build-apps/_deps/tflite-src/tensorflow/lite/kernels/internal/optimized/neon_tensor_utils_impl.h] - Woboq Code Browser</title>
<link rel="stylesheet" href="../../../../../../../../.././data/qtcreator.css" title="QtCreator"/>
<link rel="alternate stylesheet" href="../../../../../../../../.././data/kdevelop.css" title="KDevelop"/>
<script type="text/javascript" src="../../../../../../../../.././data/jquery/jquery.min.js"></script>
<script type="text/javascript" src="../../../../../../../../.././data/jquery/jquery-ui.min.js"></script>
<script>var file = 'halide/build-apps/_deps/tflite-src/tensorflow/lite/kernels/internal/optimized/neon_tensor_utils_impl.h'; var root_path = '../../../../../../../../..'; var data_path = '../../../../../../../../.././data'; var ecma_script_api_version = 2;</script>
<script src='../../../../../../../../.././data/codebrowser.js'></script>
</head>
<body><div id='header'><h1 id='breadcrumb'><span>Browse the source code of </span><a href='../../../../../../../..'>halide</a>/<a href='../../../../../../..'>build-apps</a>/<a href='../../../../../..'>_deps</a>/<a href='../../../../..'>tflite-src</a>/<a href='../../../..'>tensorflow</a>/<a href='../../..'>lite</a>/<a href='../..'>kernels</a>/<a href='..'>internal</a>/<a href='./'>optimized</a>/<a href='neon_tensor_utils_impl.h.html'>neon_tensor_utils_impl.h</a></h1></div>
<hr/><div id='content'><table class="code">
<tr><th id="1">1</th><td><i>/* Copyright 2017 The TensorFlow Authors. All Rights Reserved.</i></td></tr>
<tr><th id="2">2</th><td><i></i></td></tr>
<tr><th id="3">3</th><td><i>Licensed under the Apache License, Version 2.0 (the "License");</i></td></tr>
<tr><th id="4">4</th><td><i>you may not use this file except in compliance with the License.</i></td></tr>
<tr><th id="5">5</th><td><i>You may obtain a copy of the License at</i></td></tr>
<tr><th id="6">6</th><td><i></i></td></tr>
<tr><th id="7">7</th><td><i>    <a href="http://www.apache.org/licenses/LICENSE-2.0">http://www.apache.org/licenses/LICENSE-2.0</a></i></td></tr>
<tr><th id="8">8</th><td><i></i></td></tr>
<tr><th id="9">9</th><td><i>Unless required by applicable law or agreed to in writing, software</i></td></tr>
<tr><th id="10">10</th><td><i>distributed under the License is distributed on an "AS IS" BASIS,</i></td></tr>
<tr><th id="11">11</th><td><i>WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</i></td></tr>
<tr><th id="12">12</th><td><i>See the License for the specific language governing permissions and</i></td></tr>
<tr><th id="13">13</th><td><i>limitations under the License.</i></td></tr>
<tr><th id="14">14</th><td><i>==============================================================================*/</i></td></tr>
<tr><th id="15">15</th><td><u>#<span data-ppcond="15">ifndef</span> <span class="macro" data-ref="_M/TENSORFLOW_LITE_KERNELS_INTERNAL_OPTIMIZED_NEON_TENSOR_UTILS_IMPL_H_">TENSORFLOW_LITE_KERNELS_INTERNAL_OPTIMIZED_NEON_TENSOR_UTILS_IMPL_H_</span></u></td></tr>
<tr><th id="16">16</th><td><u>#define <dfn class="macro" id="_M/TENSORFLOW_LITE_KERNELS_INTERNAL_OPTIMIZED_NEON_TENSOR_UTILS_IMPL_H_" data-ref="_M/TENSORFLOW_LITE_KERNELS_INTERNAL_OPTIMIZED_NEON_TENSOR_UTILS_IMPL_H_">TENSORFLOW_LITE_KERNELS_INTERNAL_OPTIMIZED_NEON_TENSOR_UTILS_IMPL_H_</dfn></u></td></tr>
<tr><th id="17">17</th><td></td></tr>
<tr><th id="18">18</th><td><u>#include <a href="../../cpu_backend_context.h.html">"tensorflow/lite/kernels/cpu_backend_context.h"</a></u></td></tr>
<tr><th id="19">19</th><td><u>#include <a href="cpu_check.h.html">"tensorflow/lite/kernels/internal/optimized/cpu_check.h"</a></u></td></tr>
<tr><th id="20">20</th><td></td></tr>
<tr><th id="21">21</th><td><u>#<span data-ppcond="21">if</span> defined(<span class="macro" data-ref="_M/_MSC_VER">_MSC_VER</span>)</u></td></tr>
<tr><th id="22">22</th><td><u>#define __restrict__ __restrict</u></td></tr>
<tr><th id="23">23</th><td><u>#<span data-ppcond="21">endif</span></u></td></tr>
<tr><th id="24">24</th><td></td></tr>
<tr><th id="25">25</th><td><b>namespace</b> <span class="namespace">tflite</span> {</td></tr>
<tr><th id="26">26</th><td><b>namespace</b> <span class="namespace">tensor_utils</span> {</td></tr>
<tr><th id="27">27</th><td></td></tr>
<tr><th id="28">28</th><td><u>#<span data-ppcond="28">ifdef</span> <span class="macro" data-ref="_M/USE_NEON">USE_NEON</span></u></td></tr>
<tr><th id="29">29</th><td></td></tr>
<tr><th id="30">30</th><td><i>// Multiply a matrix by a batch vector, and store results in a batch-size</i></td></tr>
<tr><th id="31">31</th><td><i>// vector.</i></td></tr>
<tr><th id="32">32</th><td><em>void</em> NeonMatrixBatchVectorMultiplyAccumulate(<em>const</em> <em>float</em>* matrix, <em>int</em> m_rows,</td></tr>
<tr><th id="33">33</th><td>                                             <em>int</em> m_cols, <em>const</em> <em>float</em>* vector,</td></tr>
<tr><th id="34">34</th><td>                                             <em>int</em> n_batch, <em>float</em>* result);</td></tr>
<tr><th id="35">35</th><td></td></tr>
<tr><th id="36">36</th><td><i>// Matrix multiplication for quantized values using symmetric quantization.</i></td></tr>
<tr><th id="37">37</th><td><em>void</em> NeonMatrixBatchVectorMultiplyAccumulate(<em>const</em> int8_t* <b>__restrict__</b> matrix,</td></tr>
<tr><th id="38">38</th><td>                                             <em>const</em> <em>int</em> m_rows, <em>const</em> <em>int</em> m_cols,</td></tr>
<tr><th id="39">39</th><td>                                             <em>const</em> int8_t* <b>__restrict__</b> vectors,</td></tr>
<tr><th id="40">40</th><td>                                             <em>const</em> <em>float</em>* scaling_factors,</td></tr>
<tr><th id="41">41</th><td>                                             <em>int</em> n_batch,</td></tr>
<tr><th id="42">42</th><td>                                             <em>float</em>* <b>__restrict__</b> result);</td></tr>
<tr><th id="43">43</th><td></td></tr>
<tr><th id="44">44</th><td><i>// Same as above but with a scratch buffer and CpuBackendContext for the</i></td></tr>
<tr><th id="45">45</th><td><i>// int8 x int8 -&gt; int32 accumulation computation</i></td></tr>
<tr><th id="46">46</th><td><em>void</em> NeonMatrixBatchVectorMultiplyAccumulate(<em>const</em> int8_t* <b>__restrict__</b> matrix,</td></tr>
<tr><th id="47">47</th><td>                                             <em>const</em> <em>int</em> m_rows, <em>const</em> <em>int</em> m_cols,</td></tr>
<tr><th id="48">48</th><td>                                             <em>const</em> int8_t* <b>__restrict__</b> vectors,</td></tr>
<tr><th id="49">49</th><td>                                             <em>const</em> <em>float</em>* scaling_factors,</td></tr>
<tr><th id="50">50</th><td>                                             <em>int</em> n_batch, int32_t* scratch,</td></tr>
<tr><th id="51">51</th><td>                                             <em>float</em>* <b>__restrict__</b> result,</td></tr>
<tr><th id="52">52</th><td>                                             CpuBackendContext* context);</td></tr>
<tr><th id="53">53</th><td></td></tr>
<tr><th id="54">54</th><td><i>// Matrix multiplication for quantized values using asymmetric quantization.</i></td></tr>
<tr><th id="55">55</th><td><em>void</em> NeonMatrixBatchVectorMultiplyAccumulate(</td></tr>
<tr><th id="56">56</th><td>    <em>const</em> int8_t* <b>__restrict__</b> matrix, <em>const</em> <em>int</em> m_rows, <em>const</em> <em>int</em> m_cols,</td></tr>
<tr><th id="57">57</th><td>    <em>const</em> int8_t* <b>__restrict__</b> vectors, <em>const</em> <em>float</em>* scaling_factors,</td></tr>
<tr><th id="58">58</th><td>    <em>int</em> n_batch, <em>float</em>* <b>__restrict__</b> result, <em>const</em> <em>float</em>* per_channel_scale,</td></tr>
<tr><th id="59">59</th><td>    <em>const</em> int32_t* input_offset, int32_t* scratch, int32_t* row_sums,</td></tr>
<tr><th id="60">60</th><td>    <em>bool</em>* compute_row_sums, CpuBackendContext* context);</td></tr>
<tr><th id="61">61</th><td></td></tr>
<tr><th id="62">62</th><td><em>void</em> NeonApplyLayerNorm(<em>const</em> int16_t* input, <em>const</em> int16_t* layer_norm_weights,</td></tr>
<tr><th id="63">63</th><td>                        <em>const</em> int32_t* bias, int32_t layer_norm_scale_a,</td></tr>
<tr><th id="64">64</th><td>                        int32_t layer_norm_scale_b, int32_t variance_limit,</td></tr>
<tr><th id="65">65</th><td>                        <em>int</em> n_batch, <em>int</em> n_input, int16_t* output);</td></tr>
<tr><th id="66">66</th><td></td></tr>
<tr><th id="67">67</th><td><em>void</em> NeonApplySigmoid(<em>const</em> int16_t* input, int32_t n_batch, int32_t n_input,</td></tr>
<tr><th id="68">68</th><td>                      int16_t* output);</td></tr>
<tr><th id="69">69</th><td></td></tr>
<tr><th id="70">70</th><td><em>void</em> NeonApplyTanh(int32_t integer_bits, <em>const</em> int16_t* input, int32_t n_batch,</td></tr>
<tr><th id="71">71</th><td>                   int32_t n_input, int16_t* output);</td></tr>
<tr><th id="72">72</th><td></td></tr>
<tr><th id="73">73</th><td><em>void</em> NeonCwiseMul(<em>const</em> int16_t* input_1, <em>const</em> int16_t* input_2, <em>int</em> n_batch,</td></tr>
<tr><th id="74">74</th><td>                  <em>int</em> n_input, <em>int</em> shift, int16_t* output);</td></tr>
<tr><th id="75">75</th><td></td></tr>
<tr><th id="76">76</th><td><em>void</em> NeonCwiseMul(<em>const</em> int16_t* input_1, <em>const</em> int16_t* input_2,</td></tr>
<tr><th id="77">77</th><td>                  int32_t multiplier, <em>int</em> shift, <em>int</em> n_batch, <em>int</em> n_input,</td></tr>
<tr><th id="78">78</th><td>                  int32_t output_zp, int8_t* output);</td></tr>
<tr><th id="79">79</th><td></td></tr>
<tr><th id="80">80</th><td><em>void</em> NeonCwiseAdd(<em>const</em> int16_t* input_1, <em>const</em> int16_t* input_2, <em>int</em> n_batch,</td></tr>
<tr><th id="81">81</th><td>                  <em>int</em> n_input, int16_t* output);</td></tr>
<tr><th id="82">82</th><td></td></tr>
<tr><th id="83">83</th><td><em>void</em> NeonCwiseClipping(<em>float</em>* vector, <em>const</em> <em>int</em> v_size,</td></tr>
<tr><th id="84">84</th><td>                       <em>const</em> <em>float</em> clipping_value);</td></tr>
<tr><th id="85">85</th><td><em>void</em> NeonCwiseClipping(int16_t* vector, <em>const</em> <em>int</em> v_size,</td></tr>
<tr><th id="86">86</th><td>                       <em>const</em> int16_t clipping_value);</td></tr>
<tr><th id="87">87</th><td><em>void</em> NeonCwiseClipping(int8_t* vector, <em>const</em> <em>int</em> v_size,</td></tr>
<tr><th id="88">88</th><td>                       <em>const</em> int8_t clipping_value);</td></tr>
<tr><th id="89">89</th><td></td></tr>
<tr><th id="90">90</th><td><em>void</em> NeonMatrixBatchVectorMultiplyAccumulate(</td></tr>
<tr><th id="91">91</th><td>    <em>const</em> int8_t* input, <em>const</em> int32_t* bias,</td></tr>
<tr><th id="92">92</th><td>    <em>const</em> int8_t* input_to_gate_weights, int32_t multiplier, int32_t shift,</td></tr>
<tr><th id="93">93</th><td>    int32_t n_batch, int32_t n_input, int32_t n_output, int32_t output_zp,</td></tr>
<tr><th id="94">94</th><td>    int32_t* scratch, int8_t* output, CpuBackendContext* context);</td></tr>
<tr><th id="95">95</th><td></td></tr>
<tr><th id="96">96</th><td><em>void</em> NeonMatrixBatchVectorMultiplyAccumulate(</td></tr>
<tr><th id="97">97</th><td>    <em>const</em> int8_t* input, <em>const</em> int32_t* bias,</td></tr>
<tr><th id="98">98</th><td>    <em>const</em> int8_t* input_to_gate_weights, int32_t multiplier, int32_t shift,</td></tr>
<tr><th id="99">99</th><td>    int32_t n_batch, int32_t n_input, int32_t n_output, int32_t output_zp,</td></tr>
<tr><th id="100">100</th><td>    int32_t* scratch, int16_t* output, CpuBackendContext* context);</td></tr>
<tr><th id="101">101</th><td></td></tr>
<tr><th id="102">102</th><td><em>void</em> NeonMatrixScalarMultiplyAccumulate(<em>const</em> int8_t* matrix, int32_t scalar,</td></tr>
<tr><th id="103">103</th><td>                                        int32_t n_row, int32_t n_col,</td></tr>
<tr><th id="104">104</th><td>                                        int32_t* output);</td></tr>
<tr><th id="105">105</th><td></td></tr>
<tr><th id="106">106</th><td><em>void</em> NeonSparseMatrixBatchVectorMultiplyAccumulate1x4(</td></tr>
<tr><th id="107">107</th><td>    <em>const</em> <em>float</em>* <b>__restrict__</b> matrix, <em>const</em> int32_t* <b>__restrict__</b> segments,</td></tr>
<tr><th id="108">108</th><td>    <em>const</em> int32_t* <b>__restrict__</b> indices, <em>int</em> m_rows, <em>int</em> m_cols,</td></tr>
<tr><th id="109">109</th><td>    <em>const</em> <em>float</em>* <b>__restrict__</b> vector, <em>int</em> n_batch, <em>float</em>* <b>__restrict__</b> result);</td></tr>
<tr><th id="110">110</th><td></td></tr>
<tr><th id="111">111</th><td><i>// Multiply a matrix by a batch vector, and store results in a batch-size</i></td></tr>
<tr><th id="112">112</th><td><i>// vector. Sparse version.</i></td></tr>
<tr><th id="113">113</th><td><em>void</em> NeonSparseMatrixBatchVectorMultiplyAccumulate(</td></tr>
<tr><th id="114">114</th><td>    <em>const</em> <em>float</em>* <b>__restrict__</b> matrix, <em>const</em> uint8_t* <b>__restrict__</b> ledger,</td></tr>
<tr><th id="115">115</th><td>    <em>int</em> m_rows, <em>int</em> m_cols, <em>const</em> <em>float</em>* <b>__restrict__</b> vector, <em>int</em> n_batch,</td></tr>
<tr><th id="116">116</th><td>    <em>float</em>* <b>__restrict__</b> result);</td></tr>
<tr><th id="117">117</th><td></td></tr>
<tr><th id="118">118</th><td><i>// Matrix multiplication for quantized values using symmetric quantization.</i></td></tr>
<tr><th id="119">119</th><td><i>// Sparse version.</i></td></tr>
<tr><th id="120">120</th><td><em>void</em> NeonSparseMatrixBatchVectorMultiplyAccumulate(</td></tr>
<tr><th id="121">121</th><td>    <em>const</em> int8_t* <b>__restrict__</b> matrix, <em>const</em> uint8_t* ledger, <em>const</em> <em>int</em> m_rows,</td></tr>
<tr><th id="122">122</th><td>    <em>const</em> <em>int</em> m_cols, <em>const</em> int8_t* <b>__restrict__</b> vectors,</td></tr>
<tr><th id="123">123</th><td>    <em>const</em> <em>float</em>* scaling_factors, <em>int</em> n_batch, <em>float</em>* <b>__restrict__</b> result);</td></tr>
<tr><th id="124">124</th><td></td></tr>
<tr><th id="125">125</th><td><i>// Dot product of two vectors.</i></td></tr>
<tr><th id="126">126</th><td><em>float</em> NeonVectorVectorDotProduct(<em>const</em> <em>float</em>* vector1, <em>const</em> <em>float</em>* vector2,</td></tr>
<tr><th id="127">127</th><td>                                 <em>int</em> v_size);</td></tr>
<tr><th id="128">128</th><td></td></tr>
<tr><th id="129">129</th><td><i>// Compute "1.0f - elements of vector" (used in CIFG).</i></td></tr>
<tr><th id="130">130</th><td><em>void</em> NeonSub1Vector(<em>const</em> <em>float</em>* vector, <em>int</em> v_size, <em>float</em>* result);</td></tr>
<tr><th id="131">131</th><td></td></tr>
<tr><th id="132">132</th><td><em>void</em> NeonSub1Vector(<em>const</em> int16_t* vector, <em>int</em> v_size, int16_t* result);</td></tr>
<tr><th id="133">133</th><td></td></tr>
<tr><th id="134">134</th><td><i>// Multiply all elements of vector with a scalar.</i></td></tr>
<tr><th id="135">135</th><td><em>void</em> NeonVectorScalarMultiply(<em>const</em> int8_t* vector, <em>int</em> v_size, <em>float</em> scale,</td></tr>
<tr><th id="136">136</th><td>                              <em>float</em>* result);</td></tr>
<tr><th id="137">137</th><td></td></tr>
<tr><th id="138">138</th><td><i>// Check if all entries of a vector are zero.</i></td></tr>
<tr><th id="139">139</th><td><em>bool</em> NeonIsZeroVector(<em>const</em> <em>float</em>* vector, <em>int</em> v_size);</td></tr>
<tr><th id="140">140</th><td></td></tr>
<tr><th id="141">141</th><td><i>// Check if all entries of a vector are zero.</i></td></tr>
<tr><th id="142">142</th><td><em>bool</em> NeonIsZeroVector(<em>const</em> int8_t* vector, <em>int</em> v_size);</td></tr>
<tr><th id="143">143</th><td></td></tr>
<tr><th id="144">144</th><td><i>// Symmetric quantizer.</i></td></tr>
<tr><th id="145">145</th><td><em>void</em> NeonSymmetricQuantizeFloats(<em>const</em> <em>float</em>* values, <em>const</em> <em>int</em> size,</td></tr>
<tr><th id="146">146</th><td>                                 int8_t* quantized_values, <em>float</em>* min,</td></tr>
<tr><th id="147">147</th><td>                                 <em>float</em>* max, <em>float</em>* scaling_factor);</td></tr>
<tr><th id="148">148</th><td></td></tr>
<tr><th id="149">149</th><td><i>// Symmetric quantizer.</i></td></tr>
<tr><th id="150">150</th><td><em>void</em> NeonSymmetricQuantizeFloats(<em>const</em> <em>float</em>* values, <em>const</em> <em>int</em> size,</td></tr>
<tr><th id="151">151</th><td>                                 int8_t* quantized_values, <em>float</em> min, <em>float</em> max,</td></tr>
<tr><th id="152">152</th><td>                                 <em>float</em>* scaling_factor);</td></tr>
<tr><th id="153">153</th><td></td></tr>
<tr><th id="154">154</th><td><i>// Asymmetric quantizer.</i></td></tr>
<tr><th id="155">155</th><td><em>void</em> NeonAsymmetricQuantizeFloats(<em>const</em> <em>float</em>* values, <em>const</em> <em>int</em> size,</td></tr>
<tr><th id="156">156</th><td>                                  int8_t* quantized_values,</td></tr>
<tr><th id="157">157</th><td>                                  <em>float</em>* scaling_factor, int32_t* offset);</td></tr>
<tr><th id="158">158</th><td></td></tr>
<tr><th id="159">159</th><td><i>// Reduce-sum on a float input vector:</i></td></tr>
<tr><th id="160">160</th><td><i>// input_vector: float pointer to input vector.</i></td></tr>
<tr><th id="161">161</th><td><i>// output_vector: float pointer to vector.</i></td></tr>
<tr><th id="162">162</th><td><i>// output_size: output vector size.</i></td></tr>
<tr><th id="163">163</th><td><i>// reduction_size: number of consecutive elements from input vector which are</i></td></tr>
<tr><th id="164">164</th><td><i>// added to get one element of output.</i></td></tr>
<tr><th id="165">165</th><td><em>void</em> NeonReductionSumVector(<em>const</em> <em>float</em>* input_vector, <em>float</em>* output_vector,</td></tr>
<tr><th id="166">166</th><td>                            <em>int</em> output_size, <em>int</em> reduction_size);</td></tr>
<tr><th id="167">167</th><td></td></tr>
<tr><th id="168">168</th><td><em>void</em> NeonReductionSumVector(<em>const</em> int8_t* input_vector, int32_t* output_vector,</td></tr>
<tr><th id="169">169</th><td>                            <em>int</em> output_size, <em>int</em> reduction_size);</td></tr>
<tr><th id="170">170</th><td></td></tr>
<tr><th id="171">171</th><td><em>void</em> NeonVectorBatchVectorCwiseProductAccumulate(</td></tr>
<tr><th id="172">172</th><td>    <em>const</em> int16_t* vector, <em>int</em> v_size, <em>const</em> int16_t* batch_vector, <em>int</em> n_batch,</td></tr>
<tr><th id="173">173</th><td>    int32_t multiplier, <em>int</em> shift, int16_t* result);</td></tr>
<tr><th id="174">174</th><td></td></tr>
<tr><th id="175">175</th><td><i>// Layer norm for each batch.</i></td></tr>
<tr><th id="176">176</th><td><em>void</em> NeonMeanStddevNormalization(<em>const</em> <em>float</em>* <b>__restrict__</b> input_vector,</td></tr>
<tr><th id="177">177</th><td>                                 <em>float</em>* <b>__restrict__</b> output_vector, <em>int</em> v_size,</td></tr>
<tr><th id="178">178</th><td>                                 <em>int</em> n_batch);</td></tr>
<tr><th id="179">179</th><td></td></tr>
<tr><th id="180">180</th><td><u>#<span data-ppcond="28">endif</span>  // USE_NEON</u></td></tr>
<tr><th id="181">181</th><td></td></tr>
<tr><th id="182">182</th><td>}  <i>// namespace tensor_utils</i></td></tr>
<tr><th id="183">183</th><td>}  <i>// namespace tflite</i></td></tr>
<tr><th id="184">184</th><td></td></tr>
<tr><th id="185">185</th><td><u>#<span data-ppcond="15">endif</span>  // TENSORFLOW_LITE_KERNELS_INTERNAL_OPTIMIZED_NEON_TENSOR_UTILS_IMPL_H_</u></td></tr>
<tr><th id="186">186</th><td></td></tr>
</table><hr/><p id='footer'>
Generated while processing <a href='neon_tensor_utils.cc.html'>halide/build-apps/_deps/tflite-src/tensorflow/lite/kernels/internal/optimized/neon_tensor_utils.cc</a><br/>Generated on <em>2021-Aug-05</em> from project halide revision <em>v12.0.1</em>