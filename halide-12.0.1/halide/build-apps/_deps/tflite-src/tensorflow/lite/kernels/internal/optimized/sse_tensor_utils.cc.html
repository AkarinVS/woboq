<!doctype html>
<html>
<head>
<meta name="viewport" content="width=device-width, initial-scale=1.0"><title>sse_tensor_utils.cc source code [halide/build-apps/_deps/tflite-src/tensorflow/lite/kernels/internal/optimized/sse_tensor_utils.cc] - Woboq Code Browser</title>
<link rel="stylesheet" href="../../../../../../../../.././data/qtcreator.css" title="QtCreator"/>
<link rel="alternate stylesheet" href="../../../../../../../../.././data/kdevelop.css" title="KDevelop"/>
<script type="text/javascript" src="../../../../../../../../.././data/jquery/jquery.min.js"></script>
<script type="text/javascript" src="../../../../../../../../.././data/jquery/jquery-ui.min.js"></script>
<script>var file = 'halide/build-apps/_deps/tflite-src/tensorflow/lite/kernels/internal/optimized/sse_tensor_utils.cc'; var root_path = '../../../../../../../../..'; var data_path = '../../../../../../../../.././data'; var ecma_script_api_version = 2;</script>
<script src='../../../../../../../../.././data/codebrowser.js'></script>
</head>
<body><div id='header'><h1 id='breadcrumb'><span>Browse the source code of </span><a href='../../../../../../../..'>halide</a>/<a href='../../../../../../..'>build-apps</a>/<a href='../../../../../..'>_deps</a>/<a href='../../../../..'>tflite-src</a>/<a href='../../../..'>tensorflow</a>/<a href='../../..'>lite</a>/<a href='../..'>kernels</a>/<a href='..'>internal</a>/<a href='./'>optimized</a>/<a href='sse_tensor_utils.cc.html'>sse_tensor_utils.cc</a></h1></div>
<hr/><div id='content'><table class="code">
<tr><th id="1">1</th><td><i>/* Copyright 2019 The TensorFlow Authors. All Rights Reserved.</i></td></tr>
<tr><th id="2">2</th><td><i></i></td></tr>
<tr><th id="3">3</th><td><i>Licensed under the Apache License, Version 2.0 (the "License");</i></td></tr>
<tr><th id="4">4</th><td><i>you may not use this file except in compliance with the License.</i></td></tr>
<tr><th id="5">5</th><td><i>You may obtain a copy of the License at</i></td></tr>
<tr><th id="6">6</th><td><i></i></td></tr>
<tr><th id="7">7</th><td><i>    <a href="http://www.apache.org/licenses/LICENSE-2.0">http://www.apache.org/licenses/LICENSE-2.0</a></i></td></tr>
<tr><th id="8">8</th><td><i></i></td></tr>
<tr><th id="9">9</th><td><i>Unless required by applicable law or agreed to in writing, software</i></td></tr>
<tr><th id="10">10</th><td><i>distributed under the License is distributed on an "AS IS" BASIS,</i></td></tr>
<tr><th id="11">11</th><td><i>WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</i></td></tr>
<tr><th id="12">12</th><td><i>See the License for the specific language governing permissions and</i></td></tr>
<tr><th id="13">13</th><td><i>limitations under the License.</i></td></tr>
<tr><th id="14">14</th><td><i>==============================================================================*/</i></td></tr>
<tr><th id="15">15</th><td><u>#include <a href="sse_tensor_utils_impl.h.html">"tensorflow/lite/kernels/internal/optimized/sse_tensor_utils_impl.h"</a></u></td></tr>
<tr><th id="16">16</th><td></td></tr>
<tr><th id="17">17</th><td><u>#<span data-ppcond="17">ifdef</span> <span class="macro" data-ref="_M/__SSSE3__">__SSSE3__</span></u></td></tr>
<tr><th id="18">18</th><td></td></tr>
<tr><th id="19">19</th><td><u>#include &lt;emmintrin.h&gt;  // SSE2</u></td></tr>
<tr><th id="20">20</th><td><u>#include &lt;tmmintrin.h&gt;  // SSSE3</u></td></tr>
<tr><th id="21">21</th><td><u>#ifdef __SSE4_1__</u></td></tr>
<tr><th id="22">22</th><td><u>#include &lt;smmintrin.h&gt;  // SSE4.1</u></td></tr>
<tr><th id="23">23</th><td><u>#endif</u></td></tr>
<tr><th id="24">24</th><td></td></tr>
<tr><th id="25">25</th><td><u>#include &lt;cstdint&gt;</u></td></tr>
<tr><th id="26">26</th><td></td></tr>
<tr><th id="27">27</th><td><u>#include "ruy/profiler/instrumentation.h"  // from @ruy</u></td></tr>
<tr><th id="28">28</th><td><u>#include "tensorflow/lite/kernels/cpu_backend_context.h"</u></td></tr>
<tr><th id="29">29</th><td><u>#include "tensorflow/lite/kernels/cpu_backend_gemm.h"</u></td></tr>
<tr><th id="30">30</th><td><u>#include "tensorflow/lite/kernels/cpu_backend_gemm_params.h"</u></td></tr>
<tr><th id="31">31</th><td><u>#include "tensorflow/lite/kernels/internal/compatibility.h"</u></td></tr>
<tr><th id="32">32</th><td></td></tr>
<tr><th id="33">33</th><td><b>namespace</b> tflite {</td></tr>
<tr><th id="34">34</th><td><b>namespace</b> tensor_utils {</td></tr>
<tr><th id="35">35</th><td><b>namespace</b> {</td></tr>
<tr><th id="36">36</th><td></td></tr>
<tr><th id="37">37</th><td><i>// Dot product of four int8 vectors of 4 elements packed into a XMM register.</i></td></tr>
<tr><th id="38">38</th><td><i>// Result is four int32 scalars packed into a XMM register.</i></td></tr>
<tr><th id="39">39</th><td><i>// int8x4x4 · int8x4x4 =&gt; int32x4</i></td></tr>
<tr><th id="40">40</th><td><em>static</em> <b>inline</b> __m128i DotProdInt8x4x4(__m128i a_8x16, __m128i b_8x16) {</td></tr>
<tr><th id="41">41</th><td>  <i>// Transfer sign from 'a' to 'b', as _mm_maddubs_epi16 treats 'a' unsigned.</i></td></tr>
<tr><th id="42">42</th><td>  b_8x16 = _mm_sign_epi8(b_8x16, a_8x16);</td></tr>
<tr><th id="43">43</th><td>  a_8x16 = _mm_abs_epi8(a_8x16);</td></tr>
<tr><th id="44">44</th><td>  <i>// sumprod[i] = a[2*i]*b[2*i] + a[2*i+1]*b[2*i+1] (i = 0..7)</i></td></tr>
<tr><th id="45">45</th><td>  __m128i sumprod_16x8 = _mm_maddubs_epi16(a_8x16, b_8x16);</td></tr>
<tr><th id="46">46</th><td>  <i>// sumprod[i] = sumprod[2*i]*1 + sumprod[2*i+1]*1 (i = 0..3)</i></td></tr>
<tr><th id="47">47</th><td>  <b>return</b> _mm_madd_epi16(sumprod_16x8, _mm_set1_epi16(<var>1</var>));</td></tr>
<tr><th id="48">48</th><td>}</td></tr>
<tr><th id="49">49</th><td></td></tr>
<tr><th id="50">50</th><td><i>// Horizontally add 4 int32 values stored in a single XMM register to int32_t.</i></td></tr>
<tr><th id="51">51</th><td><em>static</em> <b>inline</b> int32_t ReduceInt32x4(__m128i acc) {</td></tr>
<tr><th id="52">52</th><td>  <i>// Shuffle to contain high half of acc (both in high and low halfs).</i></td></tr>
<tr><th id="53">53</th><td>  __m128i shuffle = _mm_unpackhi_epi64(acc, acc);</td></tr>
<tr><th id="54">54</th><td>  <i>// Add shuffle and acc; low half is sums of twos (high half is ignored).</i></td></tr>
<tr><th id="55">55</th><td>  acc = _mm_add_epi32(acc, shuffle);</td></tr>
<tr><th id="56">56</th><td>  <i>// Shuffle the two elements in low half (ignore high half).</i></td></tr>
<tr><th id="57">57</th><td>  shuffle = _mm_shuffle_epi32(acc, _MM_SHUFFLE(<var>2</var>, <var>3</var>, <var>0</var>, <var>1</var>));</td></tr>
<tr><th id="58">58</th><td>  <i>// Add shuffle and acc; lowest element is sum of all 4 input.</i></td></tr>
<tr><th id="59">59</th><td>  acc = _mm_add_epi32(acc, shuffle);</td></tr>
<tr><th id="60">60</th><td>  <i>// Return lowest element as int32_t.</i></td></tr>
<tr><th id="61">61</th><td>  <b>return</b> _mm_cvtsi128_si32(acc);</td></tr>
<tr><th id="62">62</th><td>}</td></tr>
<tr><th id="63">63</th><td></td></tr>
<tr><th id="64">64</th><td><i>// Horizontally add each of 4 XMM registers with 4 int32 values, pack result</i></td></tr>
<tr><th id="65">65</th><td><i>// into a single XMM register. Similar to ReduceInt32x4, but with 4x inputs.</i></td></tr>
<tr><th id="66">66</th><td><em>static</em> <b>inline</b> __m128i ReduceInt32x4x4(__m128i a, __m128i b, __m128i c,</td></tr>
<tr><th id="67">67</th><td>                                      __m128i d) {</td></tr>
<tr><th id="68">68</th><td>  <i>// Assuming x = [x0, x1, x2, x3]</i></td></tr>
<tr><th id="69">69</th><td>  <em>const</em> __m128i a_b_lo_half = _mm_unpacklo_epi32(a, b);  <i>// [a0, b0, a1, b1]</i></td></tr>
<tr><th id="70">70</th><td>  <em>const</em> __m128i a_b_hi_half = _mm_unpackhi_epi32(a, b);  <i>// [a2, b2, a3, b3]</i></td></tr>
<tr><th id="71">71</th><td>  <em>const</em> __m128i a_plus_b =</td></tr>
<tr><th id="72">72</th><td>      _mm_add_epi32(a_b_lo_half, a_b_hi_half);  <i>// [a0+a2, b0+b2, a1+a3, b1+b3]</i></td></tr>
<tr><th id="73">73</th><td>  <em>const</em> __m128i c_d_lo_half = _mm_unpacklo_epi32(c, d);  <i>// [c0, d0, c1, d1]</i></td></tr>
<tr><th id="74">74</th><td>  <em>const</em> __m128i c_d_hi_half = _mm_unpackhi_epi32(c, d);  <i>// [c2, d2, c3, d3]</i></td></tr>
<tr><th id="75">75</th><td>  <em>const</em> __m128i c_plus_d =</td></tr>
<tr><th id="76">76</th><td>      _mm_add_epi32(c_d_lo_half, c_d_hi_half);  <i>// [c0+c2, d0+d2, c1+c3, d1+d3]</i></td></tr>
<tr><th id="77">77</th><td>  <em>const</em> __m128i all_evns =</td></tr>
<tr><th id="78">78</th><td>      _mm_unpacklo_epi64(a_plus_b, c_plus_d);  <i>// [a02, b02, c02, d02]</i></td></tr>
<tr><th id="79">79</th><td>  <em>const</em> __m128i all_odds =</td></tr>
<tr><th id="80">80</th><td>      _mm_unpackhi_epi64(a_plus_b, c_plus_d);  <i>// [a13, b13, c13, d13]</i></td></tr>
<tr><th id="81">81</th><td>  <b>return</b> _mm_add_epi32(all_evns, all_odds);    <i>// [a0123, b0123, c0123, d0123]</i></td></tr>
<tr><th id="82">82</th><td>}</td></tr>
<tr><th id="83">83</th><td></td></tr>
<tr><th id="84">84</th><td><i>// Returns the ith element of a XMM register holding float numbers.</i></td></tr>
<tr><th id="85">85</th><td><b>template</b> &lt;<em>int</em> i&gt;</td></tr>
<tr><th id="86">86</th><td><em>float</em> GetFloatVectorElement(__m128 v) {</td></tr>
<tr><th id="87">87</th><td>  <b>static_assert</b>(i &gt;= <var>0</var> &amp;&amp; i &lt; <var>4</var>, <q>"The index must be 0 &lt;= i &lt; 4."</q>);</td></tr>
<tr><th id="88">88</th><td>  <i>// Note, _mm_extract_ps returns int, so we can't use it here.</i></td></tr>
<tr><th id="89">89</th><td><i>  // These lines will be optimized to extractps anyway.</i></td></tr>
<tr><th id="90">90</th><td>  v = _mm_shuffle_ps(v, v, _MM_SHUFFLE(i, i, i, i));</td></tr>
<tr><th id="91">91</th><td>  <b>return</b> _mm_cvtss_f32(v);</td></tr>
<tr><th id="92">92</th><td>}</td></tr>
<tr><th id="93">93</th><td></td></tr>
<tr><th id="94">94</th><td>}  <i>// namespace</i></td></tr>
<tr><th id="95">95</th><td></td></tr>
<tr><th id="96">96</th><td><em>void</em> SseMatrixBatchVectorMultiplyAccumulateImpl(</td></tr>
<tr><th id="97">97</th><td>    <em>const</em> int8_t* <b>__restrict__</b> matrix, <em>const</em> <em>int</em> m_rows, <em>const</em> <em>int</em> m_cols,</td></tr>
<tr><th id="98">98</th><td>    <em>const</em> int8_t* <b>__restrict__</b> vectors,</td></tr>
<tr><th id="99">99</th><td>    <em>const</em> <em>float</em>* <b>__restrict__</b> scaling_factors, <em>int</em> n_batch,</td></tr>
<tr><th id="100">100</th><td>    <em>float</em>* <b>__restrict__</b> result, <em>const</em> <em>float</em>* per_channel_scale,</td></tr>
<tr><th id="101">101</th><td>    <em>const</em> int32_t* input_offset, <em>const</em> int32_t* row_sums) {</td></tr>
<tr><th id="102">102</th><td>  <b>for</b> (std::intptr_t batch = <var>0</var>; batch &lt; n_batch; ++batch) {</td></tr>
<tr><th id="103">103</th><td>    <em>const</em> <em>float</em> batch_scaling_factor = scaling_factors[batch];</td></tr>
<tr><th id="104">104</th><td>    <em>const</em> int32_t batch_offset = input_offset ? input_offset[batch] : <var>0</var>;</td></tr>
<tr><th id="105">105</th><td>    <i>// Compute dot-product for every column.</i></td></tr>
<tr><th id="106">106</th><td>    <b>for</b> (std::intptr_t row = <var>0</var>; row &lt; m_rows; ++row) {</td></tr>
<tr><th id="107">107</th><td>      <i>// Get the address of the first element of the row.</i></td></tr>
<tr><th id="108">108</th><td>      <em>const</em> int8_t* <b>__restrict__</b> row_ptr = matrix + row * m_cols;</td></tr>
<tr><th id="109">109</th><td>      <em>const</em> <em>float</em> row_scale =</td></tr>
<tr><th id="110">110</th><td>          per_channel_scale ? per_channel_scale[row] * batch_scaling_factor</td></tr>
<tr><th id="111">111</th><td>                            : batch_scaling_factor;</td></tr>
<tr><th id="112">112</th><td>      <em>const</em> int32_t row_offset =</td></tr>
<tr><th id="113">113</th><td>          row_sums &amp;&amp; batch_offset ? batch_offset * row_sums[row] : <var>0</var>;</td></tr>
<tr><th id="114">114</th><td>      <i>// Initialize the dot product sum for the row to 0.</i></td></tr>
<tr><th id="115">115</th><td>      __m128i dotprod_32x4 = _mm_setzero_si128();</td></tr>
<tr><th id="116">116</th><td>      std::intptr_t col = <var>0</var>;</td></tr>
<tr><th id="117">117</th><td>      <i>// For every block of 16x 8-bit inputs.</i></td></tr>
<tr><th id="118">118</th><td>      <b>while</b> (col &lt; (m_cols &amp; ~<var>15</var>)) {</td></tr>
<tr><th id="119">119</th><td>        <em>const</em> __m128i vec_8x16 =</td></tr>
<tr><th id="120">120</th><td>            _mm_loadu_si128(<b>reinterpret_cast</b>&lt;<em>const</em> __m128i*&gt;(vectors + col));</td></tr>
<tr><th id="121">121</th><td>        <em>const</em> __m128i row_8x16 =</td></tr>
<tr><th id="122">122</th><td>            _mm_loadu_si128(<b>reinterpret_cast</b>&lt;<em>const</em> __m128i*&gt;(row_ptr + col));</td></tr>
<tr><th id="123">123</th><td>        <i>// dotprod += vec · row</i></td></tr>
<tr><th id="124">124</th><td>        dotprod_32x4 =</td></tr>
<tr><th id="125">125</th><td>            _mm_add_epi32(dotprod_32x4, DotProdInt8x4x4(vec_8x16, row_8x16));</td></tr>
<tr><th id="126">126</th><td>        col += <var>16</var>;</td></tr>
<tr><th id="127">127</th><td>      }</td></tr>
<tr><th id="128">128</th><td><u>#ifdef __SSE4_1__</u></td></tr>
<tr><th id="129">129</th><td>      <i>// Postamble for 8x 8-bit inputs.</i></td></tr>
<tr><th id="130">130</th><td>      <b>if</b> (col &lt; (m_cols &amp; ~<var>7</var>)) {</td></tr>
<tr><th id="131">131</th><td>        <em>const</em> __m128i vec_16x8 = _mm_cvtepi8_epi16(</td></tr>
<tr><th id="132">132</th><td>            _mm_loadl_epi64(<b>reinterpret_cast</b>&lt;<em>const</em> __m128i*&gt;(vectors + col)));</td></tr>
<tr><th id="133">133</th><td>        <em>const</em> __m128i row_16x8 = _mm_cvtepi8_epi16(</td></tr>
<tr><th id="134">134</th><td>            _mm_loadl_epi64(<b>reinterpret_cast</b>&lt;<em>const</em> __m128i*&gt;(row_ptr + col)));</td></tr>
<tr><th id="135">135</th><td>        <i>// dotprod += vec · row</i></td></tr>
<tr><th id="136">136</th><td>        dotprod_32x4 =</td></tr>
<tr><th id="137">137</th><td>            _mm_add_epi32(dotprod_32x4, _mm_madd_epi16(vec_16x8, row_16x8));</td></tr>
<tr><th id="138">138</th><td>        col += <var>8</var>;</td></tr>
<tr><th id="139">139</th><td>      }</td></tr>
<tr><th id="140">140</th><td>      <i>// Postamble for 4x 8-bit inputs.</i></td></tr>
<tr><th id="141">141</th><td>      <b>if</b> (col &lt; (m_cols &amp; ~<var>3</var>)) {</td></tr>
<tr><th id="142">142</th><td>        <em>const</em> __m128i vec_32x4 = _mm_cvtepi8_epi32(</td></tr>
<tr><th id="143">143</th><td>            _mm_loadl_epi64(<b>reinterpret_cast</b>&lt;<em>const</em> __m128i*&gt;(vectors + col)));</td></tr>
<tr><th id="144">144</th><td>        <em>const</em> __m128i row_32x4 = _mm_cvtepi8_epi32(</td></tr>
<tr><th id="145">145</th><td>            _mm_loadl_epi64(<b>reinterpret_cast</b>&lt;<em>const</em> __m128i*&gt;(row_ptr + col)));</td></tr>
<tr><th id="146">146</th><td>        <i>// dotprod += vec · row</i></td></tr>
<tr><th id="147">147</th><td>        dotprod_32x4 =</td></tr>
<tr><th id="148">148</th><td>            _mm_add_epi32(dotprod_32x4, _mm_mullo_epi32(vec_32x4, row_32x4));</td></tr>
<tr><th id="149">149</th><td>        col += <var>4</var>;</td></tr>
<tr><th id="150">150</th><td>      }</td></tr>
<tr><th id="151">151</th><td><u>#endif</u></td></tr>
<tr><th id="152">152</th><td></td></tr>
<tr><th id="153">153</th><td>      <i>// Horizontally add the 4 intermediate sum values to get the final</i></td></tr>
<tr><th id="154">154</th><td><i>      // dot-prod value for this row.</i></td></tr>
<tr><th id="155">155</th><td>      int32_t sum = ReduceInt32x4(dotprod_32x4);</td></tr>
<tr><th id="156">156</th><td></td></tr>
<tr><th id="157">157</th><td><u>#if defined(__SSE4_1__) &amp;&amp; defined(__clang__)</u></td></tr>
<tr><th id="158">158</th><td>      <i>// SSE 4.1: Don't try to unroll and vectorize this, already done above.</i></td></tr>
<tr><th id="159">159</th><td><u>#pragma clang loop unroll(disable) vectorize(disable)</u></td></tr>
<tr><th id="160">160</th><td><u>#endif</u></td></tr>
<tr><th id="161">161</th><td>      <i>// Postamble loop for &lt;4x (&lt;16x without SSE 4.1) remaining 8-bit inputs.</i></td></tr>
<tr><th id="162">162</th><td>      <b>for</b> (; col &lt; m_cols; ++col) {</td></tr>
<tr><th id="163">163</th><td>        sum += row_ptr[col] * vectors[col];</td></tr>
<tr><th id="164">164</th><td>      }  <i>// for col</i></td></tr>
<tr><th id="165">165</th><td>      <b>if</b> (row_offset) {</td></tr>
<tr><th id="166">166</th><td>        sum -= row_offset;</td></tr>
<tr><th id="167">167</th><td>      }</td></tr>
<tr><th id="168">168</th><td>      *result += sum * row_scale;</td></tr>
<tr><th id="169">169</th><td>      ++result;</td></tr>
<tr><th id="170">170</th><td>    }  <i>// for row</i></td></tr>
<tr><th id="171">171</th><td></td></tr>
<tr><th id="172">172</th><td>    vectors += m_cols;</td></tr>
<tr><th id="173">173</th><td>  }  <i>// for batch</i></td></tr>
<tr><th id="174">174</th><td>}</td></tr>
<tr><th id="175">175</th><td></td></tr>
<tr><th id="176">176</th><td><em>void</em> SseCpuBackendGemm(<em>const</em> int8_t* input, <em>const</em> int32_t* bias,</td></tr>
<tr><th id="177">177</th><td>                       <em>const</em> int8_t* input_to_gate_weights, int32_t n_batch,</td></tr>
<tr><th id="178">178</th><td>                       int32_t n_input, int32_t n_output, int32_t output_zp,</td></tr>
<tr><th id="179">179</th><td>                       int32_t* scratch, CpuBackendContext* context) {</td></tr>
<tr><th id="180">180</th><td>  <b>using</b> ::tflite::cpu_backend_gemm::Gemm;</td></tr>
<tr><th id="181">181</th><td>  <b>using</b> ::tflite::cpu_backend_gemm::GemmParams;</td></tr>
<tr><th id="182">182</th><td>  <b>using</b> ::tflite::cpu_backend_gemm::MatrixParams;</td></tr>
<tr><th id="183">183</th><td></td></tr>
<tr><th id="184">184</th><td>  MatrixParams&lt;int8_t&gt; lhs_params;</td></tr>
<tr><th id="185">185</th><td>  lhs_params.order = cpu_backend_gemm::Order::kRowMajor;</td></tr>
<tr><th id="186">186</th><td>  lhs_params.rows = n_output;</td></tr>
<tr><th id="187">187</th><td>  lhs_params.cols = n_input;</td></tr>
<tr><th id="188">188</th><td>  lhs_params.cache_policy = cpu_backend_gemm::CachePolicy::kCacheIfLargeSpeedup;</td></tr>
<tr><th id="189">189</th><td></td></tr>
<tr><th id="190">190</th><td>  MatrixParams&lt;int8_t&gt; rhs_params;</td></tr>
<tr><th id="191">191</th><td>  rhs_params.order = cpu_backend_gemm::Order::kColMajor;</td></tr>
<tr><th id="192">192</th><td>  rhs_params.rows = n_input;</td></tr>
<tr><th id="193">193</th><td>  rhs_params.cols = n_batch;</td></tr>
<tr><th id="194">194</th><td></td></tr>
<tr><th id="195">195</th><td>  MatrixParams&lt;int32_t&gt; dst_params;</td></tr>
<tr><th id="196">196</th><td>  dst_params.order = cpu_backend_gemm::Order::kColMajor;</td></tr>
<tr><th id="197">197</th><td>  dst_params.rows = n_output;</td></tr>
<tr><th id="198">198</th><td>  dst_params.cols = n_batch;</td></tr>
<tr><th id="199">199</th><td></td></tr>
<tr><th id="200">200</th><td>  GemmParams&lt;int32, int32&gt; gemm_params;</td></tr>
<tr><th id="201">201</th><td>  <b>if</b> (bias) {</td></tr>
<tr><th id="202">202</th><td>    gemm_params.bias = bias;</td></tr>
<tr><th id="203">203</th><td>  }</td></tr>
<tr><th id="204">204</th><td>  cpu_backend_gemm::Gemm(lhs_params, input_to_gate_weights, rhs_params, input,</td></tr>
<tr><th id="205">205</th><td>                         dst_params, scratch, gemm_params, context);</td></tr>
<tr><th id="206">206</th><td>}</td></tr>
<tr><th id="207">207</th><td></td></tr>
<tr><th id="208">208</th><td><em>void</em> SseMatrixBatchVectorMultiplyAccumulate(</td></tr>
<tr><th id="209">209</th><td>    <em>const</em> int8_t* <b>__restrict__</b> matrix, <em>const</em> <em>int</em> m_rows, <em>const</em> <em>int</em> m_cols,</td></tr>
<tr><th id="210">210</th><td>    <em>const</em> int8_t* <b>__restrict__</b> vectors,</td></tr>
<tr><th id="211">211</th><td>    <em>const</em> <em>float</em>* <b>__restrict__</b> scaling_factors, <em>int</em> n_batch,</td></tr>
<tr><th id="212">212</th><td>    <em>float</em>* <b>__restrict__</b> result) {</td></tr>
<tr><th id="213">213</th><td>  SseMatrixBatchVectorMultiplyAccumulateImpl(</td></tr>
<tr><th id="214">214</th><td>      matrix, m_rows, m_cols, vectors, scaling_factors, n_batch, result,</td></tr>
<tr><th id="215">215</th><td>      <i>/*per_channel_scale=*/</i><b>nullptr</b>, <i>/*input_offset=*/</i><b>nullptr</b>,</td></tr>
<tr><th id="216">216</th><td>      <i>/*row_sums=*/</i><b>nullptr</b>);</td></tr>
<tr><th id="217">217</th><td>}</td></tr>
<tr><th id="218">218</th><td></td></tr>
<tr><th id="219">219</th><td><em>void</em> SseMatrixBatchVectorMultiplyAccumulate(</td></tr>
<tr><th id="220">220</th><td>    <em>const</em> int8_t* <b>__restrict__</b> matrix, <em>const</em> <em>int</em> m_rows, <em>const</em> <em>int</em> m_cols,</td></tr>
<tr><th id="221">221</th><td>    <em>const</em> int8_t* <b>__restrict__</b> vectors,</td></tr>
<tr><th id="222">222</th><td>    <em>const</em> <em>float</em>* <b>__restrict__</b> scaling_factors, <em>int</em> n_batch, int32_t* scratch,</td></tr>
<tr><th id="223">223</th><td>    <em>float</em>* <b>__restrict__</b> result, CpuBackendContext* context) {</td></tr>
<tr><th id="224">224</th><td>  <i>// TODO(b/183178387): Use a proper query to detect AVX/optimized paths.</i></td></tr>
<tr><th id="225">225</th><td>  <b>if</b> (m_rows % <var>4</var> == <var>0</var> &amp;&amp; !context-&gt;PreferGemmlowpOnX86()) {</td></tr>
<tr><th id="226">226</th><td>    <em>const</em> int32_t* bias = <b>static_cast</b>&lt;<em>const</em> int32_t*&gt;(<b>nullptr</b>);</td></tr>
<tr><th id="227">227</th><td>    SseCpuBackendGemm(vectors, bias, matrix, n_batch, m_cols, m_rows,</td></tr>
<tr><th id="228">228</th><td>                      <i>/*output_zp=*/</i><var>0</var>, scratch, context);</td></tr>
<tr><th id="229">229</th><td></td></tr>
<tr><th id="230">230</th><td>    {</td></tr>
<tr><th id="231">231</th><td>      ruy::profiler::ScopeLabel label(<q>"HybridMultiplyScalingFactor"</q>);</td></tr>
<tr><th id="232">232</th><td>      <i>// Multiply by float scaling factors and write to result</i></td></tr>
<tr><th id="233">233</th><td>      <em>const</em> <em>int</em> total_size = n_batch * m_rows;</td></tr>
<tr><th id="234">234</th><td>      <em>int</em> i = <var>0</var>;</td></tr>
<tr><th id="235">235</th><td>      <b>for</b> (; i &lt;= total_size - <var>8</var>; i += <var>8</var>, result += <var>8</var>) {</td></tr>
<tr><th id="236">236</th><td>        <em>const</em> <em>float</em> batch_scaling_factor0 = scaling_factors[i / m_rows];</td></tr>
<tr><th id="237">237</th><td>        <em>const</em> <em>float</em> batch_scaling_factor1 = scaling_factors[(i + <var>4</var>) / m_rows];</td></tr>
<tr><th id="238">238</th><td>        <em>const</em> __m128 scaling_factor0 = _mm_set1_ps(batch_scaling_factor0);</td></tr>
<tr><th id="239">239</th><td>        <em>const</em> __m128 scaling_factor1 = _mm_set1_ps(batch_scaling_factor1);</td></tr>
<tr><th id="240">240</th><td>        <em>const</em> __m128i scratch_val0 =</td></tr>
<tr><th id="241">241</th><td>            _mm_loadu_si128(<b>reinterpret_cast</b>&lt;<em>const</em> __m128i*&gt;(scratch + i));</td></tr>
<tr><th id="242">242</th><td>        <em>const</em> __m128i scratch_val1 =</td></tr>
<tr><th id="243">243</th><td>            _mm_loadu_si128(<b>reinterpret_cast</b>&lt;<em>const</em> __m128i*&gt;(scratch + i + <var>4</var>));</td></tr>
<tr><th id="244">244</th><td>        <em>const</em> __m128 float_val0 = _mm_cvtepi32_ps(scratch_val0);</td></tr>
<tr><th id="245">245</th><td>        <em>const</em> __m128 float_val1 = _mm_cvtepi32_ps(scratch_val1);</td></tr>
<tr><th id="246">246</th><td>        <em>const</em> __m128 prod0 = _mm_mul_ps(float_val0, scaling_factor0);</td></tr>
<tr><th id="247">247</th><td>        <em>const</em> __m128 result0 = _mm_add_ps(_mm_load1_ps(result), prod0);</td></tr>
<tr><th id="248">248</th><td>        <em>const</em> __m128 prod1 = _mm_mul_ps(float_val1, scaling_factor1);</td></tr>
<tr><th id="249">249</th><td>        <em>const</em> __m128 result1 = _mm_add_ps(_mm_load1_ps(result + <var>4</var>), prod1);</td></tr>
<tr><th id="250">250</th><td>        _mm_store_ps(result, result0);</td></tr>
<tr><th id="251">251</th><td>        _mm_store_ps(result + <var>4</var>, result1);</td></tr>
<tr><th id="252">252</th><td>      }</td></tr>
<tr><th id="253">253</th><td>      scratch += i;</td></tr>
<tr><th id="254">254</th><td>      <b>for</b> (; i &lt; total_size; i++) {</td></tr>
<tr><th id="255">255</th><td>        <em>const</em> <em>float</em> batch_scaling_factor = scaling_factors[i / m_rows];</td></tr>
<tr><th id="256">256</th><td>        int32_t x = *(scratch++);</td></tr>
<tr><th id="257">257</th><td>        *result += x * batch_scaling_factor;</td></tr>
<tr><th id="258">258</th><td>        ++result;</td></tr>
<tr><th id="259">259</th><td>      }</td></tr>
<tr><th id="260">260</th><td>    }</td></tr>
<tr><th id="261">261</th><td>    <b>return</b>;</td></tr>
<tr><th id="262">262</th><td>  }</td></tr>
<tr><th id="263">263</th><td></td></tr>
<tr><th id="264">264</th><td>  SseMatrixBatchVectorMultiplyAccumulateImpl(</td></tr>
<tr><th id="265">265</th><td>      matrix, m_rows, m_cols, vectors, scaling_factors, n_batch, result,</td></tr>
<tr><th id="266">266</th><td>      <i>/*per_channel_scale=*/</i><b>nullptr</b>, <i>/*input_offset=*/</i><b>nullptr</b>,</td></tr>
<tr><th id="267">267</th><td>      <i>/*row_sums=*/</i><b>nullptr</b>);</td></tr>
<tr><th id="268">268</th><td>}</td></tr>
<tr><th id="269">269</th><td></td></tr>
<tr><th id="270">270</th><td><em>void</em> SseMatrixBatchVectorMultiplyAccumulate(</td></tr>
<tr><th id="271">271</th><td>    <em>const</em> int8_t* <b>__restrict__</b> matrix, <em>const</em> <em>int</em> m_rows, <em>const</em> <em>int</em> m_cols,</td></tr>
<tr><th id="272">272</th><td>    <em>const</em> int8_t* <b>__restrict__</b> vectors,</td></tr>
<tr><th id="273">273</th><td>    <em>const</em> <em>float</em>* <b>__restrict__</b> scaling_factors, <em>int</em> n_batch,</td></tr>
<tr><th id="274">274</th><td>    <em>float</em>* <b>__restrict__</b> result, <em>const</em> <em>float</em>* per_channel_scale,</td></tr>
<tr><th id="275">275</th><td>    <em>const</em> int32_t* input_offset, int32_t* scratch, int32_t* row_sums,</td></tr>
<tr><th id="276">276</th><td>    <em>bool</em>* compute_row_sums, CpuBackendContext* context) {</td></tr>
<tr><th id="277">277</th><td>  <b>if</b> ((input_offset != <b>nullptr</b>) &amp;&amp; (!compute_row_sums || *compute_row_sums)) {</td></tr>
<tr><th id="278">278</th><td>    SseReductionSumVector(matrix, row_sums, m_rows, m_cols);</td></tr>
<tr><th id="279">279</th><td>    <b>if</b> (compute_row_sums) {</td></tr>
<tr><th id="280">280</th><td>      *compute_row_sums = <b>false</b>;</td></tr>
<tr><th id="281">281</th><td>    }</td></tr>
<tr><th id="282">282</th><td>  }</td></tr>
<tr><th id="283">283</th><td>  SseMatrixBatchVectorMultiplyAccumulateImpl(</td></tr>
<tr><th id="284">284</th><td>      matrix, m_rows, m_cols, vectors, scaling_factors, n_batch, result,</td></tr>
<tr><th id="285">285</th><td>      per_channel_scale, input_offset, row_sums);</td></tr>
<tr><th id="286">286</th><td>}</td></tr>
<tr><th id="287">287</th><td></td></tr>
<tr><th id="288">288</th><td><b>namespace</b> {</td></tr>
<tr><th id="289">289</th><td></td></tr>
<tr><th id="290">290</th><td><i>// Implements sparse-matrix - vector multiply-accumulate.</i></td></tr>
<tr><th id="291">291</th><td><b>inline</b> <em>void</em> SseSparseMatrixVectorMultiplyAccumulate(</td></tr>
<tr><th id="292">292</th><td>    <em>const</em> int8_t* <b>__restrict__</b> matrix, <em>const</em> uint8_t* <b>__restrict__</b> ledger,</td></tr>
<tr><th id="293">293</th><td>    <em>const</em> <em>int</em> m_rows, <em>const</em> <em>int</em> m_cols, <em>const</em> int8_t* <b>__restrict__</b> vector,</td></tr>
<tr><th id="294">294</th><td>    <em>const</em> <em>float</em> scaling_factor, <em>float</em>* <b>__restrict__</b> result) {</td></tr>
<tr><th id="295">295</th><td>  <em>static</em> <em>const</em> std::intptr_t kBlockSize = <var>16</var>;</td></tr>
<tr><th id="296">296</th><td>  TFLITE_DCHECK_EQ(m_cols % kBlockSize, <var>0</var>);</td></tr>
<tr><th id="297">297</th><td>  <em>const</em> uint8_t* <b>__restrict__</b> ledger_ptr = ledger;</td></tr>
<tr><th id="298">298</th><td>  <b>for</b> (std::intptr_t row = <var>0</var>; row &lt; m_rows; ++row) {</td></tr>
<tr><th id="299">299</th><td>    <i>// Initialize the dot product sum for the row to 0.</i></td></tr>
<tr><th id="300">300</th><td>    __m128i dotprod_32x4 = _mm_setzero_si128();</td></tr>
<tr><th id="301">301</th><td>    std::intptr_t num_nonzero_blocks = *ledger_ptr++;</td></tr>
<tr><th id="302">302</th><td>    <b>for</b> (std::intptr_t i = <var>0</var>; i &lt; num_nonzero_blocks; i++) {</td></tr>
<tr><th id="303">303</th><td>      <em>const</em> std::intptr_t col_index = *ledger_ptr++ * kBlockSize;</td></tr>
<tr><th id="304">304</th><td>      <em>const</em> __m128i vec_8x16 =</td></tr>
<tr><th id="305">305</th><td>          _mm_loadu_si128(<b>reinterpret_cast</b>&lt;<em>const</em> __m128i*&gt;(vector + col_index));</td></tr>
<tr><th id="306">306</th><td>      <em>const</em> __m128i row_8x16 =</td></tr>
<tr><th id="307">307</th><td>          _mm_loadu_si128(<b>reinterpret_cast</b>&lt;<em>const</em> __m128i*&gt;(matrix));</td></tr>
<tr><th id="308">308</th><td>      <i>// dotprod += vec · row</i></td></tr>
<tr><th id="309">309</th><td>      dotprod_32x4 =</td></tr>
<tr><th id="310">310</th><td>          _mm_add_epi32(dotprod_32x4, DotProdInt8x4x4(vec_8x16, row_8x16));</td></tr>
<tr><th id="311">311</th><td>      matrix += kBlockSize;</td></tr>
<tr><th id="312">312</th><td>    }  <i>// for col</i></td></tr>
<tr><th id="313">313</th><td>    <i>// Horizontally add the 4 intermediate sum values to get the final</i></td></tr>
<tr><th id="314">314</th><td><i>    // dot-prod value for this row.</i></td></tr>
<tr><th id="315">315</th><td>    int32_t dotprod = ReduceInt32x4(dotprod_32x4);</td></tr>
<tr><th id="316">316</th><td></td></tr>
<tr><th id="317">317</th><td>    result[row] += dotprod * scaling_factor;</td></tr>
<tr><th id="318">318</th><td>  }  <i>// for row</i></td></tr>
<tr><th id="319">319</th><td>}</td></tr>
<tr><th id="320">320</th><td></td></tr>
<tr><th id="321">321</th><td><i>// Implements sparse-matrix - batch-of-4-vectors multiply-accumulate.</i></td></tr>
<tr><th id="322">322</th><td><i>// The stride between vectors and results must be equal to m_cols.</i></td></tr>
<tr><th id="323">323</th><td><i>// Parameter 'batch' is the index of the first batch, must be a multiple of 4.</i></td></tr>
<tr><th id="324">324</th><td><b>inline</b> <em>void</em> SseSparseMatrix4VectorsMultiplyAccumulate(</td></tr>
<tr><th id="325">325</th><td>    <em>const</em> int8_t* <b>__restrict__</b> matrix, <em>const</em> uint8_t* <b>__restrict__</b> ledger,</td></tr>
<tr><th id="326">326</th><td>    <em>const</em> <em>int</em> m_rows, <em>const</em> <em>int</em> m_cols,</td></tr>
<tr><th id="327">327</th><td>    <em>const</em> int8_t* <b>__restrict__</b> <em>const</em> vectors, <em>const</em> __m128 scaling_factors_fx4,</td></tr>
<tr><th id="328">328</th><td>    <em>float</em>* <b>__restrict__</b> <em>const</em> results) {</td></tr>
<tr><th id="329">329</th><td>  <em>static</em> <em>const</em> std::intptr_t kBlockSize = <var>16</var>;</td></tr>
<tr><th id="330">330</th><td>  TFLITE_DCHECK_EQ(m_cols % kBlockSize, <var>0</var>);</td></tr>
<tr><th id="331">331</th><td></td></tr>
<tr><th id="332">332</th><td>  <em>const</em> int8_t* <b>__restrict__</b> vector0 = vectors + <var>0</var> * m_cols;</td></tr>
<tr><th id="333">333</th><td>  <em>const</em> int8_t* <b>__restrict__</b> vector1 = vectors + <var>1</var> * m_cols;</td></tr>
<tr><th id="334">334</th><td>  <em>const</em> int8_t* <b>__restrict__</b> vector2 = vectors + <var>2</var> * m_cols;</td></tr>
<tr><th id="335">335</th><td>  <em>const</em> int8_t* <b>__restrict__</b> vector3 = vectors + <var>3</var> * m_cols;</td></tr>
<tr><th id="336">336</th><td>  <em>float</em>* <b>__restrict__</b> result0 = results + <var>0</var> * m_rows;</td></tr>
<tr><th id="337">337</th><td>  <em>float</em>* <b>__restrict__</b> result1 = results + <var>1</var> * m_rows;</td></tr>
<tr><th id="338">338</th><td>  <em>float</em>* <b>__restrict__</b> result2 = results + <var>2</var> * m_rows;</td></tr>
<tr><th id="339">339</th><td>  <em>float</em>* <b>__restrict__</b> result3 = results + <var>3</var> * m_rows;</td></tr>
<tr><th id="340">340</th><td></td></tr>
<tr><th id="341">341</th><td>  <b>for</b> (std::intptr_t row = <var>0</var>; row &lt; m_rows; ++row) {</td></tr>
<tr><th id="342">342</th><td>    <i>// Initialize the dot product sum for the row to 0.</i></td></tr>
<tr><th id="343">343</th><td>    __m128i dp0_32x4 = _mm_setzero_si128();</td></tr>
<tr><th id="344">344</th><td>    __m128i dp1_32x4 = _mm_setzero_si128();</td></tr>
<tr><th id="345">345</th><td>    __m128i dp2_32x4 = _mm_setzero_si128();</td></tr>
<tr><th id="346">346</th><td>    __m128i dp3_32x4 = _mm_setzero_si128();</td></tr>
<tr><th id="347">347</th><td></td></tr>
<tr><th id="348">348</th><td>    std::intptr_t num_nonzero_blocks = *ledger++;</td></tr>
<tr><th id="349">349</th><td>    <b>for</b> (std::intptr_t i = <var>0</var>; i &lt; num_nonzero_blocks; i++) {</td></tr>
<tr><th id="350">350</th><td>      <em>const</em> std::intptr_t col_index = *ledger++ * kBlockSize;</td></tr>
<tr><th id="351">351</th><td>      <i>// vecN are for different batches</i></td></tr>
<tr><th id="352">352</th><td>      <em>const</em> __m128i vec0_8x16 = _mm_loadu_si128(</td></tr>
<tr><th id="353">353</th><td>          <b>reinterpret_cast</b>&lt;<em>const</em> __m128i*&gt;(vector0 + col_index));</td></tr>
<tr><th id="354">354</th><td>      <em>const</em> __m128i vec1_8x16 = _mm_loadu_si128(</td></tr>
<tr><th id="355">355</th><td>          <b>reinterpret_cast</b>&lt;<em>const</em> __m128i*&gt;(vector1 + col_index));</td></tr>
<tr><th id="356">356</th><td>      <em>const</em> __m128i vec2_8x16 = _mm_loadu_si128(</td></tr>
<tr><th id="357">357</th><td>          <b>reinterpret_cast</b>&lt;<em>const</em> __m128i*&gt;(vector2 + col_index));</td></tr>
<tr><th id="358">358</th><td>      <em>const</em> __m128i vec3_8x16 = _mm_loadu_si128(</td></tr>
<tr><th id="359">359</th><td>          <b>reinterpret_cast</b>&lt;<em>const</em> __m128i*&gt;(vector3 + col_index));</td></tr>
<tr><th id="360">360</th><td>      <em>const</em> __m128i row_8x16 =</td></tr>
<tr><th id="361">361</th><td>          _mm_loadu_si128(<b>reinterpret_cast</b>&lt;<em>const</em> __m128i*&gt;(matrix));</td></tr>
<tr><th id="362">362</th><td>      <i>// dp += vec · row</i></td></tr>
<tr><th id="363">363</th><td><i>      // dpN are for different batches</i></td></tr>
<tr><th id="364">364</th><td>      dp0_32x4 = _mm_add_epi32(dp0_32x4, DotProdInt8x4x4(row_8x16, vec0_8x16));</td></tr>
<tr><th id="365">365</th><td>      dp1_32x4 = _mm_add_epi32(dp1_32x4, DotProdInt8x4x4(row_8x16, vec1_8x16));</td></tr>
<tr><th id="366">366</th><td>      dp2_32x4 = _mm_add_epi32(dp2_32x4, DotProdInt8x4x4(row_8x16, vec2_8x16));</td></tr>
<tr><th id="367">367</th><td>      dp3_32x4 = _mm_add_epi32(dp3_32x4, DotProdInt8x4x4(row_8x16, vec3_8x16));</td></tr>
<tr><th id="368">368</th><td>      matrix += kBlockSize;</td></tr>
<tr><th id="369">369</th><td>    }  <i>// for col</i></td></tr>
<tr><th id="370">370</th><td></td></tr>
<tr><th id="371">371</th><td>    <i>// Horizontally add the 4 intermediate values.</i></td></tr>
<tr><th id="372">372</th><td>    <em>const</em> __m128i dp_32x4 =</td></tr>
<tr><th id="373">373</th><td>        ReduceInt32x4x4(dp0_32x4, dp1_32x4, dp2_32x4, dp3_32x4);</td></tr>
<tr><th id="374">374</th><td>    <i>// Convert to float</i></td></tr>
<tr><th id="375">375</th><td>    <em>const</em> __m128 dp_fx4 = _mm_cvtepi32_ps(dp_32x4);</td></tr>
<tr><th id="376">376</th><td>    <i>// Load the results (This is an Accumulate function..)</i></td></tr>
<tr><th id="377">377</th><td>    __m128 result_fx4 =</td></tr>
<tr><th id="378">378</th><td>        _mm_set_ps(result3[row], result2[row], result1[row], result0[row]);</td></tr>
<tr><th id="379">379</th><td>    <i>// result += dp .* scaling</i></td></tr>
<tr><th id="380">380</th><td>    result_fx4 =</td></tr>
<tr><th id="381">381</th><td>        _mm_add_ps(result_fx4, _mm_mul_ps(dp_fx4, scaling_factors_fx4));</td></tr>
<tr><th id="382">382</th><td>    <i>// Save the results</i></td></tr>
<tr><th id="383">383</th><td>    result0[row] = GetFloatVectorElement&lt;<var>0</var>&gt;(result_fx4);</td></tr>
<tr><th id="384">384</th><td>    result1[row] = GetFloatVectorElement&lt;<var>1</var>&gt;(result_fx4);</td></tr>
<tr><th id="385">385</th><td>    result2[row] = GetFloatVectorElement&lt;<var>2</var>&gt;(result_fx4);</td></tr>
<tr><th id="386">386</th><td>    result3[row] = GetFloatVectorElement&lt;<var>3</var>&gt;(result_fx4);</td></tr>
<tr><th id="387">387</th><td>  }  <i>// for row</i></td></tr>
<tr><th id="388">388</th><td>}</td></tr>
<tr><th id="389">389</th><td></td></tr>
<tr><th id="390">390</th><td>}  <i>// namespace</i></td></tr>
<tr><th id="391">391</th><td></td></tr>
<tr><th id="392">392</th><td><em>void</em> SseSparseMatrixBatchVectorMultiplyAccumulate(</td></tr>
<tr><th id="393">393</th><td>    <em>const</em> int8_t* <b>__restrict__</b> matrix, <em>const</em> uint8_t* <b>__restrict__</b> ledger,</td></tr>
<tr><th id="394">394</th><td>    <em>const</em> <em>int</em> m_rows, <em>const</em> <em>int</em> m_cols, <em>const</em> int8_t* <b>__restrict__</b> vectors,</td></tr>
<tr><th id="395">395</th><td>    <em>const</em> <em>float</em>* <b>__restrict__</b> scaling_factors, <em>int</em> n_batch,</td></tr>
<tr><th id="396">396</th><td>    <em>float</em>* <b>__restrict__</b> results) {</td></tr>
<tr><th id="397">397</th><td>  <em>int</em> batch = <var>0</var>;</td></tr>
<tr><th id="398">398</th><td>  <em>const</em> <em>int</em> kBatchSize4 = <var>4</var>;</td></tr>
<tr><th id="399">399</th><td>  <em>const</em> <em>int</em> n_batch_rounddown_to_batchsize_4 = n_batch &amp; ~(kBatchSize4 - <var>1</var>);</td></tr>
<tr><th id="400">400</th><td>  <b>while</b> (batch &lt; n_batch_rounddown_to_batchsize_4) {</td></tr>
<tr><th id="401">401</th><td>    <em>const</em> __m128 scaling_factors_fx4 = _mm_loadu_ps(scaling_factors + batch);</td></tr>
<tr><th id="402">402</th><td>    SseSparseMatrix4VectorsMultiplyAccumulate(</td></tr>
<tr><th id="403">403</th><td>        matrix, ledger, m_rows, m_cols, vectors, scaling_factors_fx4, results);</td></tr>
<tr><th id="404">404</th><td>    batch += kBatchSize4;</td></tr>
<tr><th id="405">405</th><td>    vectors += kBatchSize4 * m_cols;</td></tr>
<tr><th id="406">406</th><td>    results += kBatchSize4 * m_rows;</td></tr>
<tr><th id="407">407</th><td>  }  <i>// for batch</i></td></tr>
<tr><th id="408">408</th><td>  <b>while</b> (batch &lt; n_batch) {</td></tr>
<tr><th id="409">409</th><td>    SseSparseMatrixVectorMultiplyAccumulate(matrix, ledger, m_rows, m_cols,</td></tr>
<tr><th id="410">410</th><td>                                            vectors, scaling_factors[batch],</td></tr>
<tr><th id="411">411</th><td>                                            results);</td></tr>
<tr><th id="412">412</th><td>    ++batch;</td></tr>
<tr><th id="413">413</th><td>    vectors += m_cols;</td></tr>
<tr><th id="414">414</th><td>    results += m_rows;</td></tr>
<tr><th id="415">415</th><td>  }  <i>// for batch</i></td></tr>
<tr><th id="416">416</th><td>}</td></tr>
<tr><th id="417">417</th><td></td></tr>
<tr><th id="418">418</th><td><em>void</em> SseReductionSumVector(<em>const</em> int8_t* input_vector, int32_t* output_vector,</td></tr>
<tr><th id="419">419</th><td>                           <em>const</em> <em>int</em> output_size, <em>const</em> <em>int</em> reduction_size) {</td></tr>
<tr><th id="420">420</th><td>  <em>static</em> <b>constexpr</b> std::intptr_t kBlockSize = <var>16</var>;</td></tr>
<tr><th id="421">421</th><td>  <b>for</b> (std::intptr_t row = <var>0</var>; row &lt; output_size; ++row) {</td></tr>
<tr><th id="422">422</th><td>    <em>const</em> int8_t* <b>__restrict__</b> row_ptr = input_vector + row * reduction_size;</td></tr>
<tr><th id="423">423</th><td>    __m128i row_sum_16x8 = _mm_setzero_si128();</td></tr>
<tr><th id="424">424</th><td>    std::intptr_t col = <var>0</var>;</td></tr>
<tr><th id="425">425</th><td>    <b>for</b> (; col &lt; (reduction_size &amp; ~(kBlockSize - <var>1</var>)); col += kBlockSize) {</td></tr>
<tr><th id="426">426</th><td>      <em>const</em> __m128i row_8x16 =</td></tr>
<tr><th id="427">427</th><td>          _mm_loadu_si128(<b>reinterpret_cast</b>&lt;<em>const</em> __m128i*&gt;(row_ptr + col));</td></tr>
<tr><th id="428">428</th><td>      <em>const</em> __m128i row_16x8 = _mm_maddubs_epi16(_mm_set1_epi8(<var>1</var>), row_8x16);</td></tr>
<tr><th id="429">429</th><td>      row_sum_16x8 = _mm_add_epi16(row_sum_16x8, row_16x8);</td></tr>
<tr><th id="430">430</th><td>    }  <i>// for col</i></td></tr>
<tr><th id="431">431</th><td><u>#ifdef __SSE4_1__</u></td></tr>
<tr><th id="432">432</th><td>    <i>// Postamble for 8x 8-bit inputs.</i></td></tr>
<tr><th id="433">433</th><td>    <b>if</b> (col &lt; (reduction_size &amp; ~<var>7</var>)) {</td></tr>
<tr><th id="434">434</th><td>      <i>// _mm_loadu_si64 not supported in gcc versions &lt; 9, breaks kokoro build.</i></td></tr>
<tr><th id="435">435</th><td>      <em>const</em> __m128i row_16x8 = _mm_cvtepi8_epi16(</td></tr>
<tr><th id="436">436</th><td>          _mm_loadl_epi64(<b>reinterpret_cast</b>&lt;<em>const</em> __m128i*&gt;(row_ptr + col)));</td></tr>
<tr><th id="437">437</th><td>      <i>// dotprod += vec · row</i></td></tr>
<tr><th id="438">438</th><td>      row_sum_16x8 = _mm_add_epi16(row_sum_16x8, row_16x8);</td></tr>
<tr><th id="439">439</th><td>      col += <var>8</var>;</td></tr>
<tr><th id="440">440</th><td>    }</td></tr>
<tr><th id="441">441</th><td><u>#endif</u></td></tr>
<tr><th id="442">442</th><td>    <em>const</em> __m128i row_sum_32x4 =</td></tr>
<tr><th id="443">443</th><td>        _mm_madd_epi16(row_sum_16x8, _mm_set1_epi16(<var>1</var>));</td></tr>
<tr><th id="444">444</th><td>    int32_t row_sum = ReduceInt32x4(row_sum_32x4);</td></tr>
<tr><th id="445">445</th><td><u>#if defined(__SSE4_1__) &amp;&amp; defined(__clang__)</u></td></tr>
<tr><th id="446">446</th><td>    <i>// SSE 4.1: Don't try to unroll and vectorize this, already done above.</i></td></tr>
<tr><th id="447">447</th><td><u>#pragma clang loop unroll(disable) vectorize(disable)</u></td></tr>
<tr><th id="448">448</th><td><u>#endif</u></td></tr>
<tr><th id="449">449</th><td>    <b>for</b> (; col &lt; reduction_size; col++) {</td></tr>
<tr><th id="450">450</th><td>      row_sum += row_ptr[col];</td></tr>
<tr><th id="451">451</th><td>    }</td></tr>
<tr><th id="452">452</th><td>    output_vector[row] = row_sum;</td></tr>
<tr><th id="453">453</th><td>  }</td></tr>
<tr><th id="454">454</th><td>}</td></tr>
<tr><th id="455">455</th><td></td></tr>
<tr><th id="456">456</th><td>}  <i>// namespace tensor_utils</i></td></tr>
<tr><th id="457">457</th><td>}  <i>// namespace tflite</i></td></tr>
<tr><th id="458">458</th><td></td></tr>
<tr><th id="459">459</th><td><u>#<span data-ppcond="17">endif</span>  // __SSSE3__</u></td></tr>
<tr><th id="460">460</th><td></td></tr>
</table><hr/><p id='footer'>
Generated on <em>2021-Aug-05</em> from project halide revision <em>v12.0.1</em>