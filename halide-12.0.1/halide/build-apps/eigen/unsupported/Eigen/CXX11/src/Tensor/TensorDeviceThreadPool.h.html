<!doctype html>
<html>
<head>
<meta name="viewport" content="width=device-width, initial-scale=1.0"><title>TensorDeviceThreadPool.h source code [halide/build-apps/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorDeviceThreadPool.h] - Woboq Code Browser</title>
<link rel="stylesheet" href="../../../../../../../.././data/qtcreator.css" title="QtCreator"/>
<link rel="alternate stylesheet" href="../../../../../../../.././data/kdevelop.css" title="KDevelop"/>
<script type="text/javascript" src="../../../../../../../.././data/jquery/jquery.min.js"></script>
<script type="text/javascript" src="../../../../../../../.././data/jquery/jquery-ui.min.js"></script>
<script>var file = 'halide/build-apps/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorDeviceThreadPool.h'; var root_path = '../../../../../../../..'; var data_path = '../../../../../../../.././data'; var ecma_script_api_version = 2;</script>
<script src='../../../../../../../.././data/codebrowser.js'></script>
</head>
<body><div id='header'><h1 id='breadcrumb'><span>Browse the source code of </span><a href='../../../../../../..'>halide</a>/<a href='../../../../../..'>build-apps</a>/<a href='../../../../..'>eigen</a>/<a href='../../../..'>unsupported</a>/<a href='../../..'>Eigen</a>/<a href='../..'>CXX11</a>/<a href='..'>src</a>/<a href='./'>Tensor</a>/<a href='TensorDeviceThreadPool.h.html'>TensorDeviceThreadPool.h</a></h1></div>
<hr/><div id='content'><table class="code">
<tr><th id="1">1</th><td><i>// This file is part of Eigen, a lightweight C++ template library</i></td></tr>
<tr><th id="2">2</th><td><i>// for linear algebra.</i></td></tr>
<tr><th id="3">3</th><td><i>//</i></td></tr>
<tr><th id="4">4</th><td><i>// Copyright (C) 2014 Benoit Steiner &lt;benoit.steiner.goog@gmail.com&gt;</i></td></tr>
<tr><th id="5">5</th><td><i>//</i></td></tr>
<tr><th id="6">6</th><td><i>// This Source Code Form is subject to the terms of the Mozilla</i></td></tr>
<tr><th id="7">7</th><td><i>// Public License v. 2.0. If a copy of the MPL was not distributed</i></td></tr>
<tr><th id="8">8</th><td><i>// with this file, You can obtain one at <a href="http://mozilla.org/MPL/2.0/">http://mozilla.org/MPL/2.0/</a>.</i></td></tr>
<tr><th id="9">9</th><td></td></tr>
<tr><th id="10">10</th><td><u>#<span data-ppcond="10">if</span> defined(<span class="macro" data-ref="_M/EIGEN_USE_THREADS">EIGEN_USE_THREADS</span>) &amp;&amp; !defined(<span class="macro" data-ref="_M/EIGEN_CXX11_TENSOR_TENSOR_DEVICE_THREAD_POOL_H">EIGEN_CXX11_TENSOR_TENSOR_DEVICE_THREAD_POOL_H</span>)</u></td></tr>
<tr><th id="11">11</th><td><u>#define EIGEN_CXX11_TENSOR_TENSOR_DEVICE_THREAD_POOL_H</u></td></tr>
<tr><th id="12">12</th><td></td></tr>
<tr><th id="13">13</th><td><b>namespace</b> Eigen {</td></tr>
<tr><th id="14">14</th><td></td></tr>
<tr><th id="15">15</th><td><i>// Runs an arbitrary function and then calls Notify() on the passed in</i></td></tr>
<tr><th id="16">16</th><td><i>// Notification.</i></td></tr>
<tr><th id="17">17</th><td><b>template</b> &lt;<b>typename</b> Function, <b>typename</b>... Args&gt; <b>struct</b> FunctionWrapperWithNotification</td></tr>
<tr><th id="18">18</th><td>{</td></tr>
<tr><th id="19">19</th><td>  <em>static</em> <em>void</em> run(Notification* n, Function f, Args... args) {</td></tr>
<tr><th id="20">20</th><td>    f(args...);</td></tr>
<tr><th id="21">21</th><td>    <b>if</b> (n) {</td></tr>
<tr><th id="22">22</th><td>      n-&gt;Notify();</td></tr>
<tr><th id="23">23</th><td>    }</td></tr>
<tr><th id="24">24</th><td>  }</td></tr>
<tr><th id="25">25</th><td>};</td></tr>
<tr><th id="26">26</th><td></td></tr>
<tr><th id="27">27</th><td><b>template</b> &lt;<b>typename</b> Function, <b>typename</b>... Args&gt; <b>struct</b> FunctionWrapperWithBarrier</td></tr>
<tr><th id="28">28</th><td>{</td></tr>
<tr><th id="29">29</th><td>  <em>static</em> <em>void</em> run(Barrier* b, Function f, Args... args) {</td></tr>
<tr><th id="30">30</th><td>    f(args...);</td></tr>
<tr><th id="31">31</th><td>    <b>if</b> (b) {</td></tr>
<tr><th id="32">32</th><td>      b-&gt;Notify();</td></tr>
<tr><th id="33">33</th><td>    }</td></tr>
<tr><th id="34">34</th><td>  }</td></tr>
<tr><th id="35">35</th><td>};</td></tr>
<tr><th id="36">36</th><td></td></tr>
<tr><th id="37">37</th><td><b>template</b> &lt;<b>typename</b> SyncType&gt;</td></tr>
<tr><th id="38">38</th><td><em>static</em> EIGEN_STRONG_INLINE <em>void</em> wait_until_ready(SyncType* n) {</td></tr>
<tr><th id="39">39</th><td>  <b>if</b> (n) {</td></tr>
<tr><th id="40">40</th><td>    n-&gt;Wait();</td></tr>
<tr><th id="41">41</th><td>  }</td></tr>
<tr><th id="42">42</th><td>}</td></tr>
<tr><th id="43">43</th><td></td></tr>
<tr><th id="44">44</th><td><i>// An abstract interface to a device specific memory allocator.</i></td></tr>
<tr><th id="45">45</th><td><b>class</b> Allocator {</td></tr>
<tr><th id="46">46</th><td> <b>public</b>:</td></tr>
<tr><th id="47">47</th><td>  <b>virtual</b> ~Allocator() {}</td></tr>
<tr><th id="48">48</th><td>  <b>virtual</b> <em>void</em>* allocate(size_t num_bytes) <em>const</em> = <var>0</var>;</td></tr>
<tr><th id="49">49</th><td>  <b>virtual</b> <em>void</em> deallocate(<em>void</em>* buffer) <em>const</em> = <var>0</var>;</td></tr>
<tr><th id="50">50</th><td>};</td></tr>
<tr><th id="51">51</th><td></td></tr>
<tr><th id="52">52</th><td><i>// Build a thread pool device on top the an existing pool of threads.</i></td></tr>
<tr><th id="53">53</th><td><b>struct</b> ThreadPoolDevice {</td></tr>
<tr><th id="54">54</th><td>  <i>// The ownership of the thread pool remains with the caller.</i></td></tr>
<tr><th id="55">55</th><td>  ThreadPoolDevice(ThreadPoolInterface* pool, <em>int</em> num_cores, Allocator* allocator = <b>nullptr</b>)</td></tr>
<tr><th id="56">56</th><td>      : pool_(pool), num_threads_(num_cores), allocator_(allocator) { }</td></tr>
<tr><th id="57">57</th><td></td></tr>
<tr><th id="58">58</th><td>  EIGEN_STRONG_INLINE <em>void</em>* allocate(size_t num_bytes) <em>const</em> {</td></tr>
<tr><th id="59">59</th><td>    <b>return</b> allocator_ ? allocator_-&gt;allocate(num_bytes)</td></tr>
<tr><th id="60">60</th><td>        : internal::aligned_malloc(num_bytes);</td></tr>
<tr><th id="61">61</th><td>  }</td></tr>
<tr><th id="62">62</th><td></td></tr>
<tr><th id="63">63</th><td>  EIGEN_STRONG_INLINE <em>void</em> deallocate(<em>void</em>* buffer) <em>const</em> {</td></tr>
<tr><th id="64">64</th><td>    <b>if</b> (allocator_) {</td></tr>
<tr><th id="65">65</th><td>      allocator_-&gt;deallocate(buffer);</td></tr>
<tr><th id="66">66</th><td>    } <b>else</b> {</td></tr>
<tr><th id="67">67</th><td>      internal::aligned_free(buffer);</td></tr>
<tr><th id="68">68</th><td>    }</td></tr>
<tr><th id="69">69</th><td>  }</td></tr>
<tr><th id="70">70</th><td></td></tr>
<tr><th id="71">71</th><td>    EIGEN_STRONG_INLINE <em>void</em>* allocate_temp(size_t num_bytes) <em>const</em> {</td></tr>
<tr><th id="72">72</th><td>    <b>return</b> allocate(num_bytes);</td></tr>
<tr><th id="73">73</th><td>  }</td></tr>
<tr><th id="74">74</th><td></td></tr>
<tr><th id="75">75</th><td>  EIGEN_STRONG_INLINE <em>void</em> deallocate_temp(<em>void</em>* buffer) <em>const</em> {</td></tr>
<tr><th id="76">76</th><td>    deallocate(buffer);</td></tr>
<tr><th id="77">77</th><td>  }</td></tr>
<tr><th id="78">78</th><td></td></tr>
<tr><th id="79">79</th><td>  <b>template</b>&lt;<b>typename</b> Type&gt;</td></tr>
<tr><th id="80">80</th><td>  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Type get(Type data) <em>const</em> {</td></tr>
<tr><th id="81">81</th><td>    <b>return</b> data;</td></tr>
<tr><th id="82">82</th><td>  }</td></tr>
<tr><th id="83">83</th><td></td></tr>
<tr><th id="84">84</th><td>  EIGEN_STRONG_INLINE <em>void</em> memcpy(<em>void</em>* dst, <em>const</em> <em>void</em>* src, size_t n) <em>const</em> {</td></tr>
<tr><th id="85">85</th><td><u>#ifdef __ANDROID__</u></td></tr>
<tr><th id="86">86</th><td>    ::memcpy(dst, src, n);</td></tr>
<tr><th id="87">87</th><td><u>#else</u></td></tr>
<tr><th id="88">88</th><td>    <i>// TODO(rmlarsen): Align blocks on cache lines.</i></td></tr>
<tr><th id="89">89</th><td><i>    // We have observed that going beyond 4 threads usually just wastes</i></td></tr>
<tr><th id="90">90</th><td><i>    // CPU cycles due to the threads competing for memory bandwidth, so we</i></td></tr>
<tr><th id="91">91</th><td><i>    // statically schedule at most 4 block copies here.</i></td></tr>
<tr><th id="92">92</th><td>    <em>const</em> size_t kMinBlockSize = <var>32768</var>;</td></tr>
<tr><th id="93">93</th><td>    <em>const</em> size_t num_threads = CostModel::numThreads(n, TensorOpCost(<var>1.0</var>, <var>1.0</var>, <var>0</var>), <var>4</var>);</td></tr>
<tr><th id="94">94</th><td>    <b>if</b> (n &lt;= kMinBlockSize || num_threads &lt; <var>2</var>) {</td></tr>
<tr><th id="95">95</th><td>      ::memcpy(dst, src, n);</td></tr>
<tr><th id="96">96</th><td>    } <b>else</b> {</td></tr>
<tr><th id="97">97</th><td>      <em>const</em> <em>char</em>* src_ptr = <b>static_cast</b>&lt;<em>const</em> <em>char</em>*&gt;(src);</td></tr>
<tr><th id="98">98</th><td>      <em>char</em>* dst_ptr = <b>static_cast</b>&lt;<em>char</em>*&gt;(dst);</td></tr>
<tr><th id="99">99</th><td>      <em>const</em> size_t blocksize = (n + (num_threads - <var>1</var>)) / num_threads;</td></tr>
<tr><th id="100">100</th><td>      Barrier barrier(<b>static_cast</b>&lt;<em>int</em>&gt;(num_threads - <var>1</var>));</td></tr>
<tr><th id="101">101</th><td>      <i>// Launch the last 3 blocks on worker threads.</i></td></tr>
<tr><th id="102">102</th><td>      <b>for</b> (size_t i = <var>1</var>; i &lt; num_threads; ++i) {</td></tr>
<tr><th id="103">103</th><td>        enqueue_with_barrier(&amp;barrier, [n, i, src_ptr, dst_ptr, blocksize] {</td></tr>
<tr><th id="104">104</th><td>          ::memcpy(dst_ptr + i * blocksize, src_ptr + i * blocksize,</td></tr>
<tr><th id="105">105</th><td>                   numext::mini(blocksize, n - (i * blocksize)));</td></tr>
<tr><th id="106">106</th><td>        });</td></tr>
<tr><th id="107">107</th><td>      }</td></tr>
<tr><th id="108">108</th><td>      <i>// Launch the first block on the main thread.</i></td></tr>
<tr><th id="109">109</th><td>      ::memcpy(dst_ptr, src_ptr, blocksize);</td></tr>
<tr><th id="110">110</th><td>      barrier.Wait();</td></tr>
<tr><th id="111">111</th><td>    }</td></tr>
<tr><th id="112">112</th><td><u>#endif</u></td></tr>
<tr><th id="113">113</th><td>  }</td></tr>
<tr><th id="114">114</th><td>  EIGEN_STRONG_INLINE <em>void</em> memcpyHostToDevice(<em>void</em>* dst, <em>const</em> <em>void</em>* src, size_t n) <em>const</em> {</td></tr>
<tr><th id="115">115</th><td>    memcpy(dst, src, n);</td></tr>
<tr><th id="116">116</th><td>  }</td></tr>
<tr><th id="117">117</th><td>  EIGEN_STRONG_INLINE <em>void</em> memcpyDeviceToHost(<em>void</em>* dst, <em>const</em> <em>void</em>* src, size_t n) <em>const</em> {</td></tr>
<tr><th id="118">118</th><td>    memcpy(dst, src, n);</td></tr>
<tr><th id="119">119</th><td>  }</td></tr>
<tr><th id="120">120</th><td></td></tr>
<tr><th id="121">121</th><td>  EIGEN_STRONG_INLINE <em>void</em> memset(<em>void</em>* buffer, <em>int</em> c, size_t n) <em>const</em> {</td></tr>
<tr><th id="122">122</th><td>    ::memset(buffer, c, n);</td></tr>
<tr><th id="123">123</th><td>  }</td></tr>
<tr><th id="124">124</th><td></td></tr>
<tr><th id="125">125</th><td>  EIGEN_STRONG_INLINE <em>int</em> numThreads() <em>const</em> {</td></tr>
<tr><th id="126">126</th><td>    <b>return</b> num_threads_;</td></tr>
<tr><th id="127">127</th><td>  }</td></tr>
<tr><th id="128">128</th><td></td></tr>
<tr><th id="129">129</th><td>  <i>// Number of theads available in the underlying thread pool. This number can</i></td></tr>
<tr><th id="130">130</th><td><i>  // be different from the value returned by numThreads().</i></td></tr>
<tr><th id="131">131</th><td>  EIGEN_STRONG_INLINE <em>int</em> numThreadsInPool() <em>const</em> {</td></tr>
<tr><th id="132">132</th><td>    <b>return</b> pool_-&gt;NumThreads();</td></tr>
<tr><th id="133">133</th><td>  }</td></tr>
<tr><th id="134">134</th><td></td></tr>
<tr><th id="135">135</th><td>  EIGEN_STRONG_INLINE size_t firstLevelCacheSize() <em>const</em> {</td></tr>
<tr><th id="136">136</th><td>    <b>return</b> l1CacheSize();</td></tr>
<tr><th id="137">137</th><td>  }</td></tr>
<tr><th id="138">138</th><td></td></tr>
<tr><th id="139">139</th><td>  EIGEN_STRONG_INLINE size_t lastLevelCacheSize() <em>const</em> {</td></tr>
<tr><th id="140">140</th><td>    <i>// The l3 cache size is shared between all the cores.</i></td></tr>
<tr><th id="141">141</th><td>    <b>return</b> l3CacheSize() / num_threads_;</td></tr>
<tr><th id="142">142</th><td>  }</td></tr>
<tr><th id="143">143</th><td></td></tr>
<tr><th id="144">144</th><td>  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE <em>int</em> majorDeviceVersion() <em>const</em> {</td></tr>
<tr><th id="145">145</th><td>    <i>// Should return an enum that encodes the ISA supported by the CPU</i></td></tr>
<tr><th id="146">146</th><td>    <b>return</b> <var>1</var>;</td></tr>
<tr><th id="147">147</th><td>  }</td></tr>
<tr><th id="148">148</th><td></td></tr>
<tr><th id="149">149</th><td>  <b>template</b> &lt;<b>class</b> Function, <b>class</b>... Args&gt;</td></tr>
<tr><th id="150">150</th><td>  EIGEN_STRONG_INLINE Notification* enqueue(Function&amp;&amp; f,</td></tr>
<tr><th id="151">151</th><td>                                            Args&amp;&amp;... args) <em>const</em> {</td></tr>
<tr><th id="152">152</th><td>    Notification* n = <b>new</b> Notification();</td></tr>
<tr><th id="153">153</th><td>    pool_-&gt;Schedule(</td></tr>
<tr><th id="154">154</th><td>        std::bind(&amp;FunctionWrapperWithNotification&lt;Function, Args...&gt;::run, n,</td></tr>
<tr><th id="155">155</th><td>                  std::move(f), args...));</td></tr>
<tr><th id="156">156</th><td>    <b>return</b> n;</td></tr>
<tr><th id="157">157</th><td>  }</td></tr>
<tr><th id="158">158</th><td></td></tr>
<tr><th id="159">159</th><td>  <b>template</b> &lt;<b>class</b> Function, <b>class</b>... Args&gt;</td></tr>
<tr><th id="160">160</th><td>  EIGEN_STRONG_INLINE <em>void</em> enqueue_with_barrier(Barrier* b, Function&amp;&amp; f,</td></tr>
<tr><th id="161">161</th><td>                                                Args&amp;&amp;... args) <em>const</em> {</td></tr>
<tr><th id="162">162</th><td>    pool_-&gt;Schedule(</td></tr>
<tr><th id="163">163</th><td>        std::bind(&amp;FunctionWrapperWithBarrier&lt;Function, Args...&gt;::run, b,</td></tr>
<tr><th id="164">164</th><td>                  std::move(f), args...));</td></tr>
<tr><th id="165">165</th><td>  }</td></tr>
<tr><th id="166">166</th><td></td></tr>
<tr><th id="167">167</th><td>  <b>template</b> &lt;<b>class</b> Function, <b>class</b>... Args&gt;</td></tr>
<tr><th id="168">168</th><td>  EIGEN_STRONG_INLINE <em>void</em> enqueueNoNotification(Function&amp;&amp; f,</td></tr>
<tr><th id="169">169</th><td>                                                 Args&amp;&amp;... args) <em>const</em> {</td></tr>
<tr><th id="170">170</th><td>    <b>if</b> (<b>sizeof</b>...(args) &gt; <var>0</var>) {</td></tr>
<tr><th id="171">171</th><td>      pool_-&gt;Schedule(std::bind(std::move(f), args...));</td></tr>
<tr><th id="172">172</th><td>    } <b>else</b> {</td></tr>
<tr><th id="173">173</th><td>      pool_-&gt;Schedule(std::move(f));</td></tr>
<tr><th id="174">174</th><td>    }</td></tr>
<tr><th id="175">175</th><td>  }</td></tr>
<tr><th id="176">176</th><td></td></tr>
<tr><th id="177">177</th><td>  <i>// Returns a logical thread index between 0 and pool_-&gt;NumThreads() - 1 if</i></td></tr>
<tr><th id="178">178</th><td><i>  // called from one of the threads in pool_. Returns -1 otherwise.</i></td></tr>
<tr><th id="179">179</th><td>  EIGEN_STRONG_INLINE <em>int</em> currentThreadId() <em>const</em> {</td></tr>
<tr><th id="180">180</th><td>    <b>return</b> pool_-&gt;CurrentThreadId();</td></tr>
<tr><th id="181">181</th><td>  }</td></tr>
<tr><th id="182">182</th><td></td></tr>
<tr><th id="183">183</th><td>  <i>// WARNING: This function is synchronous and will block the calling thread.</i></td></tr>
<tr><th id="184">184</th><td><i>  //</i></td></tr>
<tr><th id="185">185</th><td><i>  // Synchronous parallelFor executes f with [0, n) arguments in parallel and</i></td></tr>
<tr><th id="186">186</th><td><i>  // waits for completion. F accepts a half-open interval [first, last). Block</i></td></tr>
<tr><th id="187">187</th><td><i>  // size is chosen based on the iteration cost and resulting parallel</i></td></tr>
<tr><th id="188">188</th><td><i>  // efficiency. If block_align is not nullptr, it is called to round up the</i></td></tr>
<tr><th id="189">189</th><td><i>  // block size.</i></td></tr>
<tr><th id="190">190</th><td>  <em>void</em> parallelFor(Index n, <em>const</em> TensorOpCost&amp; cost,</td></tr>
<tr><th id="191">191</th><td>                   std::function&lt;Index(Index)&gt; block_align,</td></tr>
<tr><th id="192">192</th><td>                   std::function&lt;<em>void</em>(Index, Index)&gt; f) <em>const</em> {</td></tr>
<tr><th id="193">193</th><td>    <b>if</b> (EIGEN_PREDICT_FALSE(n &lt;= <var>0</var>)){</td></tr>
<tr><th id="194">194</th><td>      <b>return</b>;</td></tr>
<tr><th id="195">195</th><td>    <i>// Compute small problems directly in the caller thread.</i></td></tr>
<tr><th id="196">196</th><td>    } <b>else</b> <b>if</b> (n == <var>1</var> || numThreads() == <var>1</var> ||</td></tr>
<tr><th id="197">197</th><td>               CostModel::numThreads(n, cost, <b>static_cast</b>&lt;<em>int</em>&gt;(numThreads())) == <var>1</var>) {</td></tr>
<tr><th id="198">198</th><td>      f(<var>0</var>, n);</td></tr>
<tr><th id="199">199</th><td>      <b>return</b>;</td></tr>
<tr><th id="200">200</th><td>    }</td></tr>
<tr><th id="201">201</th><td></td></tr>
<tr><th id="202">202</th><td>    <i>// Compute block size and total count of blocks.</i></td></tr>
<tr><th id="203">203</th><td>    ParallelForBlock block = CalculateParallelForBlock(n, cost, block_align);</td></tr>
<tr><th id="204">204</th><td></td></tr>
<tr><th id="205">205</th><td>    <i>// Recursively divide size into halves until we reach block_size.</i></td></tr>
<tr><th id="206">206</th><td><i>    // Division code rounds mid to block_size, so we are guaranteed to get</i></td></tr>
<tr><th id="207">207</th><td><i>    // block_count leaves that do actual computations.</i></td></tr>
<tr><th id="208">208</th><td>    Barrier barrier(<b>static_cast</b>&lt;<em>unsigned</em> <em>int</em>&gt;(block.count));</td></tr>
<tr><th id="209">209</th><td>    std::function&lt;<em>void</em>(Index, Index)&gt; handleRange;</td></tr>
<tr><th id="210">210</th><td>    handleRange = [=, &amp;handleRange, &amp;barrier, &amp;f](Index firstIdx,</td></tr>
<tr><th id="211">211</th><td>                                                  Index lastIdx) {</td></tr>
<tr><th id="212">212</th><td>      <b>while</b> (lastIdx - firstIdx &gt; block.size) {</td></tr>
<tr><th id="213">213</th><td>        <i>// Split into halves and schedule the second half on a different thread.</i></td></tr>
<tr><th id="214">214</th><td>        <em>const</em> Index midIdx = firstIdx + divup((lastIdx - firstIdx) / <var>2</var>, block.size) * block.size;</td></tr>
<tr><th id="215">215</th><td>        pool_-&gt;Schedule([=, &amp;handleRange]() { handleRange(midIdx, lastIdx); });</td></tr>
<tr><th id="216">216</th><td>        lastIdx = midIdx;</td></tr>
<tr><th id="217">217</th><td>      }</td></tr>
<tr><th id="218">218</th><td>      <i>// Single block or less, execute directly.</i></td></tr>
<tr><th id="219">219</th><td>      f(firstIdx, lastIdx);</td></tr>
<tr><th id="220">220</th><td>      barrier.Notify();</td></tr>
<tr><th id="221">221</th><td>    };</td></tr>
<tr><th id="222">222</th><td></td></tr>
<tr><th id="223">223</th><td>    <b>if</b> (block.count &lt;= numThreads()) {</td></tr>
<tr><th id="224">224</th><td>      <i>// Avoid a thread hop by running the root of the tree and one block on the</i></td></tr>
<tr><th id="225">225</th><td><i>      // main thread.</i></td></tr>
<tr><th id="226">226</th><td>      handleRange(<var>0</var>, n);</td></tr>
<tr><th id="227">227</th><td>    } <b>else</b> {</td></tr>
<tr><th id="228">228</th><td>      <i>// Execute the root in the thread pool to avoid running work on more than</i></td></tr>
<tr><th id="229">229</th><td><i>      // numThreads() threads.</i></td></tr>
<tr><th id="230">230</th><td>      pool_-&gt;Schedule([=, &amp;handleRange]() { handleRange(<var>0</var>, n); });</td></tr>
<tr><th id="231">231</th><td>    }</td></tr>
<tr><th id="232">232</th><td></td></tr>
<tr><th id="233">233</th><td>    barrier.Wait();</td></tr>
<tr><th id="234">234</th><td>  }</td></tr>
<tr><th id="235">235</th><td></td></tr>
<tr><th id="236">236</th><td>  <i>// Convenience wrapper for parallelFor that does not align blocks.</i></td></tr>
<tr><th id="237">237</th><td>  <em>void</em> parallelFor(Index n, <em>const</em> TensorOpCost&amp; cost,</td></tr>
<tr><th id="238">238</th><td>                   std::function&lt;<em>void</em>(Index, Index)&gt; f) <em>const</em> {</td></tr>
<tr><th id="239">239</th><td>    parallelFor(n, cost, <b>nullptr</b>, std::move(f));</td></tr>
<tr><th id="240">240</th><td>  }</td></tr>
<tr><th id="241">241</th><td></td></tr>
<tr><th id="242">242</th><td>  <i>// WARNING: This function is asynchronous and will not block the calling thread.</i></td></tr>
<tr><th id="243">243</th><td><i>  //</i></td></tr>
<tr><th id="244">244</th><td><i>  // Asynchronous parallelFor executes f with [0, n) arguments in parallel</i></td></tr>
<tr><th id="245">245</th><td><i>  // without waiting for completion. When the last block finished, it will call</i></td></tr>
<tr><th id="246">246</th><td><i>  // 'done' callback. F accepts a half-open interval [first, last). Block size</i></td></tr>
<tr><th id="247">247</th><td><i>  // is chosen based on the iteration cost and resulting parallel efficiency. If</i></td></tr>
<tr><th id="248">248</th><td><i>  // block_align is not nullptr, it is called to round up the block size.</i></td></tr>
<tr><th id="249">249</th><td>  <em>void</em> parallelForAsync(Index n, <em>const</em> TensorOpCost&amp; cost,</td></tr>
<tr><th id="250">250</th><td>                        std::function&lt;Index(Index)&gt; block_align,</td></tr>
<tr><th id="251">251</th><td>                        std::function&lt;<em>void</em>(Index, Index)&gt; f,</td></tr>
<tr><th id="252">252</th><td>                        std::function&lt;<em>void</em>()&gt; done) <em>const</em> {</td></tr>
<tr><th id="253">253</th><td>    <i>// Compute small problems directly in the caller thread.</i></td></tr>
<tr><th id="254">254</th><td>    <b>if</b> (n &lt;= <var>1</var> || numThreads() == <var>1</var> ||</td></tr>
<tr><th id="255">255</th><td>        CostModel::numThreads(n, cost, <b>static_cast</b>&lt;<em>int</em>&gt;(numThreads())) == <var>1</var>) {</td></tr>
<tr><th id="256">256</th><td>      f(<var>0</var>, n);</td></tr>
<tr><th id="257">257</th><td>      done();</td></tr>
<tr><th id="258">258</th><td>      <b>return</b>;</td></tr>
<tr><th id="259">259</th><td>    }</td></tr>
<tr><th id="260">260</th><td></td></tr>
<tr><th id="261">261</th><td>    <i>// Compute block size and total count of blocks.</i></td></tr>
<tr><th id="262">262</th><td>    ParallelForBlock block = CalculateParallelForBlock(n, cost, block_align);</td></tr>
<tr><th id="263">263</th><td></td></tr>
<tr><th id="264">264</th><td>    ParallelForAsyncContext* <em>const</em> ctx =</td></tr>
<tr><th id="265">265</th><td>        <b>new</b> ParallelForAsyncContext(block.count, std::move(f), std::move(done));</td></tr>
<tr><th id="266">266</th><td></td></tr>
<tr><th id="267">267</th><td>    <i>// Recursively divide size into halves until we reach block_size.</i></td></tr>
<tr><th id="268">268</th><td><i>    // Division code rounds mid to block_size, so we are guaranteed to get</i></td></tr>
<tr><th id="269">269</th><td><i>    // block_count leaves that do actual computations.</i></td></tr>
<tr><th id="270">270</th><td>    ctx-&gt;handle_range = [<b>this</b>, ctx, block](Index firstIdx, Index lastIdx) {</td></tr>
<tr><th id="271">271</th><td>      <b>while</b> (lastIdx - firstIdx &gt; block.size) {</td></tr>
<tr><th id="272">272</th><td>        <i>// Split into halves and schedule the second half on a different thread.</i></td></tr>
<tr><th id="273">273</th><td>        <em>const</em> Index midIdx = firstIdx + divup((lastIdx - firstIdx) / <var>2</var>, block.size) * block.size;</td></tr>
<tr><th id="274">274</th><td>        pool_-&gt;Schedule(</td></tr>
<tr><th id="275">275</th><td>            [ctx, midIdx, lastIdx]() { ctx-&gt;handle_range(midIdx, lastIdx); });</td></tr>
<tr><th id="276">276</th><td>        lastIdx = midIdx;</td></tr>
<tr><th id="277">277</th><td>      }</td></tr>
<tr><th id="278">278</th><td></td></tr>
<tr><th id="279">279</th><td>      <i>// Single block or less, execute directly.</i></td></tr>
<tr><th id="280">280</th><td>      ctx-&gt;f(firstIdx, lastIdx);</td></tr>
<tr><th id="281">281</th><td></td></tr>
<tr><th id="282">282</th><td>      <i>// Delete async context if it was the last block.</i></td></tr>
<tr><th id="283">283</th><td>      <b>if</b> (ctx-&gt;count.fetch_sub(<var>1</var>) == <var>1</var>) <b>delete</b> ctx;</td></tr>
<tr><th id="284">284</th><td>    };</td></tr>
<tr><th id="285">285</th><td></td></tr>
<tr><th id="286">286</th><td>    <b>if</b> (block.count &lt;= numThreads()) {</td></tr>
<tr><th id="287">287</th><td>      <i>// Avoid a thread hop by running the root of the tree and one block on the</i></td></tr>
<tr><th id="288">288</th><td><i>      // main thread.</i></td></tr>
<tr><th id="289">289</th><td>      ctx-&gt;handle_range(<var>0</var>, n);</td></tr>
<tr><th id="290">290</th><td>    } <b>else</b> {</td></tr>
<tr><th id="291">291</th><td>      <i>// Execute the root in the thread pool to avoid running work on more than</i></td></tr>
<tr><th id="292">292</th><td><i>      // numThreads() threads.</i></td></tr>
<tr><th id="293">293</th><td>      pool_-&gt;Schedule([ctx, n]() { ctx-&gt;handle_range(<var>0</var>, n); });</td></tr>
<tr><th id="294">294</th><td>    }</td></tr>
<tr><th id="295">295</th><td>  }</td></tr>
<tr><th id="296">296</th><td></td></tr>
<tr><th id="297">297</th><td>  <i>// Convenience wrapper for parallelForAsync that does not align blocks.</i></td></tr>
<tr><th id="298">298</th><td>  <em>void</em> parallelForAsync(Index n, <em>const</em> TensorOpCost&amp; cost,</td></tr>
<tr><th id="299">299</th><td>                        std::function&lt;<em>void</em>(Index, Index)&gt; f,</td></tr>
<tr><th id="300">300</th><td>                        std::function&lt;<em>void</em>()&gt; done) <em>const</em> {</td></tr>
<tr><th id="301">301</th><td>    parallelForAsync(n, cost, <b>nullptr</b>, std::move(f), std::move(done));</td></tr>
<tr><th id="302">302</th><td>  }</td></tr>
<tr><th id="303">303</th><td></td></tr>
<tr><th id="304">304</th><td>  <i>// Thread pool accessor.</i></td></tr>
<tr><th id="305">305</th><td>  ThreadPoolInterface* getPool() <em>const</em> { <b>return</b> pool_; }</td></tr>
<tr><th id="306">306</th><td></td></tr>
<tr><th id="307">307</th><td>  <i>// Allocator accessor.</i></td></tr>
<tr><th id="308">308</th><td>  Allocator* allocator() <em>const</em> { <b>return</b> allocator_; }</td></tr>
<tr><th id="309">309</th><td></td></tr>
<tr><th id="310">310</th><td> <b>private</b>:</td></tr>
<tr><th id="311">311</th><td>  <b>typedef</b> TensorCostModel&lt;ThreadPoolDevice&gt; CostModel;</td></tr>
<tr><th id="312">312</th><td></td></tr>
<tr><th id="313">313</th><td>  <i>// For parallelForAsync we must keep passed in closures on the heap, and</i></td></tr>
<tr><th id="314">314</th><td><i>  // delete them only after `done` callback finished.</i></td></tr>
<tr><th id="315">315</th><td>  <b>struct</b> ParallelForAsyncContext {</td></tr>
<tr><th id="316">316</th><td>    ParallelForAsyncContext(Index block_count,</td></tr>
<tr><th id="317">317</th><td>                            std::function&lt;<em>void</em>(Index, Index)&gt; block_f,</td></tr>
<tr><th id="318">318</th><td>                            std::function&lt;<em>void</em>()&gt; done_callback)</td></tr>
<tr><th id="319">319</th><td>        : count(block_count),</td></tr>
<tr><th id="320">320</th><td>          f(std::move(block_f)),</td></tr>
<tr><th id="321">321</th><td>          done(std::move(done_callback)) {}</td></tr>
<tr><th id="322">322</th><td>    ~ParallelForAsyncContext() { done(); }</td></tr>
<tr><th id="323">323</th><td></td></tr>
<tr><th id="324">324</th><td>    std::atomic&lt;Index&gt; count;</td></tr>
<tr><th id="325">325</th><td>    std::function&lt;<em>void</em>(Index, Index)&gt; f;</td></tr>
<tr><th id="326">326</th><td>    std::function&lt;<em>void</em>()&gt; done;</td></tr>
<tr><th id="327">327</th><td></td></tr>
<tr><th id="328">328</th><td>    std::function&lt;<em>void</em>(Index, Index)&gt; handle_range;</td></tr>
<tr><th id="329">329</th><td>  };</td></tr>
<tr><th id="330">330</th><td></td></tr>
<tr><th id="331">331</th><td>  <b>struct</b> ParallelForBlock {</td></tr>
<tr><th id="332">332</th><td>    Index size;   <i>// block size</i></td></tr>
<tr><th id="333">333</th><td>    Index count;  <i>// number of blocks</i></td></tr>
<tr><th id="334">334</th><td>  };</td></tr>
<tr><th id="335">335</th><td></td></tr>
<tr><th id="336">336</th><td>  <i>// Calculates block size based on (1) the iteration cost and (2) parallel</i></td></tr>
<tr><th id="337">337</th><td><i>  // efficiency. We want blocks to be not too small to mitigate parallelization</i></td></tr>
<tr><th id="338">338</th><td><i>  // overheads; not too large to mitigate tail effect and potential load</i></td></tr>
<tr><th id="339">339</th><td><i>  // imbalance and we also want number of blocks to be evenly dividable across</i></td></tr>
<tr><th id="340">340</th><td><i>  // threads.</i></td></tr>
<tr><th id="341">341</th><td>  ParallelForBlock CalculateParallelForBlock(</td></tr>
<tr><th id="342">342</th><td>      <em>const</em> Index n, <em>const</em> TensorOpCost&amp; cost,</td></tr>
<tr><th id="343">343</th><td>      std::function&lt;Index(Index)&gt; block_align) <em>const</em> {</td></tr>
<tr><th id="344">344</th><td>    <em>const</em> <em>double</em> block_size_f = <var>1.0</var> / CostModel::taskSize(<var>1</var>, cost);</td></tr>
<tr><th id="345">345</th><td>    <em>const</em> Index max_oversharding_factor = <var>4</var>;</td></tr>
<tr><th id="346">346</th><td>    Index block_size = numext::mini(</td></tr>
<tr><th id="347">347</th><td>        n, numext::maxi&lt;Index&gt;(</td></tr>
<tr><th id="348">348</th><td>               divup&lt;Index&gt;(n, max_oversharding_factor * numThreads()),</td></tr>
<tr><th id="349">349</th><td>               block_size_f));</td></tr>
<tr><th id="350">350</th><td>    <em>const</em> Index max_block_size = numext::mini(n, <var>2</var> * block_size);</td></tr>
<tr><th id="351">351</th><td></td></tr>
<tr><th id="352">352</th><td>    <b>if</b> (block_align) {</td></tr>
<tr><th id="353">353</th><td>      Index new_block_size = block_align(block_size);</td></tr>
<tr><th id="354">354</th><td>      eigen_assert(new_block_size &gt;= block_size);</td></tr>
<tr><th id="355">355</th><td>      block_size = numext::mini(n, new_block_size);</td></tr>
<tr><th id="356">356</th><td>    }</td></tr>
<tr><th id="357">357</th><td></td></tr>
<tr><th id="358">358</th><td>    Index block_count = divup(n, block_size);</td></tr>
<tr><th id="359">359</th><td></td></tr>
<tr><th id="360">360</th><td>    <i>// Calculate parallel efficiency as fraction of total CPU time used for</i></td></tr>
<tr><th id="361">361</th><td><i>    // computations:</i></td></tr>
<tr><th id="362">362</th><td>    <em>double</em> max_efficiency =</td></tr>
<tr><th id="363">363</th><td>        <b>static_cast</b>&lt;<em>double</em>&gt;(block_count) /</td></tr>
<tr><th id="364">364</th><td>        (divup&lt;<em>int</em>&gt;(block_count, numThreads()) * numThreads());</td></tr>
<tr><th id="365">365</th><td></td></tr>
<tr><th id="366">366</th><td>    <i>// Now try to increase block size up to max_block_size as long as it</i></td></tr>
<tr><th id="367">367</th><td><i>    // doesn't decrease parallel efficiency.</i></td></tr>
<tr><th id="368">368</th><td>    <b>for</b> (Index prev_block_count = block_count;</td></tr>
<tr><th id="369">369</th><td>         max_efficiency &lt; <var>1.0</var> &amp;&amp; prev_block_count &gt; <var>1</var>;) {</td></tr>
<tr><th id="370">370</th><td>      <i>// This is the next block size that divides size into a smaller number</i></td></tr>
<tr><th id="371">371</th><td><i>      // of blocks than the current block_size.</i></td></tr>
<tr><th id="372">372</th><td>      Index coarser_block_size = divup(n, prev_block_count - <var>1</var>);</td></tr>
<tr><th id="373">373</th><td>      <b>if</b> (block_align) {</td></tr>
<tr><th id="374">374</th><td>        Index new_block_size = block_align(coarser_block_size);</td></tr>
<tr><th id="375">375</th><td>        eigen_assert(new_block_size &gt;= coarser_block_size);</td></tr>
<tr><th id="376">376</th><td>        coarser_block_size = numext::mini(n, new_block_size);</td></tr>
<tr><th id="377">377</th><td>      }</td></tr>
<tr><th id="378">378</th><td>      <b>if</b> (coarser_block_size &gt; max_block_size) {</td></tr>
<tr><th id="379">379</th><td>        <b>break</b>;  <i>// Reached max block size. Stop.</i></td></tr>
<tr><th id="380">380</th><td>      }</td></tr>
<tr><th id="381">381</th><td>      <i>// Recalculate parallel efficiency.</i></td></tr>
<tr><th id="382">382</th><td>      <em>const</em> Index coarser_block_count = divup(n, coarser_block_size);</td></tr>
<tr><th id="383">383</th><td>      eigen_assert(coarser_block_count &lt; prev_block_count);</td></tr>
<tr><th id="384">384</th><td>      prev_block_count = coarser_block_count;</td></tr>
<tr><th id="385">385</th><td>      <em>const</em> <em>double</em> coarser_efficiency =</td></tr>
<tr><th id="386">386</th><td>          <b>static_cast</b>&lt;<em>double</em>&gt;(coarser_block_count) /</td></tr>
<tr><th id="387">387</th><td>          (divup&lt;<em>int</em>&gt;(coarser_block_count, numThreads()) * numThreads());</td></tr>
<tr><th id="388">388</th><td>      <b>if</b> (coarser_efficiency + <var>0.01</var> &gt;= max_efficiency) {</td></tr>
<tr><th id="389">389</th><td>        <i>// Taking it.</i></td></tr>
<tr><th id="390">390</th><td>        block_size = coarser_block_size;</td></tr>
<tr><th id="391">391</th><td>        block_count = coarser_block_count;</td></tr>
<tr><th id="392">392</th><td>        <b>if</b> (max_efficiency &lt; coarser_efficiency) {</td></tr>
<tr><th id="393">393</th><td>          max_efficiency = coarser_efficiency;</td></tr>
<tr><th id="394">394</th><td>        }</td></tr>
<tr><th id="395">395</th><td>      }</td></tr>
<tr><th id="396">396</th><td>    }</td></tr>
<tr><th id="397">397</th><td></td></tr>
<tr><th id="398">398</th><td>    <b>return</b> {block_size, block_count};</td></tr>
<tr><th id="399">399</th><td>  }</td></tr>
<tr><th id="400">400</th><td></td></tr>
<tr><th id="401">401</th><td>  ThreadPoolInterface* pool_;</td></tr>
<tr><th id="402">402</th><td>  <em>int</em> num_threads_;</td></tr>
<tr><th id="403">403</th><td>  Allocator* allocator_;</td></tr>
<tr><th id="404">404</th><td>};</td></tr>
<tr><th id="405">405</th><td></td></tr>
<tr><th id="406">406</th><td></td></tr>
<tr><th id="407">407</th><td>}  <i>// end namespace Eigen</i></td></tr>
<tr><th id="408">408</th><td></td></tr>
<tr><th id="409">409</th><td><u>#<span data-ppcond="10">endif</span> // EIGEN_CXX11_TENSOR_TENSOR_DEVICE_THREAD_POOL_H</u></td></tr>
<tr><th id="410">410</th><td></td></tr>
</table><hr/><p id='footer'>
Generated while processing <a href='../../../../../../_deps/tflite-src/tensorflow/lite/kernels/activations.cc.html'>halide/build-apps/_deps/tflite-src/tensorflow/lite/kernels/activations.cc</a><br/>Generated on <em>2021-Aug-05</em> from project halide revision <em>v12.0.1</em>