<!doctype html>
<html>
<head>
<meta name="viewport" content="width=device-width, initial-scale=1.0"><title>TensorReductionGpu.h source code [halide/build-apps/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorReductionGpu.h] - Woboq Code Browser</title>
<link rel="stylesheet" href="../../../../../../../.././data/qtcreator.css" title="QtCreator"/>
<link rel="alternate stylesheet" href="../../../../../../../.././data/kdevelop.css" title="KDevelop"/>
<script type="text/javascript" src="../../../../../../../.././data/jquery/jquery.min.js"></script>
<script type="text/javascript" src="../../../../../../../.././data/jquery/jquery-ui.min.js"></script>
<script>var file = 'halide/build-apps/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorReductionGpu.h'; var root_path = '../../../../../../../..'; var data_path = '../../../../../../../.././data'; var ecma_script_api_version = 2;</script>
<script src='../../../../../../../.././data/codebrowser.js'></script>
</head>
<body><div id='header'><h1 id='breadcrumb'><span>Browse the source code of </span><a href='../../../../../../..'>halide</a>/<a href='../../../../../..'>build-apps</a>/<a href='../../../../..'>eigen</a>/<a href='../../../..'>unsupported</a>/<a href='../../..'>Eigen</a>/<a href='../..'>CXX11</a>/<a href='..'>src</a>/<a href='./'>Tensor</a>/<a href='TensorReductionGpu.h.html'>TensorReductionGpu.h</a></h1></div>
<hr/><div id='content'><table class="code">
<tr><th id="1">1</th><td><i>// This file is part of Eigen, a lightweight C++ template library</i></td></tr>
<tr><th id="2">2</th><td><i>// for linear algebra.</i></td></tr>
<tr><th id="3">3</th><td><i>//</i></td></tr>
<tr><th id="4">4</th><td><i>// Copyright (C) 2014 Benoit Steiner &lt;benoit.steiner.goog@gmail.com&gt;</i></td></tr>
<tr><th id="5">5</th><td><i>//</i></td></tr>
<tr><th id="6">6</th><td><i>// This Source Code Form is subject to the terms of the Mozilla</i></td></tr>
<tr><th id="7">7</th><td><i>// Public License v. 2.0. If a copy of the MPL was not distributed</i></td></tr>
<tr><th id="8">8</th><td><i>// with this file, You can obtain one at <a href="http://mozilla.org/MPL/2.0/">http://mozilla.org/MPL/2.0/</a>.</i></td></tr>
<tr><th id="9">9</th><td></td></tr>
<tr><th id="10">10</th><td><u>#<span data-ppcond="10">ifndef</span> <span class="macro" data-ref="_M/EIGEN_CXX11_TENSOR_TENSOR_REDUCTION_GPU_H">EIGEN_CXX11_TENSOR_TENSOR_REDUCTION_GPU_H</span></u></td></tr>
<tr><th id="11">11</th><td><u>#define <dfn class="macro" id="_M/EIGEN_CXX11_TENSOR_TENSOR_REDUCTION_GPU_H" data-ref="_M/EIGEN_CXX11_TENSOR_TENSOR_REDUCTION_GPU_H">EIGEN_CXX11_TENSOR_TENSOR_REDUCTION_GPU_H</dfn></u></td></tr>
<tr><th id="12">12</th><td></td></tr>
<tr><th id="13">13</th><td><b>namespace</b> <span class="namespace">Eigen</span> {</td></tr>
<tr><th id="14">14</th><td><b>namespace</b> <span class="namespace">internal</span> {</td></tr>
<tr><th id="15">15</th><td></td></tr>
<tr><th id="16">16</th><td></td></tr>
<tr><th id="17">17</th><td><u>#<span data-ppcond="17">if</span> defined(<span class="macro" data-ref="_M/EIGEN_USE_GPU">EIGEN_USE_GPU</span>) &amp;&amp; defined(<span class="macro" data-ref="_M/EIGEN_GPUCC">EIGEN_GPUCC</span>)</u></td></tr>
<tr><th id="18">18</th><td><i>// Full reducers for GPU, don't vectorize for now</i></td></tr>
<tr><th id="19">19</th><td><i></i></td></tr>
<tr><th id="20">20</th><td><i>// Reducer function that enables multiple gpu thread to safely accumulate at the same</i></td></tr>
<tr><th id="21">21</th><td><i>// output address. It basically reads the current value of the output variable, and</i></td></tr>
<tr><th id="22">22</th><td><i>// attempts to update it with the new value. If in the meantime another gpu thread</i></td></tr>
<tr><th id="23">23</th><td><i>// updated the content of the output address it will try again.</i></td></tr>
<tr><th id="24">24</th><td><b>template</b> &lt;<b>typename</b> T, <b>typename</b> R&gt;</td></tr>
<tr><th id="25">25</th><td>__device__ EIGEN_ALWAYS_INLINE <em>void</em> atomicReduce(T* output, T accum, R&amp; reducer) {</td></tr>
<tr><th id="26">26</th><td><u>#if (defined(EIGEN_HIP_DEVICE_COMPILE) &amp;&amp; defined(__HIP_ARCH_HAS_WARP_SHUFFLE__)) || (EIGEN_CUDA_ARCH &gt;= 300)</u></td></tr>
<tr><th id="27">27</th><td>  <b>if</b> (<b>sizeof</b>(T) == <var>4</var>)</td></tr>
<tr><th id="28">28</th><td>  {</td></tr>
<tr><th id="29">29</th><td>    <em>unsigned</em> <em>int</em> oldval = *<b>reinterpret_cast</b>&lt;<em>unsigned</em> <em>int</em>*&gt;(output);</td></tr>
<tr><th id="30">30</th><td>    <em>unsigned</em> <em>int</em> newval = oldval;</td></tr>
<tr><th id="31">31</th><td>    reducer.reduce(accum, <b>reinterpret_cast</b>&lt;T*&gt;(&amp;newval));</td></tr>
<tr><th id="32">32</th><td>    <b>if</b> (newval == oldval) {</td></tr>
<tr><th id="33">33</th><td>      <b>return</b>;</td></tr>
<tr><th id="34">34</th><td>    }</td></tr>
<tr><th id="35">35</th><td>    <em>unsigned</em> <em>int</em> readback;</td></tr>
<tr><th id="36">36</th><td>    <b>while</b> ((readback = atomicCAS((<em>unsigned</em> <em>int</em>*)output, oldval, newval)) != oldval) {</td></tr>
<tr><th id="37">37</th><td>      oldval = readback;</td></tr>
<tr><th id="38">38</th><td>      newval = oldval;</td></tr>
<tr><th id="39">39</th><td>      reducer.reduce(accum, <b>reinterpret_cast</b>&lt;T*&gt;(&amp;newval));</td></tr>
<tr><th id="40">40</th><td>      <b>if</b> (newval == oldval) {</td></tr>
<tr><th id="41">41</th><td>        <b>return</b>;</td></tr>
<tr><th id="42">42</th><td>      }</td></tr>
<tr><th id="43">43</th><td>    }</td></tr>
<tr><th id="44">44</th><td>  }</td></tr>
<tr><th id="45">45</th><td>  <b>else</b> <b>if</b> (<b>sizeof</b>(T) == <var>8</var>) {</td></tr>
<tr><th id="46">46</th><td>    <em>unsigned</em> <em>long</em> <em>long</em> oldval = *<b>reinterpret_cast</b>&lt;<em>unsigned</em> <em>long</em> <em>long</em>*&gt;(output);</td></tr>
<tr><th id="47">47</th><td>    <em>unsigned</em> <em>long</em> <em>long</em> newval = oldval;</td></tr>
<tr><th id="48">48</th><td>    reducer.reduce(accum, <b>reinterpret_cast</b>&lt;T*&gt;(&amp;newval));</td></tr>
<tr><th id="49">49</th><td>    <b>if</b> (newval == oldval) {</td></tr>
<tr><th id="50">50</th><td>      <b>return</b>;</td></tr>
<tr><th id="51">51</th><td>    }</td></tr>
<tr><th id="52">52</th><td>    <em>unsigned</em> <em>long</em> <em>long</em> readback;</td></tr>
<tr><th id="53">53</th><td>    <b>while</b> ((readback = atomicCAS((<em>unsigned</em> <em>long</em> <em>long</em>*)output, oldval, newval)) != oldval) {</td></tr>
<tr><th id="54">54</th><td>      oldval = readback;</td></tr>
<tr><th id="55">55</th><td>      newval = oldval;</td></tr>
<tr><th id="56">56</th><td>      reducer.reduce(accum, <b>reinterpret_cast</b>&lt;T*&gt;(&amp;newval));</td></tr>
<tr><th id="57">57</th><td>      <b>if</b> (newval == oldval) {</td></tr>
<tr><th id="58">58</th><td>        <b>return</b>;</td></tr>
<tr><th id="59">59</th><td>      }</td></tr>
<tr><th id="60">60</th><td>    }</td></tr>
<tr><th id="61">61</th><td>  }</td></tr>
<tr><th id="62">62</th><td>  <b>else</b> {</td></tr>
<tr><th id="63">63</th><td>    gpu_assert(<var>0</var> &amp;&amp; <q>"Wordsize not supported"</q>);</td></tr>
<tr><th id="64">64</th><td>  }</td></tr>
<tr><th id="65">65</th><td><u>#else // EIGEN_CUDA_ARCH &gt;= 300</u></td></tr>
<tr><th id="66">66</th><td>  gpu_assert(<var>0</var> &amp;&amp; <q>"Shouldn't be called on unsupported device"</q>);</td></tr>
<tr><th id="67">67</th><td><u>#endif // EIGEN_CUDA_ARCH &gt;= 300</u></td></tr>
<tr><th id="68">68</th><td>}</td></tr>
<tr><th id="69">69</th><td></td></tr>
<tr><th id="70">70</th><td><i>// We extend atomicExch to support extra data types</i></td></tr>
<tr><th id="71">71</th><td><b>template</b> &lt;<b>typename</b> Type&gt;</td></tr>
<tr><th id="72">72</th><td>__device__ <b>inline</b> Type atomicExchCustom(Type* address, Type val) {</td></tr>
<tr><th id="73">73</th><td>  <b>return</b> atomicExch(address, val);</td></tr>
<tr><th id="74">74</th><td>}</td></tr>
<tr><th id="75">75</th><td></td></tr>
<tr><th id="76">76</th><td><b>template</b> &lt;&gt;</td></tr>
<tr><th id="77">77</th><td>__device__ <b>inline</b> <em>double</em> atomicExchCustom(<em>double</em>* address, <em>double</em> val) {</td></tr>
<tr><th id="78">78</th><td>  <em>unsigned</em> <em>long</em> <em>long</em> <em>int</em>* address_as_ull = <b>reinterpret_cast</b>&lt;<em>unsigned</em> <em>long</em> <em>long</em> <em>int</em>*&gt;(address);</td></tr>
<tr><th id="79">79</th><td>  <b>return</b> __longlong_as_double(atomicExch(address_as_ull, __double_as_longlong(val)));</td></tr>
<tr><th id="80">80</th><td>}</td></tr>
<tr><th id="81">81</th><td></td></tr>
<tr><th id="82">82</th><td><u>#ifdef EIGEN_HAS_GPU_FP16</u></td></tr>
<tr><th id="83">83</th><td><b>template</b> &lt;<b>template</b> &lt;<b>typename</b> T&gt; <b>class</b> R&gt;</td></tr>
<tr><th id="84">84</th><td>__device__ <b>inline</b> <em>void</em> atomicReduce(half2* output, half2 accum, R&lt;half&gt;&amp; reducer) {</td></tr>
<tr><th id="85">85</th><td>  <em>unsigned</em> <em>int</em> oldval = *<b>reinterpret_cast</b>&lt;<em>unsigned</em> <em>int</em>*&gt;(output);</td></tr>
<tr><th id="86">86</th><td>  <em>unsigned</em> <em>int</em> newval = oldval;</td></tr>
<tr><th id="87">87</th><td>  reducer.reducePacket(accum, <b>reinterpret_cast</b>&lt;half2*&gt;(&amp;newval));</td></tr>
<tr><th id="88">88</th><td>  <b>if</b> (newval == oldval) {</td></tr>
<tr><th id="89">89</th><td>    <b>return</b>;</td></tr>
<tr><th id="90">90</th><td>  }</td></tr>
<tr><th id="91">91</th><td>  <em>unsigned</em> <em>int</em> readback;</td></tr>
<tr><th id="92">92</th><td>  <b>while</b> ((readback = atomicCAS((<em>unsigned</em> <em>int</em>*)output, oldval, newval)) != oldval) {</td></tr>
<tr><th id="93">93</th><td>    oldval = readback;</td></tr>
<tr><th id="94">94</th><td>    newval = oldval;</td></tr>
<tr><th id="95">95</th><td>    reducer.reducePacket(accum, <b>reinterpret_cast</b>&lt;half2*&gt;(&amp;newval));</td></tr>
<tr><th id="96">96</th><td>    <b>if</b> (newval == oldval) {</td></tr>
<tr><th id="97">97</th><td>      <b>return</b>;</td></tr>
<tr><th id="98">98</th><td>    }</td></tr>
<tr><th id="99">99</th><td>  }</td></tr>
<tr><th id="100">100</th><td>}</td></tr>
<tr><th id="101">101</th><td><i>// reduction should be associative since reduction is not atomic in wide vector but atomic in half2 operations</i></td></tr>
<tr><th id="102">102</th><td><b>template</b> &lt;<b>template</b> &lt;<b>typename</b> T&gt; <b>class</b> R&gt;</td></tr>
<tr><th id="103">103</th><td>__device__ <b>inline</b> <em>void</em> atomicReduce(Packet4h2* output, Packet4h2 accum,</td></tr>
<tr><th id="104">104</th><td>                                    R&lt;half&gt;&amp; reducer) {</td></tr>
<tr><th id="105">105</th><td>  half2* houtput=<b>reinterpret_cast</b>&lt;half2*&gt;(output);</td></tr>
<tr><th id="106">106</th><td>  half2* haccum=<b>reinterpret_cast</b>&lt;half2*&gt;(&amp;accum);</td></tr>
<tr><th id="107">107</th><td>  <b>for</b>(<em>int</em> i=<var>0</var>;i&lt;<var>4</var>;++i){</td></tr>
<tr><th id="108">108</th><td>    atomicReduce(houtput+i,*(haccum+i),reducer);</td></tr>
<tr><th id="109">109</th><td>  }</td></tr>
<tr><th id="110">110</th><td>}</td></tr>
<tr><th id="111">111</th><td><u>#endif  // EIGEN_HAS_GPU_FP16</u></td></tr>
<tr><th id="112">112</th><td></td></tr>
<tr><th id="113">113</th><td><b>template</b> &lt;&gt;</td></tr>
<tr><th id="114">114</th><td>__device__ <b>inline</b> <em>void</em> atomicReduce(<em>float</em>* output, <em>float</em> accum, SumReducer&lt;<em>float</em>&gt;&amp;) {</td></tr>
<tr><th id="115">115</th><td><u>#if (defined(EIGEN_HIP_DEVICE_COMPILE) &amp;&amp; defined(__HIP_ARCH_HAS_WARP_SHUFFLE__)) || (EIGEN_CUDA_ARCH &gt;= 300)</u></td></tr>
<tr><th id="116">116</th><td>  atomicAdd(output, accum);</td></tr>
<tr><th id="117">117</th><td><u>#else // EIGEN_CUDA_ARCH &gt;= 300</u></td></tr>
<tr><th id="118">118</th><td>  gpu_assert(<var>0</var> &amp;&amp; <q>"Shouldn't be called on unsupported device"</q>);</td></tr>
<tr><th id="119">119</th><td><u>#endif // EIGEN_CUDA_ARCH &gt;= 300</u></td></tr>
<tr><th id="120">120</th><td>}</td></tr>
<tr><th id="121">121</th><td></td></tr>
<tr><th id="122">122</th><td></td></tr>
<tr><th id="123">123</th><td><b>template</b> &lt;<b>typename</b> CoeffType, <b>typename</b> Index&gt;</td></tr>
<tr><th id="124">124</th><td>__global__ __launch_bounds__(<var>1024</var>) <em>void</em> ReductionInitKernel(<em>const</em> CoeffType val, Index num_preserved_coeffs, CoeffType* output) {</td></tr>
<tr><th id="125">125</th><td>  <em>const</em> Index thread_id = blockIdx.x * blockDim.x + threadIdx.x;</td></tr>
<tr><th id="126">126</th><td>  <em>const</em> Index num_threads = blockDim.x * gridDim.x;</td></tr>
<tr><th id="127">127</th><td>  <b>for</b> (Index i = thread_id; i &lt; num_preserved_coeffs; i += num_threads) {</td></tr>
<tr><th id="128">128</th><td>    output[i] = val;</td></tr>
<tr><th id="129">129</th><td>  }</td></tr>
<tr><th id="130">130</th><td>}</td></tr>
<tr><th id="131">131</th><td></td></tr>
<tr><th id="132">132</th><td></td></tr>
<tr><th id="133">133</th><td><b>template</b> &lt;<em>int</em> BlockSize, <em>int</em> NumPerThread, <b>typename</b> Self,</td></tr>
<tr><th id="134">134</th><td>          <b>typename</b> Reducer, <b>typename</b> Index&gt;</td></tr>
<tr><th id="135">135</th><td>__global__ __launch_bounds__(<var>1024</var>) <em>void</em> FullReductionKernel(Reducer reducer, <em>const</em> Self input, Index num_coeffs,</td></tr>
<tr><th id="136">136</th><td>                                    <b>typename</b> Self::CoeffReturnType* output, <em>unsigned</em> <em>int</em>* semaphore) {</td></tr>
<tr><th id="137">137</th><td><u>#if (defined(EIGEN_HIP_DEVICE_COMPILE) &amp;&amp; defined(__HIP_ARCH_HAS_WARP_SHUFFLE__)) || (EIGEN_CUDA_ARCH &gt;= 300)</u></td></tr>
<tr><th id="138">138</th><td>  <i>// Initialize the output value</i></td></tr>
<tr><th id="139">139</th><td>  <em>const</em> Index first_index = blockIdx.x * BlockSize * NumPerThread + threadIdx.x;</td></tr>
<tr><th id="140">140</th><td>  <b>if</b> (gridDim.x == <var>1</var>) {</td></tr>
<tr><th id="141">141</th><td>    <b>if</b> (first_index == <var>0</var>) {</td></tr>
<tr><th id="142">142</th><td>      *output = reducer.initialize();</td></tr>
<tr><th id="143">143</th><td>    }</td></tr>
<tr><th id="144">144</th><td>  }</td></tr>
<tr><th id="145">145</th><td>  <b>else</b> {</td></tr>
<tr><th id="146">146</th><td>    <b>if</b> (threadIdx.x == <var>0</var>) {</td></tr>
<tr><th id="147">147</th><td>      <em>unsigned</em> <em>int</em> block = atomicCAS(semaphore, <var>0u</var>, <var>1u</var>);</td></tr>
<tr><th id="148">148</th><td>      <b>if</b> (block == <var>0</var>) {</td></tr>
<tr><th id="149">149</th><td>        <i>// We're the first block to run, initialize the output value</i></td></tr>
<tr><th id="150">150</th><td>        atomicExchCustom(output, reducer.initialize());</td></tr>
<tr><th id="151">151</th><td>        __threadfence();</td></tr>
<tr><th id="152">152</th><td>        atomicExch(semaphore, <var>2u</var>);</td></tr>
<tr><th id="153">153</th><td>      }</td></tr>
<tr><th id="154">154</th><td>      <b>else</b> {</td></tr>
<tr><th id="155">155</th><td>        <i>// Wait for the first block to initialize the output value.</i></td></tr>
<tr><th id="156">156</th><td><i>        // Use atomicCAS here to ensure that the reads aren't cached</i></td></tr>
<tr><th id="157">157</th><td>        <em>unsigned</em> <em>int</em> val;</td></tr>
<tr><th id="158">158</th><td>        <b>do</b> {</td></tr>
<tr><th id="159">159</th><td>          val = atomicCAS(semaphore, <var>2u</var>, <var>2u</var>);</td></tr>
<tr><th id="160">160</th><td>        }</td></tr>
<tr><th id="161">161</th><td>        <b>while</b> (val &lt; <var>2u</var>);</td></tr>
<tr><th id="162">162</th><td>      }</td></tr>
<tr><th id="163">163</th><td>    }</td></tr>
<tr><th id="164">164</th><td>  }</td></tr>
<tr><th id="165">165</th><td></td></tr>
<tr><th id="166">166</th><td>  __syncthreads();</td></tr>
<tr><th id="167">167</th><td></td></tr>
<tr><th id="168">168</th><td>  eigen_assert(gridDim.x == <var>1</var> || *semaphore &gt;= <var>2u</var>);</td></tr>
<tr><th id="169">169</th><td></td></tr>
<tr><th id="170">170</th><td>  <b>typename</b> Self::CoeffReturnType accum = reducer.initialize();</td></tr>
<tr><th id="171">171</th><td>  Index max_iter = numext::mini&lt;Index&gt;(num_coeffs - first_index, NumPerThread*BlockSize);</td></tr>
<tr><th id="172">172</th><td>  <b>for</b> (Index i = <var>0</var>; i &lt; max_iter; i+=BlockSize) {</td></tr>
<tr><th id="173">173</th><td>    <em>const</em> Index index = first_index + i;</td></tr>
<tr><th id="174">174</th><td>    eigen_assert(index &lt; num_coeffs);</td></tr>
<tr><th id="175">175</th><td>    <b>typename</b> Self::CoeffReturnType val = input.m_impl.coeff(index);</td></tr>
<tr><th id="176">176</th><td>    reducer.reduce(val, &amp;accum);</td></tr>
<tr><th id="177">177</th><td>  }</td></tr>
<tr><th id="178">178</th><td></td></tr>
<tr><th id="179">179</th><td><u>#pragma unroll</u></td></tr>
<tr><th id="180">180</th><td>  <b>for</b> (<em>int</em> offset = warpSize/<var>2</var>; offset &gt; <var>0</var>; offset /= <var>2</var>) {</td></tr>
<tr><th id="181">181</th><td>  <u>#if defined(EIGEN_HIPCC)</u></td></tr>
<tr><th id="182">182</th><td>    <i>// use std::is_floating_point to determine the type of reduced_val </i></td></tr>
<tr><th id="183">183</th><td><i>    // This is needed because when Type == double, hipcc will give a "call to __shfl_down is ambguous" error </i></td></tr>
<tr><th id="184">184</th><td><i>    // and list the float and int versions of __shfl_down as the candidate functions. </i></td></tr>
<tr><th id="185">185</th><td>    <b>if</b> (std::is_floating_point&lt;<b>typename</b> Self::CoeffReturnType&gt;::value) {</td></tr>
<tr><th id="186">186</th><td>      reducer.reduce(__shfl_down(<b>static_cast</b>&lt;<em>float</em>&gt;(accum), offset, warpSize), &amp;accum);</td></tr>
<tr><th id="187">187</th><td>    } <b>else</b> {</td></tr>
<tr><th id="188">188</th><td>      reducer.reduce(__shfl_down(<b>static_cast</b>&lt;<em>int</em>&gt;(accum), offset, warpSize), &amp;accum);</td></tr>
<tr><th id="189">189</th><td>    }</td></tr>
<tr><th id="190">190</th><td>  <u>#elif defined(EIGEN_CUDA_SDK_VER) &amp;&amp; EIGEN_CUDA_SDK_VER &lt; 90000</u></td></tr>
<tr><th id="191">191</th><td>    reducer.reduce(__shfl_down(accum, offset, warpSize), &amp;accum);</td></tr>
<tr><th id="192">192</th><td>  <u>#else</u></td></tr>
<tr><th id="193">193</th><td>    reducer.reduce(__shfl_down_sync(<var>0xFFFFFFFF</var>, accum, offset, warpSize), &amp;accum);</td></tr>
<tr><th id="194">194</th><td>  <u>#endif</u></td></tr>
<tr><th id="195">195</th><td>  }</td></tr>
<tr><th id="196">196</th><td></td></tr>
<tr><th id="197">197</th><td>  <b>if</b> ((threadIdx.x &amp; (warpSize - <var>1</var>)) == <var>0</var>) {</td></tr>
<tr><th id="198">198</th><td>    atomicReduce(output, accum, reducer);</td></tr>
<tr><th id="199">199</th><td>  }</td></tr>
<tr><th id="200">200</th><td></td></tr>
<tr><th id="201">201</th><td>  <b>if</b> (gridDim.x &gt; <var>1</var> &amp;&amp; threadIdx.x == <var>0</var>) {</td></tr>
<tr><th id="202">202</th><td>    <i>// Let the last block reset the semaphore</i></td></tr>
<tr><th id="203">203</th><td>    atomicInc(semaphore, gridDim.x + <var>1</var>);</td></tr>
<tr><th id="204">204</th><td><u>#if defined(EIGEN_HIPCC)</u></td></tr>
<tr><th id="205">205</th><td>    __threadfence_system();</td></tr>
<tr><th id="206">206</th><td><u>#endif</u></td></tr>
<tr><th id="207">207</th><td>  }</td></tr>
<tr><th id="208">208</th><td><u>#else // EIGEN_CUDA_ARCH &gt;= 300</u></td></tr>
<tr><th id="209">209</th><td>  gpu_assert(<var>0</var> &amp;&amp; <q>"Shouldn't be called on unsupported device"</q>);</td></tr>
<tr><th id="210">210</th><td><u>#endif // EIGEN_CUDA_ARCH &gt;= 300</u></td></tr>
<tr><th id="211">211</th><td>}</td></tr>
<tr><th id="212">212</th><td></td></tr>
<tr><th id="213">213</th><td></td></tr>
<tr><th id="214">214</th><td><u>#ifdef EIGEN_HAS_GPU_FP16</u></td></tr>
<tr><th id="215">215</th><td><b>template</b> &lt;<b>typename</b> Self,</td></tr>
<tr><th id="216">216</th><td>          <b>typename</b> Reducer, <b>typename</b> Index&gt;</td></tr>
<tr><th id="217">217</th><td>__global__ __launch_bounds__(<var>1024</var>) <em>void</em> ReductionInitFullReduxKernelHalfFloat(Reducer reducer, <em>const</em> Self input, Index num_coeffs,</td></tr>
<tr><th id="218">218</th><td>                                                      packet_traits&lt;Eigen::half&gt;::type* scratch) {</td></tr>
<tr><th id="219">219</th><td>  eigen_assert(blockDim.x == <var>1</var>);</td></tr>
<tr><th id="220">220</th><td>  eigen_assert(gridDim.x == <var>1</var>);</td></tr>
<tr><th id="221">221</th><td>  <b>typedef</b> packet_traits&lt;Eigen::half&gt;::type packet_type;</td></tr>
<tr><th id="222">222</th><td>  Index packet_remainder =</td></tr>
<tr><th id="223">223</th><td>      num_coeffs % Index(unpacket_traits&lt;packet_type&gt;::size);</td></tr>
<tr><th id="224">224</th><td>  <b>if</b> (packet_remainder != <var>0</var>) {</td></tr>
<tr><th id="225">225</th><td>    half2* h2scratch = <b>reinterpret_cast</b>&lt;half2*&gt;(scratch);</td></tr>
<tr><th id="226">226</th><td>    <b>for</b> (Index i = num_coeffs - packet_remainder; i + <var>2</var> &lt;= num_coeffs; i += <var>2</var>) {</td></tr>
<tr><th id="227">227</th><td>      *h2scratch =</td></tr>
<tr><th id="228">228</th><td>          __halves2half2(input.m_impl.coeff(i), input.m_impl.coeff(i + <var>1</var>));</td></tr>
<tr><th id="229">229</th><td>      h2scratch++;</td></tr>
<tr><th id="230">230</th><td>    }</td></tr>
<tr><th id="231">231</th><td>    <b>if</b> ((num_coeffs &amp; <var>1</var>) != <var>0</var>) {</td></tr>
<tr><th id="232">232</th><td>      half lastCoeff = input.m_impl.coeff(num_coeffs - <var>1</var>);</td></tr>
<tr><th id="233">233</th><td>      *h2scratch = __halves2half2(lastCoeff, reducer.initialize());</td></tr>
<tr><th id="234">234</th><td>    }</td></tr>
<tr><th id="235">235</th><td>  } <b>else</b> {</td></tr>
<tr><th id="236">236</th><td>    *scratch = reducer.<b>template</b> initializePacket&lt;packet_type&gt;();</td></tr>
<tr><th id="237">237</th><td>  }</td></tr>
<tr><th id="238">238</th><td>}</td></tr>
<tr><th id="239">239</th><td></td></tr>
<tr><th id="240">240</th><td><b>template</b> &lt;<b>typename</b> Self,</td></tr>
<tr><th id="241">241</th><td>          <b>typename</b> Reducer, <b>typename</b> Index&gt;</td></tr>
<tr><th id="242">242</th><td>__global__ __launch_bounds__(<var>1024</var>) <em>void</em> ReductionInitKernelHalfFloat(Reducer reducer, <em>const</em> Self input, Index num_coeffs, half* output) {</td></tr>
<tr><th id="243">243</th><td>  <em>const</em> Index thread_id = blockIdx.x * blockDim.x + threadIdx.x;</td></tr>
<tr><th id="244">244</th><td>  <em>const</em> Index num_threads = blockDim.x * gridDim.x;</td></tr>
<tr><th id="245">245</th><td>  <b>typedef</b> <b>typename</b> packet_traits&lt;Eigen::half&gt;::type PacketType;</td></tr>
<tr><th id="246">246</th><td></td></tr>
<tr><th id="247">247</th><td>  <em>const</em> Index num_packets =</td></tr>
<tr><th id="248">248</th><td>      num_coeffs / Index(unpacket_traits&lt;PacketType&gt;::size);</td></tr>
<tr><th id="249">249</th><td>  PacketType* p_output = <b>reinterpret_cast</b>&lt;PacketType*&gt;(output);</td></tr>
<tr><th id="250">250</th><td>  <b>for</b> (Index i = thread_id; i &lt; num_packets; i += num_threads) {</td></tr>
<tr><th id="251">251</th><td>    p_output[i] = reducer.<b>template</b> initializePacket&lt;PacketType&gt;();</td></tr>
<tr><th id="252">252</th><td>  }</td></tr>
<tr><th id="253">253</th><td>  Index packet_remainder =</td></tr>
<tr><th id="254">254</th><td>      num_coeffs % Index(unpacket_traits&lt;PacketType&gt;::size);</td></tr>
<tr><th id="255">255</th><td>  <b>if</b> (thread_id &lt; packet_remainder) {</td></tr>
<tr><th id="256">256</th><td>    output[num_coeffs - packet_remainder + thread_id] = reducer.initialize();</td></tr>
<tr><th id="257">257</th><td>  }</td></tr>
<tr><th id="258">258</th><td>}</td></tr>
<tr><th id="259">259</th><td></td></tr>
<tr><th id="260">260</th><td><b>template</b> &lt;<em>int</em> BlockSize, <em>int</em> NumPerThread, <b>typename</b> Self,</td></tr>
<tr><th id="261">261</th><td>          <b>typename</b> Reducer, <b>typename</b> Index&gt;</td></tr>
<tr><th id="262">262</th><td>__global__ __launch_bounds__(<var>1024</var>) <em>void</em> FullReductionKernelHalfFloat(Reducer reducer, <em>const</em> Self input, Index num_coeffs,</td></tr>
<tr><th id="263">263</th><td>                                    half* output, packet_traits&lt;Eigen::half&gt;::type* scratch) {</td></tr>
<tr><th id="264">264</th><td>  <b>typedef</b> <b>typename</b> packet_traits&lt;Eigen::half&gt;::type PacketType;</td></tr>
<tr><th id="265">265</th><td>  <em>const</em> <em>int</em> packet_width = unpacket_traits&lt;PacketType&gt;::size;</td></tr>
<tr><th id="266">266</th><td>  eigen_assert(NumPerThread % packet_width == <var>0</var>);</td></tr>
<tr><th id="267">267</th><td>  <em>const</em> Index first_index =</td></tr>
<tr><th id="268">268</th><td>      blockIdx.x * BlockSize * NumPerThread + packet_width * threadIdx.x;</td></tr>
<tr><th id="269">269</th><td></td></tr>
<tr><th id="270">270</th><td>  <i>// Initialize the output value if it wasn't initialized by the ReductionInitKernel</i></td></tr>
<tr><th id="271">271</th><td></td></tr>
<tr><th id="272">272</th><td>  <b>if</b> (gridDim.x == <var>1</var>) {</td></tr>
<tr><th id="273">273</th><td>    <b>if</b> (first_index == <var>0</var>) {</td></tr>
<tr><th id="274">274</th><td>      <em>int</em> rem = num_coeffs % packet_width;</td></tr>
<tr><th id="275">275</th><td>      <b>if</b> (rem != <var>0</var>) {</td></tr>
<tr><th id="276">276</th><td>        half2* p_scratch = <b>reinterpret_cast</b>&lt;half2*&gt;(scratch);</td></tr>
<tr><th id="277">277</th><td>        *scratch = reducer.<b>template</b> initializePacket&lt;PacketType&gt;();</td></tr>
<tr><th id="278">278</th><td>        <b>for</b> (<em>int</em> i = <var>0</var>; i &lt; rem / <var>2</var>; i++) {</td></tr>
<tr><th id="279">279</th><td>          *p_scratch = __halves2half2(</td></tr>
<tr><th id="280">280</th><td>              input.m_impl.coeff(num_coeffs - packet_width + <var>2</var> * i),</td></tr>
<tr><th id="281">281</th><td>              input.m_impl.coeff(num_coeffs - packet_width + <var>2</var> * i + <var>1</var>));</td></tr>
<tr><th id="282">282</th><td>          p_scratch++;</td></tr>
<tr><th id="283">283</th><td>        }</td></tr>
<tr><th id="284">284</th><td>        <b>if</b> ((num_coeffs &amp; <var>1</var>) != <var>0</var>) {</td></tr>
<tr><th id="285">285</th><td>          half last = input.m_impl.coeff(num_coeffs - <var>1</var>);</td></tr>
<tr><th id="286">286</th><td>          *p_scratch = __halves2half2(last, reducer.initialize());</td></tr>
<tr><th id="287">287</th><td>        }</td></tr>
<tr><th id="288">288</th><td>      } <b>else</b> {</td></tr>
<tr><th id="289">289</th><td>        *scratch = reducer.<b>template</b> initializePacket&lt;PacketType&gt;();</td></tr>
<tr><th id="290">290</th><td>      }</td></tr>
<tr><th id="291">291</th><td>    }</td></tr>
<tr><th id="292">292</th><td>    __syncthreads();</td></tr>
<tr><th id="293">293</th><td>  }</td></tr>
<tr><th id="294">294</th><td></td></tr>
<tr><th id="295">295</th><td>  PacketType accum = reducer.<b>template</b> initializePacket&lt;PacketType&gt;();</td></tr>
<tr><th id="296">296</th><td>  <em>const</em> Index max_iter =</td></tr>
<tr><th id="297">297</th><td>      numext::mini&lt;Index&gt;((num_coeffs - first_index) / packet_width,</td></tr>
<tr><th id="298">298</th><td>                          NumPerThread * BlockSize / packet_width);</td></tr>
<tr><th id="299">299</th><td>  <b>for</b> (Index i = <var>0</var>; i &lt; max_iter; i += BlockSize) {</td></tr>
<tr><th id="300">300</th><td>    <em>const</em> Index index = first_index + packet_width * i;</td></tr>
<tr><th id="301">301</th><td>    eigen_assert(index + packet_width &lt; num_coeffs);</td></tr>
<tr><th id="302">302</th><td>    PacketType val = input.m_impl.<b>template</b> packet&lt;Unaligned&gt;(index);</td></tr>
<tr><th id="303">303</th><td>    reducer.reducePacket(val, &amp;accum);</td></tr>
<tr><th id="304">304</th><td>  }</td></tr>
<tr><th id="305">305</th><td></td></tr>
<tr><th id="306">306</th><td><u>#pragma unroll</u></td></tr>
<tr><th id="307">307</th><td>  <b>for</b> (<em>int</em> offset = warpSize/<var>2</var>; offset &gt; <var>0</var>; offset /= <var>2</var>) {</td></tr>
<tr><th id="308">308</th><td>  <u>#if defined(EIGEN_HIPCC)</u></td></tr>
<tr><th id="309">309</th><td>    PacketType r1;</td></tr>
<tr><th id="310">310</th><td>    half2* hr = <b>reinterpret_cast</b>&lt;half2*&gt;(&amp;r1);</td></tr>
<tr><th id="311">311</th><td>    half2* hacc = <b>reinterpret_cast</b>&lt;half2*&gt;(&amp;accum);</td></tr>
<tr><th id="312">312</th><td>    <b>for</b> (<em>int</em> i = <var>0</var>; i &lt; packet_width / <var>2</var>; i++) {</td></tr>
<tr><th id="313">313</th><td>      <i>// FIXME : remove this workaround once we have native half/half2 support for __shfl_down</i></td></tr>
<tr><th id="314">314</th><td>      <b>union</b> { <em>int</em> i; half2 h; } wka_in, wka_out;</td></tr>
<tr><th id="315">315</th><td>      wka_in.h = hacc[i];</td></tr>
<tr><th id="316">316</th><td>      wka_out.i = __shfl_down(wka_in.i, offset, warpSize);</td></tr>
<tr><th id="317">317</th><td>      hr[i] = wka_out.h;</td></tr>
<tr><th id="318">318</th><td>    }</td></tr>
<tr><th id="319">319</th><td>    reducer.reducePacket(r1, &amp;accum);</td></tr>
<tr><th id="320">320</th><td>  <u>#elif defined(EIGEN_CUDA_SDK_VER) &amp;&amp; EIGEN_CUDA_SDK_VER &lt; 90000</u></td></tr>
<tr><th id="321">321</th><td>    PacketType r1;</td></tr>
<tr><th id="322">322</th><td>    half2* hr = <b>reinterpret_cast</b>&lt;half2*&gt;(&amp;r1);</td></tr>
<tr><th id="323">323</th><td>    half2* hacc = <b>reinterpret_cast</b>&lt;half2*&gt;(&amp;accum);</td></tr>
<tr><th id="324">324</th><td>    <b>for</b> (<em>int</em> i = <var>0</var>; i &lt; packet_width / <var>2</var>; i++) {</td></tr>
<tr><th id="325">325</th><td>      hr[i] = __shfl_down(hacc[i], offset, warpSize);</td></tr>
<tr><th id="326">326</th><td>    }</td></tr>
<tr><th id="327">327</th><td>    reducer.reducePacket(r1, &amp;accum);</td></tr>
<tr><th id="328">328</th><td>  <u>#else</u></td></tr>
<tr><th id="329">329</th><td>    PacketType r1;</td></tr>
<tr><th id="330">330</th><td>    half2* hr = <b>reinterpret_cast</b>&lt;half2*&gt;(&amp;r1);</td></tr>
<tr><th id="331">331</th><td>    half2* hacc = <b>reinterpret_cast</b>&lt;half2*&gt;(&amp;accum);</td></tr>
<tr><th id="332">332</th><td>    <b>for</b> (<em>int</em> i = <var>0</var>; i &lt; packet_width / <var>2</var>; i++) {</td></tr>
<tr><th id="333">333</th><td>      hr[i] = __shfl_down_sync(<var>0xFFFFFFFF</var>, hacc[i], (<em>unsigned</em>)offset, warpSize);</td></tr>
<tr><th id="334">334</th><td>    }</td></tr>
<tr><th id="335">335</th><td>    reducer.reducePacket(r1, &amp;accum);</td></tr>
<tr><th id="336">336</th><td></td></tr>
<tr><th id="337">337</th><td>  <u>#endif</u></td></tr>
<tr><th id="338">338</th><td>  }</td></tr>
<tr><th id="339">339</th><td></td></tr>
<tr><th id="340">340</th><td>  <b>if</b> ((threadIdx.x &amp; (warpSize - <var>1</var>)) == <var>0</var>) {</td></tr>
<tr><th id="341">341</th><td>    atomicReduce(scratch, accum, reducer);</td></tr>
<tr><th id="342">342</th><td>  }</td></tr>
<tr><th id="343">343</th><td></td></tr>
<tr><th id="344">344</th><td>  __syncthreads();</td></tr>
<tr><th id="345">345</th><td>  half2* rv1 = <b>reinterpret_cast</b>&lt;half2*&gt;(scratch);</td></tr>
<tr><th id="346">346</th><td>  <b>if</b> (packet_width &gt; <var>2</var>) {</td></tr>
<tr><th id="347">347</th><td>    reducer.reducePacket(rv1[<var>2</var>], rv1);</td></tr>
<tr><th id="348">348</th><td>    reducer.reducePacket(rv1[<var>3</var>], rv1 + <var>1</var>);</td></tr>
<tr><th id="349">349</th><td>    reducer.reducePacket(rv1[<var>1</var>], rv1);</td></tr>
<tr><th id="350">350</th><td>  }</td></tr>
<tr><th id="351">351</th><td>  <b>if</b> (gridDim.x == <var>1</var>) {</td></tr>
<tr><th id="352">352</th><td>    <b>if</b> (first_index == <var>0</var>) {</td></tr>
<tr><th id="353">353</th><td>      half tmp = __low2half(*rv1);</td></tr>
<tr><th id="354">354</th><td>      reducer.reduce(__high2half(*rv1), &amp;tmp);</td></tr>
<tr><th id="355">355</th><td>      *output = tmp;</td></tr>
<tr><th id="356">356</th><td>    }</td></tr>
<tr><th id="357">357</th><td>  }</td></tr>
<tr><th id="358">358</th><td>}</td></tr>
<tr><th id="359">359</th><td></td></tr>
<tr><th id="360">360</th><td><b>template</b> &lt;<b>typename</b> Op&gt;</td></tr>
<tr><th id="361">361</th><td>__global__ __launch_bounds__(<var>1024</var>) <em>void</em> ReductionCleanupKernelHalfFloat(Op reducer, half* output, packet_traits&lt;Eigen::half&gt;::type* scratch) {</td></tr>
<tr><th id="362">362</th><td>  eigen_assert(threadIdx.x == <var>1</var>);</td></tr>
<tr><th id="363">363</th><td>  half2* pscratch = <b>reinterpret_cast</b>&lt;half2*&gt;(scratch);</td></tr>
<tr><th id="364">364</th><td>  half tmp = __float2half(<var>0.f</var>);</td></tr>
<tr><th id="365">365</th><td>  <b>typedef</b> packet_traits&lt;Eigen::half&gt;::type packet_type;</td></tr>
<tr><th id="366">366</th><td>  <b>for</b> (<em>int</em> i = <var>0</var>; i &lt; unpacket_traits&lt;packet_type&gt;::size; i += <var>2</var>) {</td></tr>
<tr><th id="367">367</th><td>    reducer.reduce(__low2half(*pscratch), &amp;tmp);</td></tr>
<tr><th id="368">368</th><td>    reducer.reduce(__high2half(*pscratch), &amp;tmp);</td></tr>
<tr><th id="369">369</th><td>    pscratch++;</td></tr>
<tr><th id="370">370</th><td>  }</td></tr>
<tr><th id="371">371</th><td>  *output = tmp;</td></tr>
<tr><th id="372">372</th><td>}</td></tr>
<tr><th id="373">373</th><td></td></tr>
<tr><th id="374">374</th><td><u>#endif // EIGEN_HAS_GPU_FP16</u></td></tr>
<tr><th id="375">375</th><td></td></tr>
<tr><th id="376">376</th><td><b>template</b> &lt;<b>typename</b> Self, <b>typename</b> Op, <b>typename</b> OutputType, <em>bool</em> PacketAccess, <b>typename</b> Enabled = <em>void</em>&gt;</td></tr>
<tr><th id="377">377</th><td><b>struct</b> FullReductionLauncher {</td></tr>
<tr><th id="378">378</th><td>  <em>static</em> <em>void</em> run(<em>const</em> Self&amp;, Op&amp;, <em>const</em> GpuDevice&amp;, OutputType*, <b>typename</b> Self::Index) {</td></tr>
<tr><th id="379">379</th><td>    gpu_assert(<b>false</b> &amp;&amp; <q>"Should only be called on doubles, floats and half floats"</q>);</td></tr>
<tr><th id="380">380</th><td>  }</td></tr>
<tr><th id="381">381</th><td>};</td></tr>
<tr><th id="382">382</th><td></td></tr>
<tr><th id="383">383</th><td><i>// Specialization for float and double</i></td></tr>
<tr><th id="384">384</th><td><b>template</b> &lt;<b>typename</b> Self, <b>typename</b> Op, <b>typename</b> OutputType, <em>bool</em> PacketAccess&gt;</td></tr>
<tr><th id="385">385</th><td><b>struct</b> FullReductionLauncher&lt;</td></tr>
<tr><th id="386">386</th><td>    Self, Op, OutputType, PacketAccess,</td></tr>
<tr><th id="387">387</th><td>    <b>typename</b> internal::enable_if&lt;</td></tr>
<tr><th id="388">388</th><td>      internal::is_same&lt;<em>float</em>, OutputType&gt;::value ||</td></tr>
<tr><th id="389">389</th><td>      internal::is_same&lt;<em>double</em>, OutputType&gt;::value,</td></tr>
<tr><th id="390">390</th><td>    <em>void</em>&gt;::type&gt; {</td></tr>
<tr><th id="391">391</th><td>  <em>static</em> <em>void</em> run(<em>const</em> Self&amp; self, Op&amp; reducer, <em>const</em> GpuDevice&amp; device, OutputType* output, <b>typename</b> Self::Index num_coeffs) {</td></tr>
<tr><th id="392">392</th><td></td></tr>
<tr><th id="393">393</th><td>    <b>typedef</b> <b>typename</b> Self::Index Index;</td></tr>
<tr><th id="394">394</th><td>    <em>const</em> <em>int</em> block_size = <var>256</var>;</td></tr>
<tr><th id="395">395</th><td>    <em>const</em> <em>int</em> num_per_thread = <var>128</var>;</td></tr>
<tr><th id="396">396</th><td>    <em>const</em> <em>int</em> num_blocks = divup&lt;<em>int</em>&gt;(num_coeffs, block_size * num_per_thread);</td></tr>
<tr><th id="397">397</th><td></td></tr>
<tr><th id="398">398</th><td>    <em>unsigned</em> <em>int</em>* semaphore = NULL;</td></tr>
<tr><th id="399">399</th><td>    <b>if</b> (num_blocks &gt; <var>1</var>) {</td></tr>
<tr><th id="400">400</th><td>      semaphore = device.semaphore();</td></tr>
<tr><th id="401">401</th><td>    }</td></tr>
<tr><th id="402">402</th><td></td></tr>
<tr><th id="403">403</th><td>    LAUNCH_GPU_KERNEL((FullReductionKernel&lt;block_size, num_per_thread, Self, Op, Index&gt;),</td></tr>
<tr><th id="404">404</th><td>                       num_blocks, block_size, <var>0</var>, device, reducer, self, num_coeffs, output, semaphore);</td></tr>
<tr><th id="405">405</th><td>  }</td></tr>
<tr><th id="406">406</th><td>};</td></tr>
<tr><th id="407">407</th><td></td></tr>
<tr><th id="408">408</th><td><u>#ifdef EIGEN_HAS_GPU_FP16</u></td></tr>
<tr><th id="409">409</th><td><b>template</b> &lt;<b>typename</b> Self, <b>typename</b> Op&gt;</td></tr>
<tr><th id="410">410</th><td><b>struct</b> FullReductionLauncher&lt;Self, Op, Eigen::half, <b>false</b>&gt; {</td></tr>
<tr><th id="411">411</th><td>  <em>static</em> <em>void</em> run(<em>const</em> Self&amp;, Op&amp;, <em>const</em> GpuDevice&amp;, half*, <b>typename</b> Self::Index) {</td></tr>
<tr><th id="412">412</th><td>    gpu_assert(<b>false</b> &amp;&amp; <q>"Should not be called since there is no packet accessor"</q>);</td></tr>
<tr><th id="413">413</th><td>  }</td></tr>
<tr><th id="414">414</th><td>};</td></tr>
<tr><th id="415">415</th><td></td></tr>
<tr><th id="416">416</th><td><b>template</b> &lt;<b>typename</b> Self, <b>typename</b> Op&gt;</td></tr>
<tr><th id="417">417</th><td><b>struct</b> FullReductionLauncher&lt;Self, Op, Eigen::half, <b>true</b>&gt; {</td></tr>
<tr><th id="418">418</th><td>  <em>static</em> <em>void</em> run(<em>const</em> Self&amp; self, Op&amp; reducer, <em>const</em> GpuDevice&amp; device, half* output, <b>typename</b> Self::Index num_coeffs) {</td></tr>
<tr><th id="419">419</th><td>    <b>typedef</b> <b>typename</b> Self::Index Index;</td></tr>
<tr><th id="420">420</th><td>    <b>typedef</b> <b>typename</b> packet_traits&lt;Eigen::half&gt;::type PacketType;</td></tr>
<tr><th id="421">421</th><td></td></tr>
<tr><th id="422">422</th><td>    <em>const</em> <em>int</em> block_size = <var>256</var>;</td></tr>
<tr><th id="423">423</th><td>    <em>const</em> <em>int</em> num_per_thread = <var>128</var>;</td></tr>
<tr><th id="424">424</th><td>    <em>const</em> <em>int</em> num_blocks = divup&lt;<em>int</em>&gt;(num_coeffs, block_size * num_per_thread);</td></tr>
<tr><th id="425">425</th><td>    PacketType* scratch = <b>static_cast</b>&lt;PacketType*&gt;(device.scratchpad());</td></tr>
<tr><th id="426">426</th><td>    <i>// half2* scratch = static_cast&lt;half2*&gt;(device.scratchpad());</i></td></tr>
<tr><th id="427">427</th><td></td></tr>
<tr><th id="428">428</th><td>    <b>if</b> (num_blocks &gt; <var>1</var>) {</td></tr>
<tr><th id="429">429</th><td>      <i>// We initialize the output and the scrathpad outside the reduction kernel when we can't be sure that there</i></td></tr>
<tr><th id="430">430</th><td><i>      // won't be a race conditions between multiple thread blocks.</i></td></tr>
<tr><th id="431">431</th><td>      LAUNCH_GPU_KERNEL((ReductionInitFullReduxKernelHalfFloat&lt;Self, Op, Index&gt;),</td></tr>
<tr><th id="432">432</th><td>                         <var>1</var>, <var>1</var>, <var>0</var>, device, reducer, self, num_coeffs, scratch);</td></tr>
<tr><th id="433">433</th><td>    }</td></tr>
<tr><th id="434">434</th><td></td></tr>
<tr><th id="435">435</th><td>    LAUNCH_GPU_KERNEL((FullReductionKernelHalfFloat&lt;block_size, num_per_thread, Self, Op, Index&gt;),</td></tr>
<tr><th id="436">436</th><td>                       num_blocks, block_size, <var>0</var>, device, reducer, self, num_coeffs, output, scratch);</td></tr>
<tr><th id="437">437</th><td></td></tr>
<tr><th id="438">438</th><td>    <b>if</b> (num_blocks &gt; <var>1</var>) {</td></tr>
<tr><th id="439">439</th><td>      LAUNCH_GPU_KERNEL((ReductionCleanupKernelHalfFloat&lt;Op&gt;),</td></tr>
<tr><th id="440">440</th><td>                         <var>1</var>, <var>1</var>, <var>0</var>, device, reducer, output, scratch);</td></tr>
<tr><th id="441">441</th><td>    }</td></tr>
<tr><th id="442">442</th><td>  }</td></tr>
<tr><th id="443">443</th><td>};</td></tr>
<tr><th id="444">444</th><td><u>#endif // EIGEN_HAS_GPU_FP16</u></td></tr>
<tr><th id="445">445</th><td></td></tr>
<tr><th id="446">446</th><td></td></tr>
<tr><th id="447">447</th><td><b>template</b> &lt;<b>typename</b> Self, <b>typename</b> Op, <em>bool</em> Vectorizable&gt;</td></tr>
<tr><th id="448">448</th><td><b>struct</b> FullReducer&lt;Self, Op, GpuDevice, Vectorizable&gt; {</td></tr>
<tr><th id="449">449</th><td>  <i>// Unfortunately nvidia doesn't support well exotic types such as complex,</i></td></tr>
<tr><th id="450">450</th><td><i>  // so reduce the scope of the optimized version of the code to the simple cases</i></td></tr>
<tr><th id="451">451</th><td><i>  // of doubles, floats and half floats</i></td></tr>
<tr><th id="452">452</th><td><u>#ifdef EIGEN_HAS_GPU_FP16</u></td></tr>
<tr><th id="453">453</th><td>  <em>static</em> <em>const</em> <em>bool</em> HasOptimizedImplementation = !Self::ReducerTraits::IsStateful &amp;&amp;</td></tr>
<tr><th id="454">454</th><td>      (internal::is_same&lt;<b>typename</b> Self::CoeffReturnType, <em>float</em>&gt;::value ||</td></tr>
<tr><th id="455">455</th><td>       internal::is_same&lt;<b>typename</b> Self::CoeffReturnType, <em>double</em>&gt;::value ||</td></tr>
<tr><th id="456">456</th><td>       (internal::is_same&lt;<b>typename</b> Self::CoeffReturnType, Eigen::half&gt;::value &amp;&amp; reducer_traits&lt;Op, GpuDevice&gt;::PacketAccess));</td></tr>
<tr><th id="457">457</th><td><u>#else // EIGEN_HAS_GPU_FP16</u></td></tr>
<tr><th id="458">458</th><td>  <em>static</em> <em>const</em> <em>bool</em> HasOptimizedImplementation = !Self::ReducerTraits::IsStateful &amp;&amp;</td></tr>
<tr><th id="459">459</th><td>                                                (internal::is_same&lt;<b>typename</b> Self::CoeffReturnType, <em>float</em>&gt;::value ||</td></tr>
<tr><th id="460">460</th><td>                                                 internal::is_same&lt;<b>typename</b> Self::CoeffReturnType, <em>double</em>&gt;::value);</td></tr>
<tr><th id="461">461</th><td><u>#endif // EIGEN_HAS_GPU_FP16</u></td></tr>
<tr><th id="462">462</th><td></td></tr>
<tr><th id="463">463</th><td>  <b>template</b> &lt;<b>typename</b> OutputType&gt;</td></tr>
<tr><th id="464">464</th><td>  <em>static</em> <em>void</em> run(<em>const</em> Self&amp; self, Op&amp; reducer, <em>const</em> GpuDevice&amp; device, OutputType* output) {</td></tr>
<tr><th id="465">465</th><td>    gpu_assert(HasOptimizedImplementation &amp;&amp; <q>"Should only be called on doubles, floats or half floats"</q>);</td></tr>
<tr><th id="466">466</th><td>    <em>const</em> Index num_coeffs = array_prod(self.m_impl.dimensions());</td></tr>
<tr><th id="467">467</th><td>    <i>// Don't crash when we're called with an input tensor of size 0.</i></td></tr>
<tr><th id="468">468</th><td>    <b>if</b> (num_coeffs == <var>0</var>) {</td></tr>
<tr><th id="469">469</th><td>      <b>return</b>;</td></tr>
<tr><th id="470">470</th><td>    }</td></tr>
<tr><th id="471">471</th><td></td></tr>
<tr><th id="472">472</th><td>    FullReductionLauncher&lt;Self, Op, OutputType, reducer_traits&lt;Op, GpuDevice&gt;::PacketAccess&gt;::run(self, reducer, device, output, num_coeffs);</td></tr>
<tr><th id="473">473</th><td>  }</td></tr>
<tr><th id="474">474</th><td>};</td></tr>
<tr><th id="475">475</th><td></td></tr>
<tr><th id="476">476</th><td></td></tr>
<tr><th id="477">477</th><td><b>template</b> &lt;<em>int</em> NumPerThread, <b>typename</b> Self,</td></tr>
<tr><th id="478">478</th><td>          <b>typename</b> Reducer, <b>typename</b> Index&gt;</td></tr>
<tr><th id="479">479</th><td>__global__ __launch_bounds__(<var>1024</var>) <em>void</em> InnerReductionKernel(Reducer reducer, <em>const</em> Self input, Index num_coeffs_to_reduce, Index num_preserved_coeffs,</td></tr>
<tr><th id="480">480</th><td>                                         <b>typename</b> Self::CoeffReturnType* output) {</td></tr>
<tr><th id="481">481</th><td><u>#if (defined(EIGEN_HIP_DEVICE_COMPILE) &amp;&amp; defined(__HIP_ARCH_HAS_WARP_SHUFFLE__)) || (EIGEN_CUDA_ARCH &gt;= 300)</u></td></tr>
<tr><th id="482">482</th><td>  <b>typedef</b> <b>typename</b> Self::CoeffReturnType Type;</td></tr>
<tr><th id="483">483</th><td>  eigen_assert(blockDim.y == <var>1</var>);</td></tr>
<tr><th id="484">484</th><td>  eigen_assert(blockDim.z == <var>1</var>);</td></tr>
<tr><th id="485">485</th><td>  eigen_assert(gridDim.y == <var>1</var>);</td></tr>
<tr><th id="486">486</th><td>  eigen_assert(gridDim.z == <var>1</var>);</td></tr>
<tr><th id="487">487</th><td></td></tr>
<tr><th id="488">488</th><td>  <em>const</em> <em>int</em> unroll_times = <var>16</var>;</td></tr>
<tr><th id="489">489</th><td>  eigen_assert(NumPerThread % unroll_times == <var>0</var>);</td></tr>
<tr><th id="490">490</th><td></td></tr>
<tr><th id="491">491</th><td>  <em>const</em> Index input_col_blocks = divup&lt;Index&gt;(num_coeffs_to_reduce, blockDim.x * NumPerThread);</td></tr>
<tr><th id="492">492</th><td>  <em>const</em> Index num_input_blocks = input_col_blocks * num_preserved_coeffs;</td></tr>
<tr><th id="493">493</th><td></td></tr>
<tr><th id="494">494</th><td>  <em>const</em> Index num_threads = blockDim.x * gridDim.x;</td></tr>
<tr><th id="495">495</th><td>  <em>const</em> Index thread_id = blockIdx.x * blockDim.x + threadIdx.x;</td></tr>
<tr><th id="496">496</th><td></td></tr>
<tr><th id="497">497</th><td>  <i>// Initialize the output values if they weren't initialized by the ReductionInitKernel</i></td></tr>
<tr><th id="498">498</th><td>  <b>if</b> (gridDim.x == <var>1</var>) {</td></tr>
<tr><th id="499">499</th><td>    <b>for</b> (Index i = thread_id; i &lt; num_preserved_coeffs; i += num_threads) {</td></tr>
<tr><th id="500">500</th><td>      output[i] = reducer.initialize();</td></tr>
<tr><th id="501">501</th><td>    }</td></tr>
<tr><th id="502">502</th><td>    __syncthreads();</td></tr>
<tr><th id="503">503</th><td>  }</td></tr>
<tr><th id="504">504</th><td></td></tr>
<tr><th id="505">505</th><td>  <b>for</b> (Index i = blockIdx.x; i &lt; num_input_blocks; i += gridDim.x) {</td></tr>
<tr><th id="506">506</th><td>    <em>const</em> Index row = i / input_col_blocks;</td></tr>
<tr><th id="507">507</th><td></td></tr>
<tr><th id="508">508</th><td>    <b>if</b> (row &lt; num_preserved_coeffs) {</td></tr>
<tr><th id="509">509</th><td>      <em>const</em> Index col_block = i % input_col_blocks;</td></tr>
<tr><th id="510">510</th><td>      <em>const</em> Index col_begin = col_block * blockDim.x * NumPerThread + threadIdx.x;</td></tr>
<tr><th id="511">511</th><td></td></tr>
<tr><th id="512">512</th><td>      Type reduced_val = reducer.initialize();</td></tr>
<tr><th id="513">513</th><td></td></tr>
<tr><th id="514">514</th><td>      <b>for</b> (Index j = <var>0</var>; j &lt; NumPerThread; j += unroll_times) {</td></tr>
<tr><th id="515">515</th><td>        <em>const</em> Index last_col = col_begin + blockDim.x * (j + unroll_times - <var>1</var>);</td></tr>
<tr><th id="516">516</th><td>        <b>if</b> (last_col &gt;= num_coeffs_to_reduce) {</td></tr>
<tr><th id="517">517</th><td>          <b>for</b> (Index col = col_begin + blockDim.x * j; col &lt; num_coeffs_to_reduce; col += blockDim.x) {</td></tr>
<tr><th id="518">518</th><td>            <em>const</em> Type val = input.m_impl.coeff(row * num_coeffs_to_reduce + col);</td></tr>
<tr><th id="519">519</th><td>            reducer.reduce(val, &amp;reduced_val);</td></tr>
<tr><th id="520">520</th><td>          }</td></tr>
<tr><th id="521">521</th><td>          <b>break</b>;</td></tr>
<tr><th id="522">522</th><td>        } <b>else</b> {</td></tr>
<tr><th id="523">523</th><td>          <i>// Faster version of the loop with no branches after unrolling.</i></td></tr>
<tr><th id="524">524</th><td><u>#pragma unroll</u></td></tr>
<tr><th id="525">525</th><td>          <b>for</b> (<em>int</em> k = <var>0</var>; k &lt; unroll_times; ++k) {</td></tr>
<tr><th id="526">526</th><td>            <em>const</em> Index col = col_begin + blockDim.x * (j + k);</td></tr>
<tr><th id="527">527</th><td>            reducer.reduce(input.m_impl.coeff(row * num_coeffs_to_reduce + col), &amp;reduced_val);</td></tr>
<tr><th id="528">528</th><td>          }</td></tr>
<tr><th id="529">529</th><td>        }</td></tr>
<tr><th id="530">530</th><td>      }</td></tr>
<tr><th id="531">531</th><td></td></tr>
<tr><th id="532">532</th><td><u>#pragma unroll</u></td></tr>
<tr><th id="533">533</th><td>      <b>for</b> (<em>int</em> offset = warpSize/<var>2</var>; offset &gt; <var>0</var>; offset /= <var>2</var>) {</td></tr>
<tr><th id="534">534</th><td>      <u>#if defined(EIGEN_HIPCC)</u></td></tr>
<tr><th id="535">535</th><td>        <i>// use std::is_floating_point to determine the type of reduced_val </i></td></tr>
<tr><th id="536">536</th><td><i>       // This is needed because when Type == double, hipcc will give a "call to __shfl_down is ambguous" error </i></td></tr>
<tr><th id="537">537</th><td><i>       // and list the float and int versions of __shfl_down as the candidate functions. </i></td></tr>
<tr><th id="538">538</th><td>        <b>if</b> (std::is_floating_point&lt;Type&gt;::value) {</td></tr>
<tr><th id="539">539</th><td>          reducer.reduce(__shfl_down(<b>static_cast</b>&lt;<em>float</em>&gt;(reduced_val), offset), &amp;reduced_val);</td></tr>
<tr><th id="540">540</th><td>        } <b>else</b> {</td></tr>
<tr><th id="541">541</th><td>          reducer.reduce(__shfl_down(<b>static_cast</b>&lt;<em>int</em>&gt;(reduced_val), offset), &amp;reduced_val);</td></tr>
<tr><th id="542">542</th><td>        }</td></tr>
<tr><th id="543">543</th><td>      <u>#elif defined(EIGEN_CUDA_SDK_VER) &amp;&amp; EIGEN_CUDA_SDK_VER &lt; 90000</u></td></tr>
<tr><th id="544">544</th><td>        reducer.reduce(__shfl_down(reduced_val, offset), &amp;reduced_val);</td></tr>
<tr><th id="545">545</th><td>      <u>#else</u></td></tr>
<tr><th id="546">546</th><td>        reducer.reduce(__shfl_down_sync(<var>0xFFFFFFFF</var>, reduced_val, offset), &amp;reduced_val);</td></tr>
<tr><th id="547">547</th><td>      <u>#endif</u></td></tr>
<tr><th id="548">548</th><td>      }</td></tr>
<tr><th id="549">549</th><td></td></tr>
<tr><th id="550">550</th><td>      <b>if</b> ((threadIdx.x &amp; (warpSize - <var>1</var>)) == <var>0</var>) {</td></tr>
<tr><th id="551">551</th><td>        atomicReduce(&amp;(output[row]), reduced_val, reducer);</td></tr>
<tr><th id="552">552</th><td>      }</td></tr>
<tr><th id="553">553</th><td>    }</td></tr>
<tr><th id="554">554</th><td>  }</td></tr>
<tr><th id="555">555</th><td><u>#else // EIGEN_CUDA_ARCH &gt;= 300</u></td></tr>
<tr><th id="556">556</th><td>  gpu_assert(<var>0</var> &amp;&amp; <q>"Shouldn't be called on unsupported device"</q>);</td></tr>
<tr><th id="557">557</th><td><u>#endif // EIGEN_CUDA_ARCH &gt;= 300</u></td></tr>
<tr><th id="558">558</th><td>}</td></tr>
<tr><th id="559">559</th><td></td></tr>
<tr><th id="560">560</th><td><u>#ifdef EIGEN_HAS_GPU_FP16</u></td></tr>
<tr><th id="561">561</th><td></td></tr>
<tr><th id="562">562</th><td><b>template</b> &lt;<em>int</em> NumPerThread, <b>typename</b> Self,</td></tr>
<tr><th id="563">563</th><td>          <b>typename</b> Reducer, <b>typename</b> Index&gt;</td></tr>
<tr><th id="564">564</th><td>__global__ __launch_bounds__(<var>1024</var>) <em>void</em> InnerReductionKernelHalfFloat(Reducer reducer, <em>const</em> Self input, Index num_coeffs_to_reduce, Index num_preserved_coeffs,</td></tr>
<tr><th id="565">565</th><td>                                              half* output) {</td></tr>
<tr><th id="566">566</th><td>  eigen_assert(blockDim.y == <var>1</var>);</td></tr>
<tr><th id="567">567</th><td>  eigen_assert(blockDim.z == <var>1</var>);</td></tr>
<tr><th id="568">568</th><td>  eigen_assert(gridDim.y == <var>1</var>);</td></tr>
<tr><th id="569">569</th><td>  eigen_assert(gridDim.z == <var>1</var>);</td></tr>
<tr><th id="570">570</th><td></td></tr>
<tr><th id="571">571</th><td>  <b>typedef</b> <b>typename</b> packet_traits&lt;Eigen::half&gt;::type PacketType;</td></tr>
<tr><th id="572">572</th><td>  <em>const</em> <em>int</em> packet_width = unpacket_traits&lt;PacketType&gt;::size;</td></tr>
<tr><th id="573">573</th><td>  <em>const</em> <em>int</em> unroll_times = <var>16</var> / packet_width;</td></tr>
<tr><th id="574">574</th><td>  eigen_assert(NumPerThread % unroll_times == <var>0</var>);</td></tr>
<tr><th id="575">575</th><td>  eigen_assert(unroll_times % <var>2</var> == <var>0</var>);</td></tr>
<tr><th id="576">576</th><td></td></tr>
<tr><th id="577">577</th><td>  <em>const</em> Index input_col_blocks = divup&lt;Index&gt;(num_coeffs_to_reduce, blockDim.x * NumPerThread * <var>2</var>);</td></tr>
<tr><th id="578">578</th><td>  <em>const</em> Index num_input_blocks = divup&lt;Index&gt;(input_col_blocks * num_preserved_coeffs, <var>2</var>);</td></tr>
<tr><th id="579">579</th><td></td></tr>
<tr><th id="580">580</th><td>  <em>const</em> Index num_threads = blockDim.x * gridDim.x;</td></tr>
<tr><th id="581">581</th><td>  <em>const</em> Index thread_id = blockIdx.x * blockDim.x + threadIdx.x;</td></tr>
<tr><th id="582">582</th><td></td></tr>
<tr><th id="583">583</th><td>  <i>// Initialize the output values if they weren't initialized by the ReductionInitKernel</i></td></tr>
<tr><th id="584">584</th><td>  <b>if</b> (gridDim.x == <var>1</var>) {</td></tr>
<tr><th id="585">585</th><td>    Index i = packet_width * thread_id;</td></tr>
<tr><th id="586">586</th><td>    <b>for</b> (; i + packet_width &lt;= num_preserved_coeffs;</td></tr>
<tr><th id="587">587</th><td>         i += packet_width * num_threads) {</td></tr>
<tr><th id="588">588</th><td>      PacketType* poutput = <b>reinterpret_cast</b>&lt;PacketType*&gt;(output + i);</td></tr>
<tr><th id="589">589</th><td>      *poutput = reducer.<b>template</b> initializePacket&lt;PacketType&gt;();</td></tr>
<tr><th id="590">590</th><td>    }</td></tr>
<tr><th id="591">591</th><td>    <b>if</b> (i &lt; num_preserved_coeffs) {</td></tr>
<tr><th id="592">592</th><td>      output[i] = reducer.initialize();</td></tr>
<tr><th id="593">593</th><td>    }</td></tr>
<tr><th id="594">594</th><td>    __syncthreads();</td></tr>
<tr><th id="595">595</th><td>  }</td></tr>
<tr><th id="596">596</th><td></td></tr>
<tr><th id="597">597</th><td>  <b>for</b> (Index i = blockIdx.x; i &lt; num_input_blocks; i += gridDim.x) {</td></tr>
<tr><th id="598">598</th><td>    <em>const</em> Index row = <var>2</var> * (i / input_col_blocks);  <i>// everybody takes 2 rows</i></td></tr>
<tr><th id="599">599</th><td></td></tr>
<tr><th id="600">600</th><td>    <b>if</b> (row + <var>1</var> &lt; num_preserved_coeffs) {</td></tr>
<tr><th id="601">601</th><td>      <em>const</em> Index col_block = i % input_col_blocks;</td></tr>
<tr><th id="602">602</th><td>      <em>const</em> Index col_begin =</td></tr>
<tr><th id="603">603</th><td>          packet_width * (col_block * blockDim.x * NumPerThread + threadIdx.x);</td></tr>
<tr><th id="604">604</th><td></td></tr>
<tr><th id="605">605</th><td>      PacketType reduced_val1 = reducer.<b>template</b> initializePacket&lt;PacketType&gt;();</td></tr>
<tr><th id="606">606</th><td>      PacketType reduced_val2 = reducer.<b>template</b> initializePacket&lt;PacketType&gt;();</td></tr>
<tr><th id="607">607</th><td></td></tr>
<tr><th id="608">608</th><td>      <b>for</b> (Index j = <var>0</var>; j &lt; NumPerThread; j += unroll_times) {</td></tr>
<tr><th id="609">609</th><td>        <em>const</em> Index last_col =</td></tr>
<tr><th id="610">610</th><td>            col_begin + blockDim.x * (j + unroll_times - <var>1</var>) * packet_width;</td></tr>
<tr><th id="611">611</th><td>        <b>if</b> (last_col &gt;= num_coeffs_to_reduce) {</td></tr>
<tr><th id="612">612</th><td>          Index col = col_begin + blockDim.x * j;</td></tr>
<tr><th id="613">613</th><td>          <b>for</b> (; col + packet_width &lt;= num_coeffs_to_reduce;</td></tr>
<tr><th id="614">614</th><td>               col += blockDim.x) {</td></tr>
<tr><th id="615">615</th><td>            <em>const</em> PacketType val1 = input.m_impl.<b>template</b> packet&lt;Unaligned&gt;(</td></tr>
<tr><th id="616">616</th><td>                row * num_coeffs_to_reduce + col);</td></tr>
<tr><th id="617">617</th><td>            reducer.reducePacket(val1, &amp;reduced_val1);</td></tr>
<tr><th id="618">618</th><td>            <em>const</em> PacketType val2 = input.m_impl.<b>template</b> packet&lt;Unaligned&gt;(</td></tr>
<tr><th id="619">619</th><td>                (row + <var>1</var>) * num_coeffs_to_reduce + col);</td></tr>
<tr><th id="620">620</th><td>            reducer.reducePacket(val2, &amp;reduced_val2);</td></tr>
<tr><th id="621">621</th><td>          }</td></tr>
<tr><th id="622">622</th><td>          <b>if</b> (col &lt; num_coeffs_to_reduce) {</td></tr>
<tr><th id="623">623</th><td>            PacketType r1 = reducer.<b>template</b> initializePacket&lt;PacketType&gt;();</td></tr>
<tr><th id="624">624</th><td>            PacketType r2 = reducer.<b>template</b> initializePacket&lt;PacketType&gt;();</td></tr>
<tr><th id="625">625</th><td>            half2* hr1 = <b>reinterpret_cast</b>&lt;half2*&gt;(&amp;r1);</td></tr>
<tr><th id="626">626</th><td>            half2* hr2 = <b>reinterpret_cast</b>&lt;half2*&gt;(&amp;r2);</td></tr>
<tr><th id="627">627</th><td>            <b>while</b> (col + <var>1</var> &lt; num_coeffs_to_reduce) {</td></tr>
<tr><th id="628">628</th><td>              *hr1 = __halves2half2(</td></tr>
<tr><th id="629">629</th><td>                  input.m_impl.coeff(row * num_coeffs_to_reduce + col),</td></tr>
<tr><th id="630">630</th><td>                  input.m_impl.coeff(row * num_coeffs_to_reduce + col + <var>1</var>));</td></tr>
<tr><th id="631">631</th><td>              *hr2 = __halves2half2(</td></tr>
<tr><th id="632">632</th><td>                  input.m_impl.coeff((row + <var>1</var>) * num_coeffs_to_reduce + col),</td></tr>
<tr><th id="633">633</th><td>                  input.m_impl.coeff((row + <var>1</var>) * num_coeffs_to_reduce + col +</td></tr>
<tr><th id="634">634</th><td>                                     <var>1</var>));</td></tr>
<tr><th id="635">635</th><td>              hr1++;</td></tr>
<tr><th id="636">636</th><td>              hr2++;</td></tr>
<tr><th id="637">637</th><td>              col += <var>2</var>;</td></tr>
<tr><th id="638">638</th><td>            }</td></tr>
<tr><th id="639">639</th><td>            <b>if</b> (col &lt; num_coeffs_to_reduce) {</td></tr>
<tr><th id="640">640</th><td>              <i>// Peel;</i></td></tr>
<tr><th id="641">641</th><td>              <em>const</em> half last1 =</td></tr>
<tr><th id="642">642</th><td>                  input.m_impl.coeff(row * num_coeffs_to_reduce + col);</td></tr>
<tr><th id="643">643</th><td>              *hr1 = __halves2half2(last1, reducer.initialize());</td></tr>
<tr><th id="644">644</th><td>              <em>const</em> half last2 =</td></tr>
<tr><th id="645">645</th><td>                  input.m_impl.coeff((row + <var>1</var>) * num_coeffs_to_reduce + col);</td></tr>
<tr><th id="646">646</th><td>              *hr2 = __halves2half2(last2, reducer.initialize());</td></tr>
<tr><th id="647">647</th><td>            }</td></tr>
<tr><th id="648">648</th><td>            reducer.reducePacket(r1, &amp;reduced_val1);</td></tr>
<tr><th id="649">649</th><td>            reducer.reducePacket(r2, &amp;reduced_val2);</td></tr>
<tr><th id="650">650</th><td>          }</td></tr>
<tr><th id="651">651</th><td>          <b>break</b>;</td></tr>
<tr><th id="652">652</th><td>        } <b>else</b> {</td></tr>
<tr><th id="653">653</th><td>          <i>// Faster version of the loop with no branches after unrolling.</i></td></tr>
<tr><th id="654">654</th><td><u>#pragma unroll</u></td></tr>
<tr><th id="655">655</th><td>          <b>for</b> (<em>int</em> k = <var>0</var>; k &lt; unroll_times; ++k) {</td></tr>
<tr><th id="656">656</th><td>            <em>const</em> Index col = col_begin + blockDim.x * (j + k) * packet_width;</td></tr>
<tr><th id="657">657</th><td>            reducer.reducePacket(input.m_impl.<b>template</b> packet&lt;Unaligned&gt;(</td></tr>
<tr><th id="658">658</th><td>                                     row * num_coeffs_to_reduce + col),</td></tr>
<tr><th id="659">659</th><td>                                 &amp;reduced_val1);</td></tr>
<tr><th id="660">660</th><td>            reducer.reducePacket(input.m_impl.<b>template</b> packet&lt;Unaligned&gt;(</td></tr>
<tr><th id="661">661</th><td>                                     (row + <var>1</var>) * num_coeffs_to_reduce + col),</td></tr>
<tr><th id="662">662</th><td>                                 &amp;reduced_val2);</td></tr>
<tr><th id="663">663</th><td>          }</td></tr>
<tr><th id="664">664</th><td>        }</td></tr>
<tr><th id="665">665</th><td>      }</td></tr>
<tr><th id="666">666</th><td></td></tr>
<tr><th id="667">667</th><td><u>#pragma unroll</u></td></tr>
<tr><th id="668">668</th><td>      <b>for</b> (<em>int</em> offset = warpSize/<var>2</var>; offset &gt; <var>0</var>; offset /= <var>2</var>) {</td></tr>
<tr><th id="669">669</th><td>      <u>#if defined(EIGEN_HIPCC)</u></td></tr>
<tr><th id="670">670</th><td>        PacketType r1;</td></tr>
<tr><th id="671">671</th><td>        PacketType r2;</td></tr>
<tr><th id="672">672</th><td>        half2* hr1 = <b>reinterpret_cast</b>&lt;half2*&gt;(&amp;r1);</td></tr>
<tr><th id="673">673</th><td>        half2* hr2 = <b>reinterpret_cast</b>&lt;half2*&gt;(&amp;r2);</td></tr>
<tr><th id="674">674</th><td>        half2* rv1 = <b>reinterpret_cast</b>&lt;half2*&gt;(&amp;reduced_val1);</td></tr>
<tr><th id="675">675</th><td>        half2* rv2 = <b>reinterpret_cast</b>&lt;half2*&gt;(&amp;reduced_val2);</td></tr>
<tr><th id="676">676</th><td>        <b>for</b> (<em>int</em> i = <var>0</var>; i &lt; packet_width / <var>2</var>; i++) {</td></tr>
<tr><th id="677">677</th><td>	  <i>// FIXME : remove this workaround once we have native half/half2 support for __shfl_down</i></td></tr>
<tr><th id="678">678</th><td>	  <b>union</b> { <em>int</em> i; half2 h; } wka_in1, wka_out1;</td></tr>
<tr><th id="679">679</th><td>	  wka_in1.h = rv1[i];</td></tr>
<tr><th id="680">680</th><td>	  wka_out1.i = __shfl_down(wka_in1.i, offset, warpSize);</td></tr>
<tr><th id="681">681</th><td>	  hr1[i] = wka_out1.h;</td></tr>
<tr><th id="682">682</th><td></td></tr>
<tr><th id="683">683</th><td>	  <b>union</b> { <em>int</em> i; half2 h; } wka_in2, wka_out2;</td></tr>
<tr><th id="684">684</th><td>	  wka_in2.h = rv2[i];</td></tr>
<tr><th id="685">685</th><td>	  wka_out2.i = __shfl_down(wka_in2.i, offset, warpSize);</td></tr>
<tr><th id="686">686</th><td>	  hr2[i] = wka_out2.h;</td></tr>
<tr><th id="687">687</th><td>        }</td></tr>
<tr><th id="688">688</th><td>        reducer.reducePacket(r1, &amp;reduced_val1);</td></tr>
<tr><th id="689">689</th><td>        reducer.reducePacket(r2, &amp;reduced_val2);</td></tr>
<tr><th id="690">690</th><td>      <u>#elif defined(EIGEN_CUDA_SDK_VER) &amp;&amp; EIGEN_CUDA_SDK_VER &lt; 90000</u></td></tr>
<tr><th id="691">691</th><td>        PacketType r1;</td></tr>
<tr><th id="692">692</th><td>        PacketType r2;</td></tr>
<tr><th id="693">693</th><td>        half2* hr1 = <b>reinterpret_cast</b>&lt;half2*&gt;(&amp;r1);</td></tr>
<tr><th id="694">694</th><td>        half2* hr2 = <b>reinterpret_cast</b>&lt;half2*&gt;(&amp;r2);</td></tr>
<tr><th id="695">695</th><td>        half2* rv1 = <b>reinterpret_cast</b>&lt;half2*&gt;(&amp;reduced_val1);</td></tr>
<tr><th id="696">696</th><td>        half2* rv2 = <b>reinterpret_cast</b>&lt;half2*&gt;(&amp;reduced_val2);</td></tr>
<tr><th id="697">697</th><td>        <b>for</b> (<em>int</em> i = <var>0</var>; i &lt; packet_width / <var>2</var>; i++) {</td></tr>
<tr><th id="698">698</th><td>          hr1[i] = __shfl_down(rv1[i], offset, warpSize);</td></tr>
<tr><th id="699">699</th><td>          hr2[i] = __shfl_down(rv2[i], offset, warpSize);</td></tr>
<tr><th id="700">700</th><td>        }</td></tr>
<tr><th id="701">701</th><td>        reducer.reducePacket(r1, &amp;reduced_val1);</td></tr>
<tr><th id="702">702</th><td>        reducer.reducePacket(r2, &amp;reduced_val2);</td></tr>
<tr><th id="703">703</th><td>      <u>#else</u></td></tr>
<tr><th id="704">704</th><td>        PacketType r1;</td></tr>
<tr><th id="705">705</th><td>        PacketType r2;</td></tr>
<tr><th id="706">706</th><td>        half2* hr1 = <b>reinterpret_cast</b>&lt;half2*&gt;(&amp;r1);</td></tr>
<tr><th id="707">707</th><td>        half2* hr2 = <b>reinterpret_cast</b>&lt;half2*&gt;(&amp;r2);</td></tr>
<tr><th id="708">708</th><td>        half2* rr1 = <b>reinterpret_cast</b>&lt;half2*&gt;(&amp;reduced_val1);</td></tr>
<tr><th id="709">709</th><td>        half2* rr2 = <b>reinterpret_cast</b>&lt;half2*&gt;(&amp;reduced_val2);</td></tr>
<tr><th id="710">710</th><td>        <b>for</b> (<em>int</em> i = <var>0</var>; i &lt; packet_width / <var>2</var>; i++) {</td></tr>
<tr><th id="711">711</th><td>          hr1[i] =</td></tr>
<tr><th id="712">712</th><td>              __shfl_down_sync(<var>0xFFFFFFFF</var>, rr1[i], (<em>unsigned</em>)offset, warpSize);</td></tr>
<tr><th id="713">713</th><td>          hr2[i] =</td></tr>
<tr><th id="714">714</th><td>              __shfl_down_sync(<var>0xFFFFFFFF</var>, rr2[i], (<em>unsigned</em>)offset, warpSize);</td></tr>
<tr><th id="715">715</th><td>        }</td></tr>
<tr><th id="716">716</th><td>        reducer.reducePacket(r1, &amp;reduced_val1);</td></tr>
<tr><th id="717">717</th><td>        reducer.reducePacket(r2, &amp;reduced_val2);</td></tr>
<tr><th id="718">718</th><td></td></tr>
<tr><th id="719">719</th><td>      <u>#endif</u></td></tr>
<tr><th id="720">720</th><td>      }</td></tr>
<tr><th id="721">721</th><td>      half2* rv1 = <b>reinterpret_cast</b>&lt;half2*&gt;(&amp;reduced_val1);</td></tr>
<tr><th id="722">722</th><td>      half2* rv2 = <b>reinterpret_cast</b>&lt;half2*&gt;(&amp;reduced_val2);</td></tr>
<tr><th id="723">723</th><td>      half2 val;</td></tr>
<tr><th id="724">724</th><td>      <b>if</b> (packet_width &gt; <var>2</var>) {</td></tr>
<tr><th id="725">725</th><td>        reducer.reducePacket(rv1[<var>2</var>], rv1);</td></tr>
<tr><th id="726">726</th><td>        reducer.reducePacket(rv1[<var>3</var>], rv1 + <var>1</var>);</td></tr>
<tr><th id="727">727</th><td>        reducer.reducePacket(rv1[<var>1</var>], rv1);</td></tr>
<tr><th id="728">728</th><td>        reducer.reducePacket(rv2[<var>2</var>], rv2);</td></tr>
<tr><th id="729">729</th><td>        reducer.reducePacket(rv2[<var>3</var>], rv2 + <var>1</var>);</td></tr>
<tr><th id="730">730</th><td>        reducer.reducePacket(rv2[<var>1</var>], rv2);</td></tr>
<tr><th id="731">731</th><td>      }</td></tr>
<tr><th id="732">732</th><td>      half val1 = __low2half(*rv1);</td></tr>
<tr><th id="733">733</th><td>      reducer.reduce(__high2half(*rv1), &amp;val1);</td></tr>
<tr><th id="734">734</th><td>      half val2 = __low2half(*rv2);</td></tr>
<tr><th id="735">735</th><td>      reducer.reduce(__high2half(*rv2), &amp;val2);</td></tr>
<tr><th id="736">736</th><td>      val = __halves2half2(val1, val2);</td></tr>
<tr><th id="737">737</th><td>      <b>if</b> ((threadIdx.x &amp; (warpSize - <var>1</var>)) == <var>0</var>) {</td></tr>
<tr><th id="738">738</th><td>        half* loc = output + row;</td></tr>
<tr><th id="739">739</th><td>        atomicReduce((half2*)loc, val, reducer);</td></tr>
<tr><th id="740">740</th><td>      }</td></tr>
<tr><th id="741">741</th><td>    }</td></tr>
<tr><th id="742">742</th><td>  }</td></tr>
<tr><th id="743">743</th><td>}</td></tr>
<tr><th id="744">744</th><td></td></tr>
<tr><th id="745">745</th><td><u>#endif // EIGEN_HAS_GPU_FP16</u></td></tr>
<tr><th id="746">746</th><td></td></tr>
<tr><th id="747">747</th><td><b>template</b> &lt;<b>typename</b> Self, <b>typename</b> Op, <b>typename</b> OutputType, <em>bool</em> PacketAccess, <b>typename</b> Enabled = <em>void</em>&gt;</td></tr>
<tr><th id="748">748</th><td><b>struct</b> InnerReductionLauncher {</td></tr>
<tr><th id="749">749</th><td>  <em>static</em> EIGEN_DEVICE_FUNC <em>bool</em> run(<em>const</em> Self&amp;, Op&amp;, <em>const</em> GpuDevice&amp;, OutputType*, <b>typename</b> Self::Index, <b>typename</b> Self::Index) {</td></tr>
<tr><th id="750">750</th><td>    gpu_assert(<b>false</b> &amp;&amp; <q>"Should only be called to reduce doubles, floats and half floats on a gpu device"</q>);</td></tr>
<tr><th id="751">751</th><td>    <b>return</b> <b>true</b>;</td></tr>
<tr><th id="752">752</th><td>  }</td></tr>
<tr><th id="753">753</th><td>};</td></tr>
<tr><th id="754">754</th><td></td></tr>
<tr><th id="755">755</th><td><i>// Specialization for float and double</i></td></tr>
<tr><th id="756">756</th><td><b>template</b> &lt;<b>typename</b> Self, <b>typename</b> Op, <b>typename</b> OutputType, <em>bool</em> PacketAccess&gt;</td></tr>
<tr><th id="757">757</th><td><b>struct</b> InnerReductionLauncher&lt;</td></tr>
<tr><th id="758">758</th><td>  Self, Op, OutputType, PacketAccess,</td></tr>
<tr><th id="759">759</th><td>  <b>typename</b> internal::enable_if&lt;</td></tr>
<tr><th id="760">760</th><td>    internal::is_same&lt;<em>float</em>, OutputType&gt;::value ||</td></tr>
<tr><th id="761">761</th><td>    internal::is_same&lt;<em>double</em>, OutputType&gt;::value,</td></tr>
<tr><th id="762">762</th><td>  <em>void</em>&gt;::type&gt; {</td></tr>
<tr><th id="763">763</th><td>  <em>static</em> <em>bool</em> run(<em>const</em> Self&amp; self, Op&amp; reducer, <em>const</em> GpuDevice&amp; device, OutputType* output, <b>typename</b> Self::Index num_coeffs_to_reduce, <b>typename</b> Self::Index num_preserved_vals) {</td></tr>
<tr><th id="764">764</th><td>    <b>typedef</b> <b>typename</b> Self::Index Index;</td></tr>
<tr><th id="765">765</th><td></td></tr>
<tr><th id="766">766</th><td>    <em>const</em> Index num_coeffs = num_coeffs_to_reduce * num_preserved_vals;</td></tr>
<tr><th id="767">767</th><td>    <em>const</em> <em>int</em> block_size = <var>256</var>;</td></tr>
<tr><th id="768">768</th><td>    <em>const</em> <em>int</em> num_per_thread = <var>128</var>;</td></tr>
<tr><th id="769">769</th><td>    <em>const</em> <em>int</em> dyn_blocks = divup&lt;<em>int</em>&gt;(num_coeffs, block_size * num_per_thread);</td></tr>
<tr><th id="770">770</th><td>    <em>const</em> <em>int</em> max_blocks = device.getNumGpuMultiProcessors() *</td></tr>
<tr><th id="771">771</th><td>                           device.maxGpuThreadsPerMultiProcessor() / block_size;</td></tr>
<tr><th id="772">772</th><td>    <em>const</em> <em>int</em> num_blocks = numext::mini&lt;<em>int</em>&gt;(max_blocks, dyn_blocks);</td></tr>
<tr><th id="773">773</th><td></td></tr>
<tr><th id="774">774</th><td>    <b>if</b> (num_blocks &gt; <var>1</var>) {</td></tr>
<tr><th id="775">775</th><td>      <i>// We initialize the outputs outside the reduction kernel when we can't be sure that there</i></td></tr>
<tr><th id="776">776</th><td><i>      // won't be a race conditions between multiple thread blocks.</i></td></tr>
<tr><th id="777">777</th><td>      <em>const</em> <em>int</em> dyn_blocks = divup&lt;<em>int</em>&gt;(num_preserved_vals, <var>1024</var>);</td></tr>
<tr><th id="778">778</th><td>      <em>const</em> <em>int</em> max_blocks = device.getNumGpuMultiProcessors() *</td></tr>
<tr><th id="779">779</th><td>                           device.maxGpuThreadsPerMultiProcessor() / <var>1024</var>;</td></tr>
<tr><th id="780">780</th><td>      <em>const</em> <em>int</em> num_blocks = numext::mini&lt;<em>int</em>&gt;(max_blocks, dyn_blocks);</td></tr>
<tr><th id="781">781</th><td>      LAUNCH_GPU_KERNEL((ReductionInitKernel&lt;OutputType, Index&gt;),</td></tr>
<tr><th id="782">782</th><td>                         num_blocks, <var>1024</var>, <var>0</var>, device, reducer.initialize(),</td></tr>
<tr><th id="783">783</th><td>                         num_preserved_vals, output);</td></tr>
<tr><th id="784">784</th><td>    }</td></tr>
<tr><th id="785">785</th><td></td></tr>
<tr><th id="786">786</th><td>    LAUNCH_GPU_KERNEL((InnerReductionKernel&lt;num_per_thread, Self, Op, Index&gt;),</td></tr>
<tr><th id="787">787</th><td>                       num_blocks, block_size, <var>0</var>, device, reducer, self, num_coeffs_to_reduce, num_preserved_vals, output);</td></tr>
<tr><th id="788">788</th><td></td></tr>
<tr><th id="789">789</th><td>    <b>return</b> <b>false</b>;</td></tr>
<tr><th id="790">790</th><td>  }</td></tr>
<tr><th id="791">791</th><td>};</td></tr>
<tr><th id="792">792</th><td></td></tr>
<tr><th id="793">793</th><td><u>#ifdef EIGEN_HAS_GPU_FP16</u></td></tr>
<tr><th id="794">794</th><td><b>template</b> &lt;<b>typename</b> Self, <b>typename</b> Op&gt;</td></tr>
<tr><th id="795">795</th><td><b>struct</b> InnerReductionLauncher&lt;Self, Op, Eigen::half, <b>false</b>&gt; {</td></tr>
<tr><th id="796">796</th><td>  <em>static</em> <em>bool</em> run(<em>const</em> Self&amp;, Op&amp;, <em>const</em> GpuDevice&amp;, half*, <b>typename</b> Self::Index, <b>typename</b> Self::Index) {</td></tr>
<tr><th id="797">797</th><td>    gpu_assert(<b>false</b> &amp;&amp; <q>"Should not be called since there is no packet accessor"</q>);</td></tr>
<tr><th id="798">798</th><td>    <b>return</b> <b>true</b>;</td></tr>
<tr><th id="799">799</th><td>  }</td></tr>
<tr><th id="800">800</th><td>};</td></tr>
<tr><th id="801">801</th><td></td></tr>
<tr><th id="802">802</th><td><b>template</b> &lt;<b>typename</b> Self, <b>typename</b> Op&gt;</td></tr>
<tr><th id="803">803</th><td><b>struct</b> InnerReductionLauncher&lt;Self, Op, Eigen::half, <b>true</b>&gt; {</td></tr>
<tr><th id="804">804</th><td>  <em>static</em> <em>bool</em> run(<em>const</em> Self&amp; self, Op&amp; reducer, <em>const</em> GpuDevice&amp; device, half* output, <b>typename</b> Self::Index num_coeffs_to_reduce, <b>typename</b> Self::Index num_preserved_vals) {</td></tr>
<tr><th id="805">805</th><td>    <b>typedef</b> <b>typename</b> Self::Index Index;</td></tr>
<tr><th id="806">806</th><td></td></tr>
<tr><th id="807">807</th><td>    <b>if</b> (num_preserved_vals % <var>2</var> != <var>0</var>) {</td></tr>
<tr><th id="808">808</th><td>      <i>// Not supported yet, revert to the slower code path</i></td></tr>
<tr><th id="809">809</th><td>      <b>return</b> <b>true</b>;</td></tr>
<tr><th id="810">810</th><td>    }</td></tr>
<tr><th id="811">811</th><td></td></tr>
<tr><th id="812">812</th><td>    <em>const</em> Index num_coeffs = num_coeffs_to_reduce * num_preserved_vals;</td></tr>
<tr><th id="813">813</th><td>    <em>const</em> <em>int</em> block_size = <i>/*256*/</i><var>128</var>;</td></tr>
<tr><th id="814">814</th><td>    <em>const</em> <em>int</em> num_per_thread = <i>/*128*/</i><var>64</var>;</td></tr>
<tr><th id="815">815</th><td>    <em>const</em> <em>int</em> dyn_blocks = divup&lt;<em>int</em>&gt;(num_coeffs, block_size * num_per_thread);</td></tr>
<tr><th id="816">816</th><td>    <em>const</em> <em>int</em> max_blocks = device.getNumGpuMultiProcessors() *</td></tr>
<tr><th id="817">817</th><td>                           device.maxGpuThreadsPerMultiProcessor() / block_size;</td></tr>
<tr><th id="818">818</th><td>    <em>const</em> <em>int</em> num_blocks = numext::mini&lt;<em>int</em>&gt;(max_blocks, dyn_blocks);</td></tr>
<tr><th id="819">819</th><td></td></tr>
<tr><th id="820">820</th><td>    <b>if</b> (num_blocks &gt; <var>1</var>) {</td></tr>
<tr><th id="821">821</th><td>      <i>// We initialize the outputs outside the reduction kernel when we can't be sure that there</i></td></tr>
<tr><th id="822">822</th><td><i>      // won't be a race conditions between multiple thread blocks.</i></td></tr>
<tr><th id="823">823</th><td>      LAUNCH_GPU_KERNEL((ReductionInitKernelHalfFloat&lt;Self, Op, Index&gt;),</td></tr>
<tr><th id="824">824</th><td>                         <var>1</var>, <var>1</var>, <var>0</var>, device, reducer, self, num_preserved_vals, output);</td></tr>
<tr><th id="825">825</th><td>    }</td></tr>
<tr><th id="826">826</th><td></td></tr>
<tr><th id="827">827</th><td>    LAUNCH_GPU_KERNEL((InnerReductionKernelHalfFloat&lt;num_per_thread, Self, Op, Index&gt;),</td></tr>
<tr><th id="828">828</th><td>                       num_blocks, block_size, <var>0</var>, device, reducer, self, num_coeffs_to_reduce, num_preserved_vals, output);</td></tr>
<tr><th id="829">829</th><td></td></tr>
<tr><th id="830">830</th><td>    <b>return</b> <b>false</b>;</td></tr>
<tr><th id="831">831</th><td>  }</td></tr>
<tr><th id="832">832</th><td>};</td></tr>
<tr><th id="833">833</th><td><u>#endif // EIGEN_HAS_GPU_FP16</u></td></tr>
<tr><th id="834">834</th><td></td></tr>
<tr><th id="835">835</th><td></td></tr>
<tr><th id="836">836</th><td><b>template</b> &lt;<b>typename</b> Self, <b>typename</b> Op&gt;</td></tr>
<tr><th id="837">837</th><td><b>struct</b> InnerReducer&lt;Self, Op, GpuDevice&gt; {</td></tr>
<tr><th id="838">838</th><td>  <i>// Unfortunately nvidia doesn't support well exotic types such as complex,</i></td></tr>
<tr><th id="839">839</th><td><i>  // so reduce the scope of the optimized version of the code to the simple case</i></td></tr>
<tr><th id="840">840</th><td><i>  // of floats and half floats.</i></td></tr>
<tr><th id="841">841</th><td><u>#ifdef EIGEN_HAS_GPU_FP16</u></td></tr>
<tr><th id="842">842</th><td>  <em>static</em> <em>const</em> <em>bool</em> HasOptimizedImplementation = !Self::ReducerTraits::IsStateful &amp;&amp;</td></tr>
<tr><th id="843">843</th><td>      (internal::is_same&lt;<b>typename</b> Self::CoeffReturnType, <em>float</em>&gt;::value ||</td></tr>
<tr><th id="844">844</th><td>       internal::is_same&lt;<b>typename</b> Self::CoeffReturnType, <em>double</em>&gt;::value ||</td></tr>
<tr><th id="845">845</th><td>       (internal::is_same&lt;<b>typename</b> Self::CoeffReturnType, Eigen::half&gt;::value &amp;&amp; reducer_traits&lt;Op, GpuDevice&gt;::PacketAccess));</td></tr>
<tr><th id="846">846</th><td><u>#else // EIGEN_HAS_GPU_FP16</u></td></tr>
<tr><th id="847">847</th><td>  <em>static</em> <em>const</em> <em>bool</em> HasOptimizedImplementation = !Self::ReducerTraits::IsStateful &amp;&amp;</td></tr>
<tr><th id="848">848</th><td>                                                 (internal::is_same&lt;<b>typename</b> Self::CoeffReturnType, <em>float</em>&gt;::value ||</td></tr>
<tr><th id="849">849</th><td>                                                  internal::is_same&lt;<b>typename</b> Self::CoeffReturnType, <em>double</em>&gt;::value);</td></tr>
<tr><th id="850">850</th><td><u>#endif // EIGEN_HAS_GPU_FP16</u></td></tr>
<tr><th id="851">851</th><td></td></tr>
<tr><th id="852">852</th><td>  <b>template</b> &lt;<b>typename</b> OutputType&gt;</td></tr>
<tr><th id="853">853</th><td>  <em>static</em> <em>bool</em> run(<em>const</em> Self&amp; self, Op&amp; reducer, <em>const</em> GpuDevice&amp; device, OutputType* output, <b>typename</b> Self::Index num_coeffs_to_reduce, <b>typename</b> Self::Index num_preserved_vals) {</td></tr>
<tr><th id="854">854</th><td>    gpu_assert(HasOptimizedImplementation &amp;&amp; <q>"Should only be called on doubles, floats or half floats"</q>);</td></tr>
<tr><th id="855">855</th><td>    <em>const</em> Index num_coeffs = array_prod(self.m_impl.dimensions());</td></tr>
<tr><th id="856">856</th><td>    <i>// Don't crash when we're called with an input tensor of size 0.</i></td></tr>
<tr><th id="857">857</th><td>    <b>if</b> (num_coeffs == <var>0</var>) {</td></tr>
<tr><th id="858">858</th><td>      <b>return</b> <b>true</b>;</td></tr>
<tr><th id="859">859</th><td>    }</td></tr>
<tr><th id="860">860</th><td>    <i>// It's faster to use the usual code.</i></td></tr>
<tr><th id="861">861</th><td>    <b>if</b> (num_coeffs_to_reduce &lt;= <var>128</var>) {</td></tr>
<tr><th id="862">862</th><td>      <b>return</b> <b>true</b>;</td></tr>
<tr><th id="863">863</th><td>    }</td></tr>
<tr><th id="864">864</th><td></td></tr>
<tr><th id="865">865</th><td>    <b>return</b> InnerReductionLauncher&lt;Self, Op, OutputType, reducer_traits&lt;Op, GpuDevice&gt;::PacketAccess&gt;::run(self, reducer, device, output, num_coeffs_to_reduce, num_preserved_vals);</td></tr>
<tr><th id="866">866</th><td>  }</td></tr>
<tr><th id="867">867</th><td>};</td></tr>
<tr><th id="868">868</th><td></td></tr>
<tr><th id="869">869</th><td><b>template</b> &lt;<em>int</em> NumPerThread, <b>typename</b> Self,</td></tr>
<tr><th id="870">870</th><td>          <b>typename</b> Reducer, <b>typename</b> Index&gt;</td></tr>
<tr><th id="871">871</th><td>__global__ __launch_bounds__(<var>1024</var>) <em>void</em> OuterReductionKernel(Reducer reducer, <em>const</em> Self input, Index num_coeffs_to_reduce, Index num_preserved_coeffs,</td></tr>
<tr><th id="872">872</th><td>                                     <b>typename</b> Self::CoeffReturnType* output) {</td></tr>
<tr><th id="873">873</th><td>  <em>const</em> Index num_threads = blockDim.x * gridDim.x;</td></tr>
<tr><th id="874">874</th><td>  <em>const</em> Index thread_id = blockIdx.x * blockDim.x + threadIdx.x;</td></tr>
<tr><th id="875">875</th><td>  <i>// Initialize the output values if they weren't initialized by the ReductionInitKernel</i></td></tr>
<tr><th id="876">876</th><td>  <b>if</b> (gridDim.x == <var>1</var>) {</td></tr>
<tr><th id="877">877</th><td>    <b>for</b> (Index i = thread_id; i &lt; num_preserved_coeffs; i += num_threads) {</td></tr>
<tr><th id="878">878</th><td>      output[i] = reducer.initialize();</td></tr>
<tr><th id="879">879</th><td>    }</td></tr>
<tr><th id="880">880</th><td>    __syncthreads();</td></tr>
<tr><th id="881">881</th><td>  }</td></tr>
<tr><th id="882">882</th><td></td></tr>
<tr><th id="883">883</th><td>  <i>// Do the reduction.</i></td></tr>
<tr><th id="884">884</th><td>  <em>const</em> Index max_iter = num_preserved_coeffs * divup&lt;Index&gt;(num_coeffs_to_reduce, NumPerThread);</td></tr>
<tr><th id="885">885</th><td>  <b>for</b> (Index i = thread_id; i &lt; max_iter; i += num_threads) {</td></tr>
<tr><th id="886">886</th><td>    <em>const</em> Index input_col = i % num_preserved_coeffs;</td></tr>
<tr><th id="887">887</th><td>    <em>const</em> Index input_row = (i / num_preserved_coeffs) * NumPerThread;</td></tr>
<tr><th id="888">888</th><td>    <b>typename</b> Self::CoeffReturnType reduced_val = reducer.initialize();</td></tr>
<tr><th id="889">889</th><td>    <em>const</em> Index max_row = numext::mini(input_row + NumPerThread, num_coeffs_to_reduce);</td></tr>
<tr><th id="890">890</th><td>    <b>for</b> (Index j = input_row; j &lt; max_row; j++) {</td></tr>
<tr><th id="891">891</th><td>      <b>typename</b> Self::CoeffReturnType val = input.m_impl.coeff(j * num_preserved_coeffs + input_col);</td></tr>
<tr><th id="892">892</th><td>      reducer.reduce(val, &amp;reduced_val);</td></tr>
<tr><th id="893">893</th><td>    }</td></tr>
<tr><th id="894">894</th><td>    atomicReduce(&amp;(output[input_col]), reduced_val, reducer);</td></tr>
<tr><th id="895">895</th><td>  }</td></tr>
<tr><th id="896">896</th><td>}</td></tr>
<tr><th id="897">897</th><td></td></tr>
<tr><th id="898">898</th><td></td></tr>
<tr><th id="899">899</th><td><b>template</b> &lt;<b>typename</b> Self, <b>typename</b> Op&gt;</td></tr>
<tr><th id="900">900</th><td><b>struct</b> OuterReducer&lt;Self, Op, GpuDevice&gt; {</td></tr>
<tr><th id="901">901</th><td>  <i>// Unfortunately nvidia doesn't support well exotic types such as complex,</i></td></tr>
<tr><th id="902">902</th><td><i>  // so reduce the scope of the optimized version of the code to the simple case</i></td></tr>
<tr><th id="903">903</th><td><i>  // of floats.</i></td></tr>
<tr><th id="904">904</th><td>  <em>static</em> <em>const</em> <em>bool</em> HasOptimizedImplementation = !Self::ReducerTraits::IsStateful &amp;&amp;</td></tr>
<tr><th id="905">905</th><td>                                                 (internal::is_same&lt;<b>typename</b> Self::CoeffReturnType, <em>float</em>&gt;::value ||</td></tr>
<tr><th id="906">906</th><td>                                                  internal::is_same&lt;<b>typename</b> Self::CoeffReturnType, <em>double</em>&gt;::value);</td></tr>
<tr><th id="907">907</th><td>  <b>template</b> &lt;<b>typename</b> Device, <b>typename</b> OutputType&gt;</td></tr>
<tr><th id="908">908</th><td>  <em>static</em></td></tr>
<tr><th id="909">909</th><td>    <u>#if !defined(EIGEN_HIPCC)</u></td></tr>
<tr><th id="910">910</th><td>    <i>// FIXME :  leaving this EIGEN_DEVICE_FUNC in, results in the following runtime error</i></td></tr>
<tr><th id="911">911</th><td><i>    //          (in the cxx11_tensor_reduction_gpu test)</i></td></tr>
<tr><th id="912">912</th><td><i>    //</i></td></tr>
<tr><th id="913">913</th><td><i>    // terminate called after throwing an instance of 'std::runtime_error'</i></td></tr>
<tr><th id="914">914</th><td><i>    //   what():  No device code available for function: _ZN5Eigen8internal20OuterReductionKernelIL...</i></td></tr>
<tr><th id="915">915</th><td><i>    //</i></td></tr>
<tr><th id="916">916</th><td><i>    // don't know why this happens (and why is it a runtime error instead of a compile time error)</i></td></tr>
<tr><th id="917">917</th><td><i>    //</i></td></tr>
<tr><th id="918">918</th><td><i>    // this will be fixed by HIP PR#457</i></td></tr>
<tr><th id="919">919</th><td>    EIGEN_DEVICE_FUNC</td></tr>
<tr><th id="920">920</th><td>    <u>#endif</u></td></tr>
<tr><th id="921">921</th><td>    <em>bool</em> run(<em>const</em> Self&amp;, Op&amp;, <em>const</em> Device&amp;, OutputType*, <b>typename</b> Self::Index, <b>typename</b> Self::Index) {</td></tr>
<tr><th id="922">922</th><td>    gpu_assert(<b>false</b> &amp;&amp; <q>"Should only be called to reduce doubles or floats on a gpu device"</q>);</td></tr>
<tr><th id="923">923</th><td>    <b>return</b> <b>true</b>;</td></tr>
<tr><th id="924">924</th><td>  }</td></tr>
<tr><th id="925">925</th><td></td></tr>
<tr><th id="926">926</th><td>  <em>static</em> <em>bool</em> run(<em>const</em> Self&amp; self, Op&amp; reducer, <em>const</em> GpuDevice&amp; device, <em>float</em>* output, <b>typename</b> Self::Index num_coeffs_to_reduce, <b>typename</b> Self::Index num_preserved_vals) {</td></tr>
<tr><th id="927">927</th><td>    <b>typedef</b> <b>typename</b> Self::Index Index;</td></tr>
<tr><th id="928">928</th><td></td></tr>
<tr><th id="929">929</th><td>    <i>// It's faster to use the usual code.</i></td></tr>
<tr><th id="930">930</th><td>    <b>if</b> (num_coeffs_to_reduce &lt;= <var>32</var>) {</td></tr>
<tr><th id="931">931</th><td>      <b>return</b> <b>true</b>;</td></tr>
<tr><th id="932">932</th><td>    }</td></tr>
<tr><th id="933">933</th><td></td></tr>
<tr><th id="934">934</th><td>    <em>const</em> Index num_coeffs = num_coeffs_to_reduce * num_preserved_vals;</td></tr>
<tr><th id="935">935</th><td>    <em>const</em> <em>int</em> block_size = <var>256</var>;</td></tr>
<tr><th id="936">936</th><td>    <em>const</em> <em>int</em> num_per_thread = <var>16</var>;</td></tr>
<tr><th id="937">937</th><td>    <em>const</em> <em>int</em> dyn_blocks = divup&lt;<em>int</em>&gt;(num_coeffs, block_size * num_per_thread);</td></tr>
<tr><th id="938">938</th><td>    <em>const</em> <em>int</em> max_blocks = device.getNumGpuMultiProcessors() *</td></tr>
<tr><th id="939">939</th><td>                           device.maxGpuThreadsPerMultiProcessor() / block_size;</td></tr>
<tr><th id="940">940</th><td>    <em>const</em> <em>int</em> num_blocks = numext::mini&lt;<em>int</em>&gt;(max_blocks, dyn_blocks);</td></tr>
<tr><th id="941">941</th><td></td></tr>
<tr><th id="942">942</th><td>    <b>if</b> (num_blocks &gt; <var>1</var>) {</td></tr>
<tr><th id="943">943</th><td>      <i>// We initialize the outputs in the reduction kernel itself when we don't have to worry</i></td></tr>
<tr><th id="944">944</th><td><i>      // about race conditions between multiple thread blocks.</i></td></tr>
<tr><th id="945">945</th><td>      <em>const</em> <em>int</em> dyn_blocks = divup&lt;<em>int</em>&gt;(num_preserved_vals, <var>1024</var>);</td></tr>
<tr><th id="946">946</th><td>      <em>const</em> <em>int</em> max_blocks = device.getNumGpuMultiProcessors() *</td></tr>
<tr><th id="947">947</th><td>                             device.maxGpuThreadsPerMultiProcessor() / <var>1024</var>;</td></tr>
<tr><th id="948">948</th><td>      <em>const</em> <em>int</em> num_blocks = numext::mini&lt;<em>int</em>&gt;(max_blocks, dyn_blocks);</td></tr>
<tr><th id="949">949</th><td>      LAUNCH_GPU_KERNEL((ReductionInitKernel&lt;<em>float</em>, Index&gt;),</td></tr>
<tr><th id="950">950</th><td>                         num_blocks, <var>1024</var>, <var>0</var>, device, reducer.initialize(),</td></tr>
<tr><th id="951">951</th><td>                         num_preserved_vals, output);</td></tr>
<tr><th id="952">952</th><td>    }</td></tr>
<tr><th id="953">953</th><td></td></tr>
<tr><th id="954">954</th><td>    LAUNCH_GPU_KERNEL((OuterReductionKernel&lt;num_per_thread, Self, Op, Index&gt;),</td></tr>
<tr><th id="955">955</th><td>                       num_blocks, block_size, <var>0</var>, device, reducer, self, num_coeffs_to_reduce, num_preserved_vals, output);</td></tr>
<tr><th id="956">956</th><td></td></tr>
<tr><th id="957">957</th><td>    <b>return</b> <b>false</b>;</td></tr>
<tr><th id="958">958</th><td>  }</td></tr>
<tr><th id="959">959</th><td>};</td></tr>
<tr><th id="960">960</th><td></td></tr>
<tr><th id="961">961</th><td><u>#<span data-ppcond="17">endif</span> // defined(EIGEN_USE_GPU) &amp;&amp; defined(EIGEN_GPUCC)</u></td></tr>
<tr><th id="962">962</th><td></td></tr>
<tr><th id="963">963</th><td></td></tr>
<tr><th id="964">964</th><td>} <i>// end namespace internal</i></td></tr>
<tr><th id="965">965</th><td>} <i>// end namespace Eigen</i></td></tr>
<tr><th id="966">966</th><td></td></tr>
<tr><th id="967">967</th><td><u>#<span data-ppcond="10">endif</span> // EIGEN_CXX11_TENSOR_TENSOR_REDUCTION_GPU_H</u></td></tr>
<tr><th id="968">968</th><td></td></tr>
</table><hr/><p id='footer'>
Generated while processing <a href='../../../../../../_deps/tflite-src/tensorflow/lite/kernels/activations.cc.html'>halide/build-apps/_deps/tflite-src/tensorflow/lite/kernels/activations.cc</a><br/>Generated on <em>2021-Aug-05</em> from project halide revision <em>v12.0.1</em>