<!doctype html>
<html>
<head>
<meta name="viewport" content="width=device-width, initial-scale=1.0"><title>pack_avx2_fma.cc source code [halide/build-apps/ruy/ruy/pack_avx2_fma.cc] - Woboq Code Browser</title>
<link rel="stylesheet" href="../../../.././data/qtcreator.css" title="QtCreator"/>
<link rel="alternate stylesheet" href="../../../.././data/kdevelop.css" title="KDevelop"/>
<script type="text/javascript" src="../../../.././data/jquery/jquery.min.js"></script>
<script type="text/javascript" src="../../../.././data/jquery/jquery-ui.min.js"></script>
<script>var file = 'halide/build-apps/ruy/ruy/pack_avx2_fma.cc'; var root_path = '../../../..'; var data_path = '../../../.././data'; var ecma_script_api_version = 2;</script>
<script src='../../../.././data/codebrowser.js'></script>
</head>
<body><div id='header'><h1 id='breadcrumb'><span>Browse the source code of </span><a href='../../..'>halide</a>/<a href='../..'>build-apps</a>/<a href='..'>ruy</a>/<a href='./'>ruy</a>/<a href='pack_avx2_fma.cc.html'>pack_avx2_fma.cc</a></h1></div>
<hr/><div id='content'><table class="code">
<tr><th id="1">1</th><td><i>/* Copyright 2019 Google LLC. All Rights Reserved.</i></td></tr>
<tr><th id="2">2</th><td><i></i></td></tr>
<tr><th id="3">3</th><td><i>Licensed under the Apache License, Version 2.0 (the "License");</i></td></tr>
<tr><th id="4">4</th><td><i>you may not use this file except in compliance with the License.</i></td></tr>
<tr><th id="5">5</th><td><i>You may obtain a copy of the License at</i></td></tr>
<tr><th id="6">6</th><td><i></i></td></tr>
<tr><th id="7">7</th><td><i>    <a href="http://www.apache.org/licenses/LICENSE-2.0">http://www.apache.org/licenses/LICENSE-2.0</a></i></td></tr>
<tr><th id="8">8</th><td><i></i></td></tr>
<tr><th id="9">9</th><td><i>Unless required by applicable law or agreed to in writing, software</i></td></tr>
<tr><th id="10">10</th><td><i>distributed under the License is distributed on an "AS IS" BASIS,</i></td></tr>
<tr><th id="11">11</th><td><i>WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</i></td></tr>
<tr><th id="12">12</th><td><i>See the License for the specific language governing permissions and</i></td></tr>
<tr><th id="13">13</th><td><i>limitations under the License.</i></td></tr>
<tr><th id="14">14</th><td><i>==============================================================================*/</i></td></tr>
<tr><th id="15">15</th><td></td></tr>
<tr><th id="16">16</th><td><u>#include &lt;cstdint&gt;</u></td></tr>
<tr><th id="17">17</th><td><u>#include &lt;cstring&gt;</u></td></tr>
<tr><th id="18">18</th><td></td></tr>
<tr><th id="19">19</th><td><u>#include <a href="check_macros.h.html">"ruy/check_macros.h"</a></u></td></tr>
<tr><th id="20">20</th><td><u>#include <a href="opt_set.h.html">"ruy/opt_set.h"</a></u></td></tr>
<tr><th id="21">21</th><td><u>#include <a href="pack_x86.h.html">"ruy/pack_x86.h"</a></u></td></tr>
<tr><th id="22">22</th><td><u>#include <a href="path.h.html">"ruy/path.h"</a></u></td></tr>
<tr><th id="23">23</th><td><u>#include <a href="platform.h.html">"ruy/platform.h"</a></u></td></tr>
<tr><th id="24">24</th><td><u>#include <a href="profiler/instrumentation.h.html">"ruy/profiler/instrumentation.h"</a></u></td></tr>
<tr><th id="25">25</th><td></td></tr>
<tr><th id="26">26</th><td><u>#<span data-ppcond="26">if</span> <a class="macro" href="platform.h.html#143" title="0" data-ref="_M/RUY_PLATFORM_AVX2_FMA">RUY_PLATFORM_AVX2_FMA</a> &amp;&amp; <a class="macro" href="opt_set.h.html#49" title="(((~0) &amp; 0x1) != 0)" data-ref="_M/RUY_OPT">RUY_OPT</a>(INTRINSICS)</u></td></tr>
<tr><th id="27">27</th><td><u>#include &lt;immintrin.h&gt;  // IWYU pragma: keep</u></td></tr>
<tr><th id="28">28</th><td><u>#<span data-ppcond="26">endif</span></u></td></tr>
<tr><th id="29">29</th><td></td></tr>
<tr><th id="30">30</th><td><b>namespace</b> <span class="namespace">ruy</span> {</td></tr>
<tr><th id="31">31</th><td></td></tr>
<tr><th id="32">32</th><td><u>#<span data-ppcond="32">if</span> !(<a class="macro" href="platform.h.html#143" title="0" data-ref="_M/RUY_PLATFORM_AVX2_FMA">RUY_PLATFORM_AVX2_FMA</a> &amp;&amp; <a class="macro" href="opt_set.h.html#49" title="(((~0) &amp; 0x2) != 0)" data-ref="_M/RUY_OPT">RUY_OPT</a>(ASM))</u></td></tr>
<tr><th id="33">33</th><td></td></tr>
<tr><th id="34">34</th><td><em>void</em> <dfn class="decl def fn" id="_ZN3ruy23Pack8bitColMajorForAvx2EPKaaS1_iiiPaPi" title='ruy::Pack8bitColMajorForAvx2' data-ref="_ZN3ruy23Pack8bitColMajorForAvx2EPKaaS1_iiiPaPi" data-ref-filename="_ZN3ruy23Pack8bitColMajorForAvx2EPKaaS1_iiiPaPi">Pack8bitColMajorForAvx2</dfn>(<em>const</em> <span class="namespace">std::</span><a class="typedef" href="../../../../include/bits/stdint-intn.h.html#int8_t" title='int8_t' data-type='__int8_t' data-ref="int8_t" data-ref-filename="int8_t">int8_t</a>*, <span class="namespace">std::</span><a class="typedef" href="../../../../include/bits/stdint-intn.h.html#int8_t" title='int8_t' data-type='__int8_t' data-ref="int8_t" data-ref-filename="int8_t">int8_t</a>,</td></tr>
<tr><th id="35">35</th><td>                             <em>const</em> <span class="namespace">std::</span><a class="typedef" href="../../../../include/bits/stdint-intn.h.html#int8_t" title='int8_t' data-type='__int8_t' data-ref="int8_t" data-ref-filename="int8_t">int8_t</a>*, <em>int</em>, <em>int</em>, <em>int</em>, <span class="namespace">std::</span><a class="typedef" href="../../../../include/bits/stdint-intn.h.html#int8_t" title='int8_t' data-type='__int8_t' data-ref="int8_t" data-ref-filename="int8_t">int8_t</a>*,</td></tr>
<tr><th id="36">36</th><td>                             <span class="namespace">std::</span><a class="typedef" href="../../../../include/bits/stdint-intn.h.html#int32_t" title='int32_t' data-type='__int32_t' data-ref="int32_t" data-ref-filename="int32_t">int32_t</a>*) {</td></tr>
<tr><th id="37">37</th><td>  <i>// CPU-ID-based checks should disable the path that would reach this point.</i></td></tr>
<tr><th id="38">38</th><td>  <a class="macro" href="check_macros.h.html#129" title="if (false) ruy::check_macros::CheckImpl(false, &quot;/fast/tmp/halide.git/build-apps/ruy/ruy/pack_avx2_fma.cc&quot;, 38, &quot;RUY_CHECK&quot;, &quot;false&quot;)" data-ref="_M/RUY_DCHECK">RUY_DCHECK</a>(<b>false</b>);</td></tr>
<tr><th id="39">39</th><td>}</td></tr>
<tr><th id="40">40</th><td></td></tr>
<tr><th id="41">41</th><td><em>void</em> <dfn class="decl def fn" id="_ZN3ruy24PackFloatColMajorForAvx2EPKfS1_iiiPf" title='ruy::PackFloatColMajorForAvx2' data-ref="_ZN3ruy24PackFloatColMajorForAvx2EPKfS1_iiiPf" data-ref-filename="_ZN3ruy24PackFloatColMajorForAvx2EPKfS1_iiiPf">PackFloatColMajorForAvx2</dfn>(<em>const</em> <em>float</em>*, <em>const</em> <em>float</em>*, <em>int</em>, <em>int</em>, <em>int</em>,</td></tr>
<tr><th id="42">42</th><td>                              <em>float</em>*) {</td></tr>
<tr><th id="43">43</th><td>  <i>// CPU-ID-based checks should disable the path that would reach this point.</i></td></tr>
<tr><th id="44">44</th><td>  <a class="macro" href="check_macros.h.html#129" title="if (false) ruy::check_macros::CheckImpl(false, &quot;/fast/tmp/halide.git/build-apps/ruy/ruy/pack_avx2_fma.cc&quot;, 44, &quot;RUY_CHECK&quot;, &quot;false&quot;)" data-ref="_M/RUY_DCHECK">RUY_DCHECK</a>(<b>false</b>);</td></tr>
<tr><th id="45">45</th><td>}</td></tr>
<tr><th id="46">46</th><td></td></tr>
<tr><th id="47">47</th><td><em>void</em> <dfn class="decl def fn" id="_ZN3ruy23Pack8bitRowMajorForAvx2EPKhiiPaiiiiiiiPi" title='ruy::Pack8bitRowMajorForAvx2' data-ref="_ZN3ruy23Pack8bitRowMajorForAvx2EPKhiiPaiiiiiiiPi" data-ref-filename="_ZN3ruy23Pack8bitRowMajorForAvx2EPKhiiPaiiiiiiiPi">Pack8bitRowMajorForAvx2</dfn>(<em>const</em> <span class="namespace">std::</span><a class="typedef" href="../../../../include/bits/stdint-uintn.h.html#uint8_t" title='uint8_t' data-type='__uint8_t' data-ref="uint8_t" data-ref-filename="uint8_t">uint8_t</a>*, <em>int</em>, <em>int</em>, <span class="namespace">std::</span><a class="typedef" href="../../../../include/bits/stdint-intn.h.html#int8_t" title='int8_t' data-type='__int8_t' data-ref="int8_t" data-ref-filename="int8_t">int8_t</a>*, <em>int</em>,</td></tr>
<tr><th id="48">48</th><td>                             <em>int</em>, <em>int</em>, <em>int</em>, <em>int</em>, <em>int</em>, <em>int</em>, <span class="namespace">std::</span><a class="typedef" href="../../../../include/bits/stdint-intn.h.html#int32_t" title='int32_t' data-type='__int32_t' data-ref="int32_t" data-ref-filename="int32_t">int32_t</a>*) {</td></tr>
<tr><th id="49">49</th><td>  <a class="macro" href="check_macros.h.html#129" title="if (false) ruy::check_macros::CheckImpl(false, &quot;/fast/tmp/halide.git/build-apps/ruy/ruy/pack_avx2_fma.cc&quot;, 49, &quot;RUY_CHECK&quot;, &quot;false&quot;)" data-ref="_M/RUY_DCHECK">RUY_DCHECK</a>(<b>false</b>);</td></tr>
<tr><th id="50">50</th><td>}</td></tr>
<tr><th id="51">51</th><td></td></tr>
<tr><th id="52">52</th><td><u>#<span data-ppcond="32">else</span>  // RUY_PLATFORM_AVX2_FMA &amp;&amp; RUY_OPT(ASM)</u></td></tr>
<tr><th id="53">53</th><td></td></tr>
<tr><th id="54">54</th><td><i>// The first int8_t template parameter is arbitrary: this routine is common to</i></td></tr>
<tr><th id="55">55</th><td><i>// all 8-bit source matrix types.</i></td></tr>
<tr><th id="56">56</th><td><b>using</b> PackImpl8bitAvx2 =</td></tr>
<tr><th id="57">57</th><td>    PackImpl&lt;Path::kAvx2Fma, FixedKernelLayout&lt;Order::kColMajor, <var>4</var>, <var>8</var>&gt;,</td></tr>
<tr><th id="58">58</th><td>             std::int8_t, std::int8_t, std::int32_t, Order::kColMajor&gt;;</td></tr>
<tr><th id="59">59</th><td></td></tr>
<tr><th id="60">60</th><td><b>using</b> PackImplFloatAvx2 =</td></tr>
<tr><th id="61">61</th><td>    PackImpl&lt;Path::kAvx2Fma, FixedKernelLayout&lt;Order::kRowMajor, <var>1</var>, <var>8</var>&gt;, <em>float</em>,</td></tr>
<tr><th id="62">62</th><td>             <em>float</em>, <em>float</em>, Order::kColMajor&gt;;</td></tr>
<tr><th id="63">63</th><td></td></tr>
<tr><th id="64">64</th><td><b>namespace</b> {</td></tr>
<tr><th id="65">65</th><td></td></tr>
<tr><th id="66">66</th><td><b>inline</b> <em>void</em> Pack8bitColMajorForAvx2Packer(</td></tr>
<tr><th id="67">67</th><td>    <em>const</em> std::int8_t* src_ptr, std::int8_t input_xor,</td></tr>
<tr><th id="68">68</th><td>    <em>const</em> std::int8_t* zerobuf, <em>int</em> src_stride, <em>int</em> remaining_src_cols,</td></tr>
<tr><th id="69">69</th><td>    <em>int</em> src_rows, std::int8_t* packed_ptr, std::int32_t* sums_ptr,</td></tr>
<tr><th id="70">70</th><td>    std::int8_t* trailing_buf) {</td></tr>
<tr><th id="71">71</th><td>  <b>using</b> Layout = PackImpl8bitAvx2::Layout;</td></tr>
<tr><th id="72">72</th><td>  RUY_DCHECK_EQ(Layout::kCols, <var>8</var>);</td></tr>
<tr><th id="73">73</th><td>  RUY_DCHECK_EQ(Layout::kRows, <var>4</var>);</td></tr>
<tr><th id="74">74</th><td>  <i>// Each Layout::Rows is 4 contiguous input, contiguous packed elements.</i></td></tr>
<tr><th id="75">75</th><td><i>  // We process 8 of these chunks at a time, padding short input chunks.</i></td></tr>
<tr><th id="76">76</th><td>  <b>constexpr</b> <em>int</em> kNumRowChunks = <var>8</var>;</td></tr>
<tr><th id="77">77</th><td>  <b>constexpr</b> <em>int</em> kNumChunkedSrcRows = kNumRowChunks * Layout::kRows;</td></tr>
<tr><th id="78">78</th><td></td></tr>
<tr><th id="79">79</th><td>  <em>const</em> std::int8_t* src_ptr0 = src_ptr;</td></tr>
<tr><th id="80">80</th><td>  <em>const</em> std::int8_t* src_ptr1 = src_ptr0 + src_stride;</td></tr>
<tr><th id="81">81</th><td>  <em>const</em> std::int8_t* src_ptr2 = src_ptr1 + src_stride;</td></tr>
<tr><th id="82">82</th><td>  <em>const</em> std::int8_t* src_ptr3 = src_ptr2 + src_stride;</td></tr>
<tr><th id="83">83</th><td>  <em>const</em> std::int8_t* src_ptr4 = src_ptr3 + src_stride;</td></tr>
<tr><th id="84">84</th><td>  <em>const</em> std::int8_t* src_ptr5 = src_ptr4 + src_stride;</td></tr>
<tr><th id="85">85</th><td>  <em>const</em> std::int8_t* src_ptr6 = src_ptr5 + src_stride;</td></tr>
<tr><th id="86">86</th><td>  <em>const</em> std::int8_t* src_ptr7 = src_ptr6 + src_stride;</td></tr>
<tr><th id="87">87</th><td>  std::int64_t src_inc0 = kNumChunkedSrcRows;</td></tr>
<tr><th id="88">88</th><td>  std::int64_t src_inc1 = kNumChunkedSrcRows;</td></tr>
<tr><th id="89">89</th><td>  std::int64_t src_inc2 = kNumChunkedSrcRows;</td></tr>
<tr><th id="90">90</th><td>  std::int64_t src_inc3 = kNumChunkedSrcRows;</td></tr>
<tr><th id="91">91</th><td>  std::int64_t src_inc4 = kNumChunkedSrcRows;</td></tr>
<tr><th id="92">92</th><td>  std::int64_t src_inc5 = kNumChunkedSrcRows;</td></tr>
<tr><th id="93">93</th><td>  std::int64_t src_inc6 = kNumChunkedSrcRows;</td></tr>
<tr><th id="94">94</th><td>  std::int64_t src_inc7 = kNumChunkedSrcRows;</td></tr>
<tr><th id="95">95</th><td>  <i>// Handle cases where source does not have Layout::kCols (8) columns.</i></td></tr>
<tr><th id="96">96</th><td>  <b>if</b> (remaining_src_cols &lt; <var>8</var>) {</td></tr>
<tr><th id="97">97</th><td>    <b>if</b> (remaining_src_cols &lt;= <var>0</var>) {</td></tr>
<tr><th id="98">98</th><td>      src_ptr0 = zerobuf;</td></tr>
<tr><th id="99">99</th><td>      src_inc0 = <var>0</var>;</td></tr>
<tr><th id="100">100</th><td>    }</td></tr>
<tr><th id="101">101</th><td>    <b>if</b> (remaining_src_cols &lt;= <var>1</var>) {</td></tr>
<tr><th id="102">102</th><td>      src_ptr1 = zerobuf;</td></tr>
<tr><th id="103">103</th><td>      src_inc1 = <var>0</var>;</td></tr>
<tr><th id="104">104</th><td>    }</td></tr>
<tr><th id="105">105</th><td>    <b>if</b> (remaining_src_cols &lt;= <var>2</var>) {</td></tr>
<tr><th id="106">106</th><td>      src_ptr2 = zerobuf;</td></tr>
<tr><th id="107">107</th><td>      src_inc2 = <var>0</var>;</td></tr>
<tr><th id="108">108</th><td>    }</td></tr>
<tr><th id="109">109</th><td>    <b>if</b> (remaining_src_cols &lt;= <var>3</var>) {</td></tr>
<tr><th id="110">110</th><td>      src_ptr3 = zerobuf;</td></tr>
<tr><th id="111">111</th><td>      src_inc3 = <var>0</var>;</td></tr>
<tr><th id="112">112</th><td>    }</td></tr>
<tr><th id="113">113</th><td>    <b>if</b> (remaining_src_cols &lt;= <var>4</var>) {</td></tr>
<tr><th id="114">114</th><td>      src_ptr4 = zerobuf;</td></tr>
<tr><th id="115">115</th><td>      src_inc4 = <var>0</var>;</td></tr>
<tr><th id="116">116</th><td>    }</td></tr>
<tr><th id="117">117</th><td>    <b>if</b> (remaining_src_cols &lt;= <var>5</var>) {</td></tr>
<tr><th id="118">118</th><td>      src_ptr5 = zerobuf;</td></tr>
<tr><th id="119">119</th><td>      src_inc5 = <var>0</var>;</td></tr>
<tr><th id="120">120</th><td>    }</td></tr>
<tr><th id="121">121</th><td>    <b>if</b> (remaining_src_cols &lt;= <var>6</var>) {</td></tr>
<tr><th id="122">122</th><td>      src_ptr6 = zerobuf;</td></tr>
<tr><th id="123">123</th><td>      src_inc6 = <var>0</var>;</td></tr>
<tr><th id="124">124</th><td>    }</td></tr>
<tr><th id="125">125</th><td>    src_ptr7 = zerobuf;</td></tr>
<tr><th id="126">126</th><td>    src_inc7 = <var>0</var>;</td></tr>
<tr><th id="127">127</th><td>  }</td></tr>
<tr><th id="128">128</th><td></td></tr>
<tr><th id="129">129</th><td>  <em>const</em> std::int8_t zero_point = zerobuf[<var>0</var>];</td></tr>
<tr><th id="130">130</th><td></td></tr>
<tr><th id="131">131</th><td>  <b>if</b> (sums_ptr) {</td></tr>
<tr><th id="132">132</th><td>    <i>// i: Layout::kCols.</i></td></tr>
<tr><th id="133">133</th><td>    <b>for</b> (<em>int</em> i = <var>0</var>; i &lt; <var>8</var>; ++i) {</td></tr>
<tr><th id="134">134</th><td>      sums_ptr[i] = <var>0</var>;</td></tr>
<tr><th id="135">135</th><td>    }</td></tr>
<tr><th id="136">136</th><td>  }</td></tr>
<tr><th id="137">137</th><td>  std::int32_t sums_adjustment = <var>0</var>;</td></tr>
<tr><th id="138">138</th><td>  <em>const</em> __m256i ones_16bit = _mm256_set1_epi16(<var>1</var>);</td></tr>
<tr><th id="139">139</th><td>  __m256i sums_4x2_32bit_lo = _mm256_set1_epi32(<var>0</var>);</td></tr>
<tr><th id="140">140</th><td>  __m256i sums_4x2_32bit_hi = _mm256_set1_epi32(<var>0</var>);</td></tr>
<tr><th id="141">141</th><td></td></tr>
<tr><th id="142">142</th><td>  <i>// The overall packing effectively pads the source rows to</i></td></tr>
<tr><th id="143">143</th><td><i>  // (src_rows + 63) &amp; ~63. The iteration over k may skip when m=1, and then we</i></td></tr>
<tr><th id="144">144</th><td><i>  // only pack for (src_rows + 31) &amp; ~31. When there is an incomplete</i></td></tr>
<tr><th id="145">145</th><td><i>  // destination block, this is stored into trailing_buf instead of packed_ptr.</i></td></tr>
<tr><th id="146">146</th><td>  <b>for</b> (<em>int</em> k = <var>0</var>; k &lt; src_rows; k += kNumChunkedSrcRows) {</td></tr>
<tr><th id="147">147</th><td>    <i>// Available source rows.</i></td></tr>
<tr><th id="148">148</th><td><i>    // If this is less than 0 (for m=1), we skip, having filled trailing</i></td></tr>
<tr><th id="149">149</th><td><i>    // buffer for m=0. Also, if source rows is zero on m=1, then we filled</i></td></tr>
<tr><th id="150">150</th><td><i>    // exactly to the end of the column in the packed buffer.</i></td></tr>
<tr><th id="151">151</th><td>    <em>const</em> <em>int</em> available_src_rows = src_rows - k;</td></tr>
<tr><th id="152">152</th><td>    <i>// Effectively,</i></td></tr>
<tr><th id="153">153</th><td><i>    // available rows = std::max(0, std::min(8, src_rows - k));</i></td></tr>
<tr><th id="154">154</th><td><i>    // treat each case separately.</i></td></tr>
<tr><th id="155">155</th><td>    <b>if</b> (available_src_rows &gt;= kNumChunkedSrcRows) {</td></tr>
<tr><th id="156">156</th><td>      <b>if</b> (sums_ptr) {</td></tr>
<tr><th id="157">157</th><td>        __m256i t0, t1, t2, t3, t4, t5, t6, t7;</td></tr>
<tr><th id="158">158</th><td>        __m256i r0, r1, r2, r3, r4, r5, r6, r7;</td></tr>
<tr><th id="159">159</th><td>        <em>const</em> __m256i input_xor_v = _mm256_set1_epi8(input_xor);</td></tr>
<tr><th id="160">160</th><td></td></tr>
<tr><th id="161">161</th><td>        t0 = _mm256_loadu_si256(<b>reinterpret_cast</b>&lt;<em>const</em> __m256i*&gt;(src_ptr0));</td></tr>
<tr><th id="162">162</th><td>        t4 = _mm256_loadu_si256(<b>reinterpret_cast</b>&lt;<em>const</em> __m256i*&gt;(src_ptr4));</td></tr>
<tr><th id="163">163</th><td>        t1 = _mm256_loadu_si256(<b>reinterpret_cast</b>&lt;<em>const</em> __m256i*&gt;(src_ptr1));</td></tr>
<tr><th id="164">164</th><td>        t5 = _mm256_loadu_si256(<b>reinterpret_cast</b>&lt;<em>const</em> __m256i*&gt;(src_ptr5));</td></tr>
<tr><th id="165">165</th><td>        t2 = _mm256_loadu_si256(<b>reinterpret_cast</b>&lt;<em>const</em> __m256i*&gt;(src_ptr2));</td></tr>
<tr><th id="166">166</th><td>        t6 = _mm256_loadu_si256(<b>reinterpret_cast</b>&lt;<em>const</em> __m256i*&gt;(src_ptr6));</td></tr>
<tr><th id="167">167</th><td>        t3 = _mm256_loadu_si256(<b>reinterpret_cast</b>&lt;<em>const</em> __m256i*&gt;(src_ptr3));</td></tr>
<tr><th id="168">168</th><td>        t7 = _mm256_loadu_si256(<b>reinterpret_cast</b>&lt;<em>const</em> __m256i*&gt;(src_ptr7));</td></tr>
<tr><th id="169">169</th><td></td></tr>
<tr><th id="170">170</th><td>        r0 = _mm256_unpacklo_epi32(t0, t1);</td></tr>
<tr><th id="171">171</th><td>        r4 = _mm256_unpacklo_epi32(t4, t5);</td></tr>
<tr><th id="172">172</th><td>        r2 = _mm256_unpackhi_epi32(t0, t1);</td></tr>
<tr><th id="173">173</th><td>        r6 = _mm256_unpackhi_epi32(t4, t5);</td></tr>
<tr><th id="174">174</th><td>        r1 = _mm256_unpacklo_epi32(t2, t3);</td></tr>
<tr><th id="175">175</th><td>        r5 = _mm256_unpacklo_epi32(t6, t7);</td></tr>
<tr><th id="176">176</th><td>        r3 = _mm256_unpackhi_epi32(t2, t3);</td></tr>
<tr><th id="177">177</th><td>        r7 = _mm256_unpackhi_epi32(t6, t7);</td></tr>
<tr><th id="178">178</th><td></td></tr>
<tr><th id="179">179</th><td>        t0 = _mm256_unpacklo_epi64(r0, r1);</td></tr>
<tr><th id="180">180</th><td>        t4 = _mm256_unpacklo_epi64(r4, r5);</td></tr>
<tr><th id="181">181</th><td>        t2 = _mm256_unpackhi_epi64(r0, r1);</td></tr>
<tr><th id="182">182</th><td>        t6 = _mm256_unpackhi_epi64(r4, r5);</td></tr>
<tr><th id="183">183</th><td>        t1 = _mm256_unpacklo_epi64(r2, r3);</td></tr>
<tr><th id="184">184</th><td>        t5 = _mm256_unpacklo_epi64(r6, r7);</td></tr>
<tr><th id="185">185</th><td>        t3 = _mm256_unpackhi_epi64(r2, r3);</td></tr>
<tr><th id="186">186</th><td>        t7 = _mm256_unpackhi_epi64(r6, r7);</td></tr>
<tr><th id="187">187</th><td></td></tr>
<tr><th id="188">188</th><td>        <i>// The preceding sets of rearrangement operations interleaved by 4 bytes</i></td></tr>
<tr><th id="189">189</th><td><i>        // and then by 8 bytes *within* lanes. The following set interleave by</i></td></tr>
<tr><th id="190">190</th><td><i>        // 16 bytes (128-bit), operating *between* AVX lanes. For instance (t0,</i></td></tr>
<tr><th id="191">191</th><td><i>        // t4) are interleaved to create (r0, r1). This complexity follows from</i></td></tr>
<tr><th id="192">192</th><td><i>        // the way that AVX is centered around MM 128-bit lanes.</i></td></tr>
<tr><th id="193">193</th><td>        r0 = _mm256_permute2x128_si256(t0, t4, <var>0x20</var>);</td></tr>
<tr><th id="194">194</th><td>        r4 = _mm256_permute2x128_si256(t1, t5, <var>0x20</var>);</td></tr>
<tr><th id="195">195</th><td>        r1 = _mm256_permute2x128_si256(t0, t4, <var>0x31</var>);</td></tr>
<tr><th id="196">196</th><td>        r5 = _mm256_permute2x128_si256(t1, t5, <var>0x31</var>);</td></tr>
<tr><th id="197">197</th><td>        r2 = _mm256_permute2x128_si256(t2, t6, <var>0x20</var>);</td></tr>
<tr><th id="198">198</th><td>        r6 = _mm256_permute2x128_si256(t3, t7, <var>0x20</var>);</td></tr>
<tr><th id="199">199</th><td>        r3 = _mm256_permute2x128_si256(t2, t6, <var>0x31</var>);</td></tr>
<tr><th id="200">200</th><td>        r7 = _mm256_permute2x128_si256(t3, t7, <var>0x31</var>);</td></tr>
<tr><th id="201">201</th><td></td></tr>
<tr><th id="202">202</th><td>        r0 = _mm256_xor_si256(r0, input_xor_v);</td></tr>
<tr><th id="203">203</th><td>        r1 = _mm256_xor_si256(r1, input_xor_v);</td></tr>
<tr><th id="204">204</th><td>        r2 = _mm256_xor_si256(r2, input_xor_v);</td></tr>
<tr><th id="205">205</th><td>        r3 = _mm256_xor_si256(r3, input_xor_v);</td></tr>
<tr><th id="206">206</th><td>        r4 = _mm256_xor_si256(r4, input_xor_v);</td></tr>
<tr><th id="207">207</th><td>        r5 = _mm256_xor_si256(r5, input_xor_v);</td></tr>
<tr><th id="208">208</th><td>        r6 = _mm256_xor_si256(r6, input_xor_v);</td></tr>
<tr><th id="209">209</th><td>        r7 = _mm256_xor_si256(r7, input_xor_v);</td></tr>
<tr><th id="210">210</th><td></td></tr>
<tr><th id="211">211</th><td>        __m256i sums_4x4_16bit_lo;</td></tr>
<tr><th id="212">212</th><td>        sums_4x4_16bit_lo = _mm256_cvtepi8_epi16(_mm256_castsi256_si128(r0));</td></tr>
<tr><th id="213">213</th><td>        sums_4x4_16bit_lo =</td></tr>
<tr><th id="214">214</th><td>            _mm256_add_epi16(sums_4x4_16bit_lo,</td></tr>
<tr><th id="215">215</th><td>                             _mm256_cvtepi8_epi16(_mm256_castsi256_si128(r1)));</td></tr>
<tr><th id="216">216</th><td>        sums_4x4_16bit_lo =</td></tr>
<tr><th id="217">217</th><td>            _mm256_add_epi16(sums_4x4_16bit_lo,</td></tr>
<tr><th id="218">218</th><td>                             _mm256_cvtepi8_epi16(_mm256_castsi256_si128(r2)));</td></tr>
<tr><th id="219">219</th><td>        sums_4x4_16bit_lo =</td></tr>
<tr><th id="220">220</th><td>            _mm256_add_epi16(sums_4x4_16bit_lo,</td></tr>
<tr><th id="221">221</th><td>                             _mm256_cvtepi8_epi16(_mm256_castsi256_si128(r3)));</td></tr>
<tr><th id="222">222</th><td>        sums_4x4_16bit_lo =</td></tr>
<tr><th id="223">223</th><td>            _mm256_add_epi16(sums_4x4_16bit_lo,</td></tr>
<tr><th id="224">224</th><td>                             _mm256_cvtepi8_epi16(_mm256_castsi256_si128(r4)));</td></tr>
<tr><th id="225">225</th><td>        sums_4x4_16bit_lo =</td></tr>
<tr><th id="226">226</th><td>            _mm256_add_epi16(sums_4x4_16bit_lo,</td></tr>
<tr><th id="227">227</th><td>                             _mm256_cvtepi8_epi16(_mm256_castsi256_si128(r5)));</td></tr>
<tr><th id="228">228</th><td>        sums_4x4_16bit_lo =</td></tr>
<tr><th id="229">229</th><td>            _mm256_add_epi16(sums_4x4_16bit_lo,</td></tr>
<tr><th id="230">230</th><td>                             _mm256_cvtepi8_epi16(_mm256_castsi256_si128(r6)));</td></tr>
<tr><th id="231">231</th><td>        sums_4x4_16bit_lo =</td></tr>
<tr><th id="232">232</th><td>            _mm256_add_epi16(sums_4x4_16bit_lo,</td></tr>
<tr><th id="233">233</th><td>                             _mm256_cvtepi8_epi16(_mm256_castsi256_si128(r7)));</td></tr>
<tr><th id="234">234</th><td></td></tr>
<tr><th id="235">235</th><td>        <i>// The sums have been performed across columns, and now we have 4x16-bit</i></td></tr>
<tr><th id="236">236</th><td><i>        // sums packed together. We use madd for pairwise 32-bit sums.</i></td></tr>
<tr><th id="237">237</th><td>        <em>const</em> __m256i sums_4x2_32bit_lo_new =</td></tr>
<tr><th id="238">238</th><td>            _mm256_madd_epi16(sums_4x4_16bit_lo, ones_16bit);</td></tr>
<tr><th id="239">239</th><td>        sums_4x2_32bit_lo =</td></tr>
<tr><th id="240">240</th><td>            _mm256_add_epi32(sums_4x2_32bit_lo, sums_4x2_32bit_lo_new);</td></tr>
<tr><th id="241">241</th><td></td></tr>
<tr><th id="242">242</th><td>        __m256i sums_4x4_16bit_hi;</td></tr>
<tr><th id="243">243</th><td>        sums_4x4_16bit_hi =</td></tr>
<tr><th id="244">244</th><td>            _mm256_cvtepi8_epi16(_mm256_extracti128_si256(r0, <var>1</var>));</td></tr>
<tr><th id="245">245</th><td>        sums_4x4_16bit_hi = _mm256_add_epi16(</td></tr>
<tr><th id="246">246</th><td>            sums_4x4_16bit_hi,</td></tr>
<tr><th id="247">247</th><td>            _mm256_cvtepi8_epi16(_mm256_extracti128_si256(r1, <var>1</var>)));</td></tr>
<tr><th id="248">248</th><td>        sums_4x4_16bit_hi = _mm256_add_epi16(</td></tr>
<tr><th id="249">249</th><td>            sums_4x4_16bit_hi,</td></tr>
<tr><th id="250">250</th><td>            _mm256_cvtepi8_epi16(_mm256_extracti128_si256(r2, <var>1</var>)));</td></tr>
<tr><th id="251">251</th><td>        sums_4x4_16bit_hi = _mm256_add_epi16(</td></tr>
<tr><th id="252">252</th><td>            sums_4x4_16bit_hi,</td></tr>
<tr><th id="253">253</th><td>            _mm256_cvtepi8_epi16(_mm256_extracti128_si256(r3, <var>1</var>)));</td></tr>
<tr><th id="254">254</th><td>        sums_4x4_16bit_hi = _mm256_add_epi16(</td></tr>
<tr><th id="255">255</th><td>            sums_4x4_16bit_hi,</td></tr>
<tr><th id="256">256</th><td>            _mm256_cvtepi8_epi16(_mm256_extracti128_si256(r4, <var>1</var>)));</td></tr>
<tr><th id="257">257</th><td>        sums_4x4_16bit_hi = _mm256_add_epi16(</td></tr>
<tr><th id="258">258</th><td>            sums_4x4_16bit_hi,</td></tr>
<tr><th id="259">259</th><td>            _mm256_cvtepi8_epi16(_mm256_extracti128_si256(r5, <var>1</var>)));</td></tr>
<tr><th id="260">260</th><td>        sums_4x4_16bit_hi = _mm256_add_epi16(</td></tr>
<tr><th id="261">261</th><td>            sums_4x4_16bit_hi,</td></tr>
<tr><th id="262">262</th><td>            _mm256_cvtepi8_epi16(_mm256_extracti128_si256(r6, <var>1</var>)));</td></tr>
<tr><th id="263">263</th><td>        sums_4x4_16bit_hi = _mm256_add_epi16(</td></tr>
<tr><th id="264">264</th><td>            sums_4x4_16bit_hi,</td></tr>
<tr><th id="265">265</th><td>            _mm256_cvtepi8_epi16(_mm256_extracti128_si256(r7, <var>1</var>)));</td></tr>
<tr><th id="266">266</th><td></td></tr>
<tr><th id="267">267</th><td>        <em>const</em> __m256i sums_4x2_32bit_hi_new =</td></tr>
<tr><th id="268">268</th><td>            _mm256_madd_epi16(sums_4x4_16bit_hi, ones_16bit);</td></tr>
<tr><th id="269">269</th><td>        sums_4x2_32bit_hi =</td></tr>
<tr><th id="270">270</th><td>            _mm256_add_epi32(sums_4x2_32bit_hi, sums_4x2_32bit_hi_new);</td></tr>
<tr><th id="271">271</th><td></td></tr>
<tr><th id="272">272</th><td>        _mm256_storeu_si256(<b>reinterpret_cast</b>&lt;__m256i*&gt;(packed_ptr + <var>0</var> * <var>8</var> * <var>4</var>),</td></tr>
<tr><th id="273">273</th><td>                            r0);</td></tr>
<tr><th id="274">274</th><td>        _mm256_storeu_si256(<b>reinterpret_cast</b>&lt;__m256i*&gt;(packed_ptr + <var>2</var> * <var>8</var> * <var>4</var>),</td></tr>
<tr><th id="275">275</th><td>                            r4);</td></tr>
<tr><th id="276">276</th><td>        _mm256_storeu_si256(<b>reinterpret_cast</b>&lt;__m256i*&gt;(packed_ptr + <var>4</var> * <var>8</var> * <var>4</var>),</td></tr>
<tr><th id="277">277</th><td>                            r1);</td></tr>
<tr><th id="278">278</th><td>        _mm256_storeu_si256(<b>reinterpret_cast</b>&lt;__m256i*&gt;(packed_ptr + <var>6</var> * <var>8</var> * <var>4</var>),</td></tr>
<tr><th id="279">279</th><td>                            r5);</td></tr>
<tr><th id="280">280</th><td>        _mm256_storeu_si256(<b>reinterpret_cast</b>&lt;__m256i*&gt;(packed_ptr + <var>1</var> * <var>8</var> * <var>4</var>),</td></tr>
<tr><th id="281">281</th><td>                            r2);</td></tr>
<tr><th id="282">282</th><td>        _mm256_storeu_si256(<b>reinterpret_cast</b>&lt;__m256i*&gt;(packed_ptr + <var>3</var> * <var>8</var> * <var>4</var>),</td></tr>
<tr><th id="283">283</th><td>                            r6);</td></tr>
<tr><th id="284">284</th><td>        _mm256_storeu_si256(<b>reinterpret_cast</b>&lt;__m256i*&gt;(packed_ptr + <var>5</var> * <var>8</var> * <var>4</var>),</td></tr>
<tr><th id="285">285</th><td>                            r3);</td></tr>
<tr><th id="286">286</th><td>        _mm256_storeu_si256(<b>reinterpret_cast</b>&lt;__m256i*&gt;(packed_ptr + <var>7</var> * <var>8</var> * <var>4</var>),</td></tr>
<tr><th id="287">287</th><td>                            r7);</td></tr>
<tr><th id="288">288</th><td>      } <b>else</b> {</td></tr>
<tr><th id="289">289</th><td>        __m256i t0, t1, t2, t3, t4, t5, t6, t7;</td></tr>
<tr><th id="290">290</th><td>        __m256i r0, r1, r2, r3, r4, r5, r6, r7;</td></tr>
<tr><th id="291">291</th><td>        <em>const</em> __m256i input_xor_v = _mm256_set1_epi8(input_xor);</td></tr>
<tr><th id="292">292</th><td></td></tr>
<tr><th id="293">293</th><td>        t0 = _mm256_loadu_si256(<b>reinterpret_cast</b>&lt;<em>const</em> __m256i*&gt;(src_ptr0));</td></tr>
<tr><th id="294">294</th><td>        t4 = _mm256_loadu_si256(<b>reinterpret_cast</b>&lt;<em>const</em> __m256i*&gt;(src_ptr4));</td></tr>
<tr><th id="295">295</th><td>        t1 = _mm256_loadu_si256(<b>reinterpret_cast</b>&lt;<em>const</em> __m256i*&gt;(src_ptr1));</td></tr>
<tr><th id="296">296</th><td>        t5 = _mm256_loadu_si256(<b>reinterpret_cast</b>&lt;<em>const</em> __m256i*&gt;(src_ptr5));</td></tr>
<tr><th id="297">297</th><td>        t2 = _mm256_loadu_si256(<b>reinterpret_cast</b>&lt;<em>const</em> __m256i*&gt;(src_ptr2));</td></tr>
<tr><th id="298">298</th><td>        t6 = _mm256_loadu_si256(<b>reinterpret_cast</b>&lt;<em>const</em> __m256i*&gt;(src_ptr6));</td></tr>
<tr><th id="299">299</th><td>        t3 = _mm256_loadu_si256(<b>reinterpret_cast</b>&lt;<em>const</em> __m256i*&gt;(src_ptr3));</td></tr>
<tr><th id="300">300</th><td>        t7 = _mm256_loadu_si256(<b>reinterpret_cast</b>&lt;<em>const</em> __m256i*&gt;(src_ptr7));</td></tr>
<tr><th id="301">301</th><td></td></tr>
<tr><th id="302">302</th><td>        r0 = _mm256_unpacklo_epi32(t0, t1);</td></tr>
<tr><th id="303">303</th><td>        r4 = _mm256_unpacklo_epi32(t4, t5);</td></tr>
<tr><th id="304">304</th><td>        r2 = _mm256_unpackhi_epi32(t0, t1);</td></tr>
<tr><th id="305">305</th><td>        r6 = _mm256_unpackhi_epi32(t4, t5);</td></tr>
<tr><th id="306">306</th><td>        r1 = _mm256_unpacklo_epi32(t2, t3);</td></tr>
<tr><th id="307">307</th><td>        r5 = _mm256_unpacklo_epi32(t6, t7);</td></tr>
<tr><th id="308">308</th><td>        r3 = _mm256_unpackhi_epi32(t2, t3);</td></tr>
<tr><th id="309">309</th><td>        r7 = _mm256_unpackhi_epi32(t6, t7);</td></tr>
<tr><th id="310">310</th><td></td></tr>
<tr><th id="311">311</th><td>        t0 = _mm256_unpacklo_epi64(r0, r1);</td></tr>
<tr><th id="312">312</th><td>        t4 = _mm256_unpacklo_epi64(r4, r5);</td></tr>
<tr><th id="313">313</th><td>        t2 = _mm256_unpackhi_epi64(r0, r1);</td></tr>
<tr><th id="314">314</th><td>        t6 = _mm256_unpackhi_epi64(r4, r5);</td></tr>
<tr><th id="315">315</th><td>        t1 = _mm256_unpacklo_epi64(r2, r3);</td></tr>
<tr><th id="316">316</th><td>        t5 = _mm256_unpacklo_epi64(r6, r7);</td></tr>
<tr><th id="317">317</th><td>        t3 = _mm256_unpackhi_epi64(r2, r3);</td></tr>
<tr><th id="318">318</th><td>        t7 = _mm256_unpackhi_epi64(r6, r7);</td></tr>
<tr><th id="319">319</th><td></td></tr>
<tr><th id="320">320</th><td>        <i>// The preceding sets of rearrangement operations interleaved by 4 bytes</i></td></tr>
<tr><th id="321">321</th><td><i>        // and then by 8 bytes *within* lanes. The following set interleave by</i></td></tr>
<tr><th id="322">322</th><td><i>        // 16 bytes (128-bit), operating *between* AVX lanes. For instance (t0,</i></td></tr>
<tr><th id="323">323</th><td><i>        // t4) are interleaved to create (r0, r1). This complexity follows from</i></td></tr>
<tr><th id="324">324</th><td><i>        // the way that AVX is centered around MM 128-bit lanes.</i></td></tr>
<tr><th id="325">325</th><td>        r0 = _mm256_permute2x128_si256(t0, t4, <var>0x20</var>);</td></tr>
<tr><th id="326">326</th><td>        r4 = _mm256_permute2x128_si256(t1, t5, <var>0x20</var>);</td></tr>
<tr><th id="327">327</th><td>        r1 = _mm256_permute2x128_si256(t0, t4, <var>0x31</var>);</td></tr>
<tr><th id="328">328</th><td>        r5 = _mm256_permute2x128_si256(t1, t5, <var>0x31</var>);</td></tr>
<tr><th id="329">329</th><td>        r2 = _mm256_permute2x128_si256(t2, t6, <var>0x20</var>);</td></tr>
<tr><th id="330">330</th><td>        r6 = _mm256_permute2x128_si256(t3, t7, <var>0x20</var>);</td></tr>
<tr><th id="331">331</th><td>        r3 = _mm256_permute2x128_si256(t2, t6, <var>0x31</var>);</td></tr>
<tr><th id="332">332</th><td>        r7 = _mm256_permute2x128_si256(t3, t7, <var>0x31</var>);</td></tr>
<tr><th id="333">333</th><td></td></tr>
<tr><th id="334">334</th><td>        r0 = _mm256_xor_si256(r0, input_xor_v);</td></tr>
<tr><th id="335">335</th><td>        r1 = _mm256_xor_si256(r1, input_xor_v);</td></tr>
<tr><th id="336">336</th><td>        r2 = _mm256_xor_si256(r2, input_xor_v);</td></tr>
<tr><th id="337">337</th><td>        r3 = _mm256_xor_si256(r3, input_xor_v);</td></tr>
<tr><th id="338">338</th><td>        r4 = _mm256_xor_si256(r4, input_xor_v);</td></tr>
<tr><th id="339">339</th><td>        r5 = _mm256_xor_si256(r5, input_xor_v);</td></tr>
<tr><th id="340">340</th><td>        r6 = _mm256_xor_si256(r6, input_xor_v);</td></tr>
<tr><th id="341">341</th><td>        r7 = _mm256_xor_si256(r7, input_xor_v);</td></tr>
<tr><th id="342">342</th><td></td></tr>
<tr><th id="343">343</th><td>        _mm256_storeu_si256(<b>reinterpret_cast</b>&lt;__m256i*&gt;(packed_ptr + <var>0</var> * <var>8</var> * <var>4</var>),</td></tr>
<tr><th id="344">344</th><td>                            r0);</td></tr>
<tr><th id="345">345</th><td>        _mm256_storeu_si256(<b>reinterpret_cast</b>&lt;__m256i*&gt;(packed_ptr + <var>2</var> * <var>8</var> * <var>4</var>),</td></tr>
<tr><th id="346">346</th><td>                            r4);</td></tr>
<tr><th id="347">347</th><td>        _mm256_storeu_si256(<b>reinterpret_cast</b>&lt;__m256i*&gt;(packed_ptr + <var>4</var> * <var>8</var> * <var>4</var>),</td></tr>
<tr><th id="348">348</th><td>                            r1);</td></tr>
<tr><th id="349">349</th><td>        _mm256_storeu_si256(<b>reinterpret_cast</b>&lt;__m256i*&gt;(packed_ptr + <var>6</var> * <var>8</var> * <var>4</var>),</td></tr>
<tr><th id="350">350</th><td>                            r5);</td></tr>
<tr><th id="351">351</th><td>        _mm256_storeu_si256(<b>reinterpret_cast</b>&lt;__m256i*&gt;(packed_ptr + <var>1</var> * <var>8</var> * <var>4</var>),</td></tr>
<tr><th id="352">352</th><td>                            r2);</td></tr>
<tr><th id="353">353</th><td>        _mm256_storeu_si256(<b>reinterpret_cast</b>&lt;__m256i*&gt;(packed_ptr + <var>3</var> * <var>8</var> * <var>4</var>),</td></tr>
<tr><th id="354">354</th><td>                            r6);</td></tr>
<tr><th id="355">355</th><td>        _mm256_storeu_si256(<b>reinterpret_cast</b>&lt;__m256i*&gt;(packed_ptr + <var>5</var> * <var>8</var> * <var>4</var>),</td></tr>
<tr><th id="356">356</th><td>                            r3);</td></tr>
<tr><th id="357">357</th><td>        _mm256_storeu_si256(<b>reinterpret_cast</b>&lt;__m256i*&gt;(packed_ptr + <var>7</var> * <var>8</var> * <var>4</var>),</td></tr>
<tr><th id="358">358</th><td>                            r7);</td></tr>
<tr><th id="359">359</th><td>      }</td></tr>
<tr><th id="360">360</th><td>    } <b>else</b> <b>if</b> (available_src_rows &gt; <var>0</var>) {</td></tr>
<tr><th id="361">361</th><td>      RUY_DCHECK_LT(available_src_rows, kNumChunkedSrcRows);</td></tr>
<tr><th id="362">362</th><td>      <i>// We do not care what goes into the trailing buffer, but we want</i></td></tr>
<tr><th id="363">363</th><td><i>      // in_data[...] ^ input_xor == 0 for irrelevant values in the summation.</i></td></tr>
<tr><th id="364">364</th><td><i>      //</i></td></tr>
<tr><th id="365">365</th><td><i>      // We compensate for padding-with-zero_point by initializing the</i></td></tr>
<tr><th id="366">366</th><td><i>      // summations with the compensating offset, effectively</i></td></tr>
<tr><th id="367">367</th><td><i>      // ((input_xor ^ input_xor) - (zero_point ^ input_xor)) *</i></td></tr>
<tr><th id="368">368</th><td><i>      //                         4 * (8 - ((available_src_rows + 3) &gt;&gt; 2)).</i></td></tr>
<tr><th id="369">369</th><td><i>      //</i></td></tr>
<tr><th id="370">370</th><td><i>      // Note that (zero_point ^ input_xor) is performed in 8-bits and then</i></td></tr>
<tr><th id="371">371</th><td><i>      // cast.</i></td></tr>
<tr><th id="372">372</th><td>      sums_adjustment +=</td></tr>
<tr><th id="373">373</th><td>          -(zero_point ^ input_xor) * <var>4</var> * (<var>8</var> - ((available_src_rows + <var>3</var>) &gt;&gt; <var>2</var>));</td></tr>
<tr><th id="374">374</th><td></td></tr>
<tr><th id="375">375</th><td>      __m256i t0, t1, t2, t3, t4, t5, t6, t7;</td></tr>
<tr><th id="376">376</th><td>      __m256i r0, r1, r2, r3, r4, r5, r6, r7;</td></tr>
<tr><th id="377">377</th><td>      <em>const</em> __m256i input_xor_v = _mm256_set1_epi8(input_xor);</td></tr>
<tr><th id="378">378</th><td></td></tr>
<tr><th id="379">379</th><td>      t0 = MaskLoadu&lt;Path::kAvx2Fma&gt;(available_src_rows, zero_point, src_ptr0);</td></tr>
<tr><th id="380">380</th><td>      t4 = MaskLoadu&lt;Path::kAvx2Fma&gt;(available_src_rows, zero_point, src_ptr4);</td></tr>
<tr><th id="381">381</th><td>      t1 = MaskLoadu&lt;Path::kAvx2Fma&gt;(available_src_rows, zero_point, src_ptr1);</td></tr>
<tr><th id="382">382</th><td>      t5 = MaskLoadu&lt;Path::kAvx2Fma&gt;(available_src_rows, zero_point, src_ptr5);</td></tr>
<tr><th id="383">383</th><td>      t2 = MaskLoadu&lt;Path::kAvx2Fma&gt;(available_src_rows, zero_point, src_ptr2);</td></tr>
<tr><th id="384">384</th><td>      t6 = MaskLoadu&lt;Path::kAvx2Fma&gt;(available_src_rows, zero_point, src_ptr6);</td></tr>
<tr><th id="385">385</th><td>      t3 = MaskLoadu&lt;Path::kAvx2Fma&gt;(available_src_rows, zero_point, src_ptr3);</td></tr>
<tr><th id="386">386</th><td>      t7 = MaskLoadu&lt;Path::kAvx2Fma&gt;(available_src_rows, zero_point, src_ptr7);</td></tr>
<tr><th id="387">387</th><td></td></tr>
<tr><th id="388">388</th><td>      r0 = _mm256_unpacklo_epi32(t0, t1);</td></tr>
<tr><th id="389">389</th><td>      r4 = _mm256_unpacklo_epi32(t4, t5);</td></tr>
<tr><th id="390">390</th><td>      r2 = _mm256_unpackhi_epi32(t0, t1);</td></tr>
<tr><th id="391">391</th><td>      r6 = _mm256_unpackhi_epi32(t4, t5);</td></tr>
<tr><th id="392">392</th><td>      r1 = _mm256_unpacklo_epi32(t2, t3);</td></tr>
<tr><th id="393">393</th><td>      r5 = _mm256_unpacklo_epi32(t6, t7);</td></tr>
<tr><th id="394">394</th><td>      r3 = _mm256_unpackhi_epi32(t2, t3);</td></tr>
<tr><th id="395">395</th><td>      r7 = _mm256_unpackhi_epi32(t6, t7);</td></tr>
<tr><th id="396">396</th><td></td></tr>
<tr><th id="397">397</th><td>      t0 = _mm256_unpacklo_epi64(r0, r1);</td></tr>
<tr><th id="398">398</th><td>      t4 = _mm256_unpacklo_epi64(r4, r5);</td></tr>
<tr><th id="399">399</th><td>      t2 = _mm256_unpackhi_epi64(r0, r1);</td></tr>
<tr><th id="400">400</th><td>      t6 = _mm256_unpackhi_epi64(r4, r5);</td></tr>
<tr><th id="401">401</th><td>      t1 = _mm256_unpacklo_epi64(r2, r3);</td></tr>
<tr><th id="402">402</th><td>      t5 = _mm256_unpacklo_epi64(r6, r7);</td></tr>
<tr><th id="403">403</th><td>      t3 = _mm256_unpackhi_epi64(r2, r3);</td></tr>
<tr><th id="404">404</th><td>      t7 = _mm256_unpackhi_epi64(r6, r7);</td></tr>
<tr><th id="405">405</th><td></td></tr>
<tr><th id="406">406</th><td>      <i>// The preceding sets of rearrangement operations interleaved by 4 bytes</i></td></tr>
<tr><th id="407">407</th><td><i>      // and then by 8 bytes *within* lanes. The following set interleave by</i></td></tr>
<tr><th id="408">408</th><td><i>      // 16 bytes (128-bit), operating *between* AVX lanes. For instance (t0,</i></td></tr>
<tr><th id="409">409</th><td><i>      // t4) are interleaved to create (r0, r1). This complexity follows from</i></td></tr>
<tr><th id="410">410</th><td><i>      // the way that AVX is centered around MM 128-bit lanes.</i></td></tr>
<tr><th id="411">411</th><td>      r0 = _mm256_permute2x128_si256(t0, t4, <var>0x20</var>);</td></tr>
<tr><th id="412">412</th><td>      r4 = _mm256_permute2x128_si256(t1, t5, <var>0x20</var>);</td></tr>
<tr><th id="413">413</th><td>      r1 = _mm256_permute2x128_si256(t0, t4, <var>0x31</var>);</td></tr>
<tr><th id="414">414</th><td>      r5 = _mm256_permute2x128_si256(t1, t5, <var>0x31</var>);</td></tr>
<tr><th id="415">415</th><td>      r2 = _mm256_permute2x128_si256(t2, t6, <var>0x20</var>);</td></tr>
<tr><th id="416">416</th><td>      r6 = _mm256_permute2x128_si256(t3, t7, <var>0x20</var>);</td></tr>
<tr><th id="417">417</th><td>      r3 = _mm256_permute2x128_si256(t2, t6, <var>0x31</var>);</td></tr>
<tr><th id="418">418</th><td>      r7 = _mm256_permute2x128_si256(t3, t7, <var>0x31</var>);</td></tr>
<tr><th id="419">419</th><td></td></tr>
<tr><th id="420">420</th><td>      r0 = _mm256_xor_si256(r0, input_xor_v);</td></tr>
<tr><th id="421">421</th><td>      r1 = _mm256_xor_si256(r1, input_xor_v);</td></tr>
<tr><th id="422">422</th><td>      r2 = _mm256_xor_si256(r2, input_xor_v);</td></tr>
<tr><th id="423">423</th><td>      r3 = _mm256_xor_si256(r3, input_xor_v);</td></tr>
<tr><th id="424">424</th><td>      r4 = _mm256_xor_si256(r4, input_xor_v);</td></tr>
<tr><th id="425">425</th><td>      r5 = _mm256_xor_si256(r5, input_xor_v);</td></tr>
<tr><th id="426">426</th><td>      r6 = _mm256_xor_si256(r6, input_xor_v);</td></tr>
<tr><th id="427">427</th><td>      r7 = _mm256_xor_si256(r7, input_xor_v);</td></tr>
<tr><th id="428">428</th><td></td></tr>
<tr><th id="429">429</th><td>      __m256i sums_4x4_16bit_lo;</td></tr>
<tr><th id="430">430</th><td>      sums_4x4_16bit_lo = _mm256_cvtepi8_epi16(_mm256_castsi256_si128(r0));</td></tr>
<tr><th id="431">431</th><td>      sums_4x4_16bit_lo = _mm256_add_epi16(</td></tr>
<tr><th id="432">432</th><td>          sums_4x4_16bit_lo, _mm256_cvtepi8_epi16(_mm256_castsi256_si128(r1)));</td></tr>
<tr><th id="433">433</th><td>      sums_4x4_16bit_lo = _mm256_add_epi16(</td></tr>
<tr><th id="434">434</th><td>          sums_4x4_16bit_lo, _mm256_cvtepi8_epi16(_mm256_castsi256_si128(r2)));</td></tr>
<tr><th id="435">435</th><td>      sums_4x4_16bit_lo = _mm256_add_epi16(</td></tr>
<tr><th id="436">436</th><td>          sums_4x4_16bit_lo, _mm256_cvtepi8_epi16(_mm256_castsi256_si128(r3)));</td></tr>
<tr><th id="437">437</th><td>      sums_4x4_16bit_lo = _mm256_add_epi16(</td></tr>
<tr><th id="438">438</th><td>          sums_4x4_16bit_lo, _mm256_cvtepi8_epi16(_mm256_castsi256_si128(r4)));</td></tr>
<tr><th id="439">439</th><td>      sums_4x4_16bit_lo = _mm256_add_epi16(</td></tr>
<tr><th id="440">440</th><td>          sums_4x4_16bit_lo, _mm256_cvtepi8_epi16(_mm256_castsi256_si128(r5)));</td></tr>
<tr><th id="441">441</th><td>      sums_4x4_16bit_lo = _mm256_add_epi16(</td></tr>
<tr><th id="442">442</th><td>          sums_4x4_16bit_lo, _mm256_cvtepi8_epi16(_mm256_castsi256_si128(r6)));</td></tr>
<tr><th id="443">443</th><td>      sums_4x4_16bit_lo = _mm256_add_epi16(</td></tr>
<tr><th id="444">444</th><td>          sums_4x4_16bit_lo, _mm256_cvtepi8_epi16(_mm256_castsi256_si128(r7)));</td></tr>
<tr><th id="445">445</th><td></td></tr>
<tr><th id="446">446</th><td>      <i>// The sums have been performed across columns, and now we have 4x16-bit</i></td></tr>
<tr><th id="447">447</th><td><i>      // sums packed together. We use madd for pairwise 32-bit sums.</i></td></tr>
<tr><th id="448">448</th><td>      <em>const</em> __m256i sums_4x2_32bit_lo_new =</td></tr>
<tr><th id="449">449</th><td>          _mm256_madd_epi16(sums_4x4_16bit_lo, ones_16bit);</td></tr>
<tr><th id="450">450</th><td>      sums_4x2_32bit_lo =</td></tr>
<tr><th id="451">451</th><td>          _mm256_add_epi32(sums_4x2_32bit_lo, sums_4x2_32bit_lo_new);</td></tr>
<tr><th id="452">452</th><td></td></tr>
<tr><th id="453">453</th><td>      __m256i sums_4x4_16bit_hi;</td></tr>
<tr><th id="454">454</th><td>      sums_4x4_16bit_hi = _mm256_cvtepi8_epi16(_mm256_extracti128_si256(r0, <var>1</var>));</td></tr>
<tr><th id="455">455</th><td>      sums_4x4_16bit_hi = _mm256_add_epi16(</td></tr>
<tr><th id="456">456</th><td>          sums_4x4_16bit_hi,</td></tr>
<tr><th id="457">457</th><td>          _mm256_cvtepi8_epi16(_mm256_extracti128_si256(r1, <var>1</var>)));</td></tr>
<tr><th id="458">458</th><td>      sums_4x4_16bit_hi = _mm256_add_epi16(</td></tr>
<tr><th id="459">459</th><td>          sums_4x4_16bit_hi,</td></tr>
<tr><th id="460">460</th><td>          _mm256_cvtepi8_epi16(_mm256_extracti128_si256(r2, <var>1</var>)));</td></tr>
<tr><th id="461">461</th><td>      sums_4x4_16bit_hi = _mm256_add_epi16(</td></tr>
<tr><th id="462">462</th><td>          sums_4x4_16bit_hi,</td></tr>
<tr><th id="463">463</th><td>          _mm256_cvtepi8_epi16(_mm256_extracti128_si256(r3, <var>1</var>)));</td></tr>
<tr><th id="464">464</th><td>      sums_4x4_16bit_hi = _mm256_add_epi16(</td></tr>
<tr><th id="465">465</th><td>          sums_4x4_16bit_hi,</td></tr>
<tr><th id="466">466</th><td>          _mm256_cvtepi8_epi16(_mm256_extracti128_si256(r4, <var>1</var>)));</td></tr>
<tr><th id="467">467</th><td>      sums_4x4_16bit_hi = _mm256_add_epi16(</td></tr>
<tr><th id="468">468</th><td>          sums_4x4_16bit_hi,</td></tr>
<tr><th id="469">469</th><td>          _mm256_cvtepi8_epi16(_mm256_extracti128_si256(r5, <var>1</var>)));</td></tr>
<tr><th id="470">470</th><td>      sums_4x4_16bit_hi = _mm256_add_epi16(</td></tr>
<tr><th id="471">471</th><td>          sums_4x4_16bit_hi,</td></tr>
<tr><th id="472">472</th><td>          _mm256_cvtepi8_epi16(_mm256_extracti128_si256(r6, <var>1</var>)));</td></tr>
<tr><th id="473">473</th><td>      sums_4x4_16bit_hi = _mm256_add_epi16(</td></tr>
<tr><th id="474">474</th><td>          sums_4x4_16bit_hi,</td></tr>
<tr><th id="475">475</th><td>          _mm256_cvtepi8_epi16(_mm256_extracti128_si256(r7, <var>1</var>)));</td></tr>
<tr><th id="476">476</th><td></td></tr>
<tr><th id="477">477</th><td>      <em>const</em> __m256i sums_4x2_32bit_hi_new =</td></tr>
<tr><th id="478">478</th><td>          _mm256_madd_epi16(sums_4x4_16bit_hi, ones_16bit);</td></tr>
<tr><th id="479">479</th><td>      sums_4x2_32bit_hi =</td></tr>
<tr><th id="480">480</th><td>          _mm256_add_epi32(sums_4x2_32bit_hi, sums_4x2_32bit_hi_new);</td></tr>
<tr><th id="481">481</th><td></td></tr>
<tr><th id="482">482</th><td>      _mm256_storeu_si256(<b>reinterpret_cast</b>&lt;__m256i*&gt;(trailing_buf + <var>0</var> * <var>8</var> * <var>4</var>),</td></tr>
<tr><th id="483">483</th><td>                          r0);</td></tr>
<tr><th id="484">484</th><td>      _mm256_storeu_si256(<b>reinterpret_cast</b>&lt;__m256i*&gt;(trailing_buf + <var>2</var> * <var>8</var> * <var>4</var>),</td></tr>
<tr><th id="485">485</th><td>                          r4);</td></tr>
<tr><th id="486">486</th><td>      _mm256_storeu_si256(<b>reinterpret_cast</b>&lt;__m256i*&gt;(trailing_buf + <var>4</var> * <var>8</var> * <var>4</var>),</td></tr>
<tr><th id="487">487</th><td>                          r1);</td></tr>
<tr><th id="488">488</th><td>      _mm256_storeu_si256(<b>reinterpret_cast</b>&lt;__m256i*&gt;(trailing_buf + <var>6</var> * <var>8</var> * <var>4</var>),</td></tr>
<tr><th id="489">489</th><td>                          r5);</td></tr>
<tr><th id="490">490</th><td>      _mm256_storeu_si256(<b>reinterpret_cast</b>&lt;__m256i*&gt;(trailing_buf + <var>1</var> * <var>8</var> * <var>4</var>),</td></tr>
<tr><th id="491">491</th><td>                          r2);</td></tr>
<tr><th id="492">492</th><td>      _mm256_storeu_si256(<b>reinterpret_cast</b>&lt;__m256i*&gt;(trailing_buf + <var>3</var> * <var>8</var> * <var>4</var>),</td></tr>
<tr><th id="493">493</th><td>                          r6);</td></tr>
<tr><th id="494">494</th><td>      _mm256_storeu_si256(<b>reinterpret_cast</b>&lt;__m256i*&gt;(trailing_buf + <var>5</var> * <var>8</var> * <var>4</var>),</td></tr>
<tr><th id="495">495</th><td>                          r3);</td></tr>
<tr><th id="496">496</th><td>      _mm256_storeu_si256(<b>reinterpret_cast</b>&lt;__m256i*&gt;(trailing_buf + <var>7</var> * <var>8</var> * <var>4</var>),</td></tr>
<tr><th id="497">497</th><td>                          r7);</td></tr>
<tr><th id="498">498</th><td>    }</td></tr>
<tr><th id="499">499</th><td></td></tr>
<tr><th id="500">500</th><td>    packed_ptr += <var>8</var> * kNumChunkedSrcRows;</td></tr>
<tr><th id="501">501</th><td>    src_ptr0 += src_inc0;</td></tr>
<tr><th id="502">502</th><td>    src_ptr1 += src_inc1;</td></tr>
<tr><th id="503">503</th><td>    src_ptr2 += src_inc2;</td></tr>
<tr><th id="504">504</th><td>    src_ptr3 += src_inc3;</td></tr>
<tr><th id="505">505</th><td>    src_ptr4 += src_inc4;</td></tr>
<tr><th id="506">506</th><td>    src_ptr5 += src_inc5;</td></tr>
<tr><th id="507">507</th><td>    src_ptr6 += src_inc6;</td></tr>
<tr><th id="508">508</th><td>    src_ptr7 += src_inc7;</td></tr>
<tr><th id="509">509</th><td>  }</td></tr>
<tr><th id="510">510</th><td></td></tr>
<tr><th id="511">511</th><td>  <b>if</b> (sums_ptr) {</td></tr>
<tr><th id="512">512</th><td>    <em>const</em> __m256i sums_adjustment_v = _mm256_set1_epi32(sums_adjustment);</td></tr>
<tr><th id="513">513</th><td></td></tr>
<tr><th id="514">514</th><td>    __m256i sums =</td></tr>
<tr><th id="515">515</th><td>        _mm256_loadu_si256(<b>reinterpret_cast</b>&lt;<em>const</em> __m256i*&gt;(sums_ptr));</td></tr>
<tr><th id="516">516</th><td>    <em>const</em> __m256i idx = _mm256_set_epi32(<var>7</var>, <var>5</var>, <var>3</var>, <var>1</var>, <var>6</var>, <var>4</var>, <var>2</var>, <var>0</var>);</td></tr>
<tr><th id="517">517</th><td></td></tr>
<tr><th id="518">518</th><td>    <i>// We earlier used madd for pairwise 32-bit sums, and now we deinterlace the</i></td></tr>
<tr><th id="519">519</th><td><i>    // neighbours, finshing up by adding them to the stored accumulated sums.</i></td></tr>
<tr><th id="520">520</th><td>    <em>const</em> __m256i sums_2x4_32bit_lo =</td></tr>
<tr><th id="521">521</th><td>        _mm256_permutevar8x32_epi32(sums_4x2_32bit_lo, idx);</td></tr>
<tr><th id="522">522</th><td>    <em>const</em> __m256i sums_2x4_32bit_hi =</td></tr>
<tr><th id="523">523</th><td>        _mm256_permutevar8x32_epi32(sums_4x2_32bit_hi, idx);</td></tr>
<tr><th id="524">524</th><td>    <em>const</em> __m256i sums_2x4_32bit_a =</td></tr>
<tr><th id="525">525</th><td>        _mm256_permute2x128_si256(sums_2x4_32bit_lo, sums_2x4_32bit_hi, <var>0x20</var>);</td></tr>
<tr><th id="526">526</th><td>    <em>const</em> __m256i sums_2x4_32bit_b =</td></tr>
<tr><th id="527">527</th><td>        _mm256_permute2x128_si256(sums_2x4_32bit_lo, sums_2x4_32bit_hi, <var>0x31</var>);</td></tr>
<tr><th id="528">528</th><td>    sums = _mm256_add_epi32(sums, sums_adjustment_v);</td></tr>
<tr><th id="529">529</th><td>    sums = _mm256_add_epi32(sums, sums_2x4_32bit_a);</td></tr>
<tr><th id="530">530</th><td>    sums = _mm256_add_epi32(sums, sums_2x4_32bit_b);</td></tr>
<tr><th id="531">531</th><td></td></tr>
<tr><th id="532">532</th><td>    _mm256_storeu_si256(<b>reinterpret_cast</b>&lt;__m256i*&gt;(sums_ptr), sums);</td></tr>
<tr><th id="533">533</th><td>  }</td></tr>
<tr><th id="534">534</th><td>}</td></tr>
<tr><th id="535">535</th><td></td></tr>
<tr><th id="536">536</th><td><i>// Use AVX2 specific intrinsic for greater than comparison.</i></td></tr>
<tr><th id="537">537</th><td><b>template</b> &lt;&gt;</td></tr>
<tr><th id="538">538</th><td><b>inline</b> __m256i CompareGreaterThan&lt;Path::kAvx2Fma&gt;(<em>const</em> __m256i&amp; a,</td></tr>
<tr><th id="539">539</th><td>                                                  <em>const</em> __m256i&amp; b) {</td></tr>
<tr><th id="540">540</th><td>  <b>return</b> _mm256_cmpgt_epi32(a, b);</td></tr>
<tr><th id="541">541</th><td>}</td></tr>
<tr><th id="542">542</th><td></td></tr>
<tr><th id="543">543</th><td>}  <i>// namespace.</i></td></tr>
<tr><th id="544">544</th><td></td></tr>
<tr><th id="545">545</th><td><em>void</em> Pack8bitColMajorForAvx2(<em>const</em> std::int8_t* src_ptr, std::int8_t input_xor,</td></tr>
<tr><th id="546">546</th><td>                             <em>const</em> std::int8_t* zerobuf, <em>int</em> src_stride,</td></tr>
<tr><th id="547">547</th><td>                             <em>int</em> remaining_src_cols, <em>int</em> src_rows,</td></tr>
<tr><th id="548">548</th><td>                             std::int8_t* packed_ptr, std::int32_t* sums_ptr) {</td></tr>
<tr><th id="549">549</th><td>  profiler::ScopeLabel label(<q>"Pack kAvx2Fma 8bit"</q>);</td></tr>
<tr><th id="550">550</th><td></td></tr>
<tr><th id="551">551</th><td>  <b>using</b> Layout = PackImpl8bitAvx2::Layout;</td></tr>
<tr><th id="552">552</th><td>  RUY_DCHECK_EQ(Layout::kCols, <var>8</var>);</td></tr>
<tr><th id="553">553</th><td>  RUY_DCHECK_EQ(Layout::kRows, <var>4</var>);</td></tr>
<tr><th id="554">554</th><td></td></tr>
<tr><th id="555">555</th><td>  <i>// Each Layout::Rows is 4 contiguous input, contiguous packed elements.</i></td></tr>
<tr><th id="556">556</th><td><i>  // We process 8 of these chunks at a time, padding short input chunks.</i></td></tr>
<tr><th id="557">557</th><td>  <em>static</em> <b>constexpr</b> <em>int</em> kNumRowChunks = <var>8</var>;  <i>// Short input is padded.</i></td></tr>
<tr><th id="558">558</th><td></td></tr>
<tr><th id="559">559</th><td>  <i>// Each packed block is 4*8, and there are normally 8. The trailing block is</i></td></tr>
<tr><th id="560">560</th><td><i>  // only slightly shorter.</i></td></tr>
<tr><th id="561">561</th><td>  <b>constexpr</b> <em>int</em> kTrailingBufSize =</td></tr>
<tr><th id="562">562</th><td>      kNumRowChunks * Layout::kCols * Layout::kRows;</td></tr>
<tr><th id="563">563</th><td>  std::int8_t trailing_buf[kTrailingBufSize];</td></tr>
<tr><th id="564">564</th><td>  memset(trailing_buf, <var>0</var>, kTrailingBufSize * <b>sizeof</b>(std::int8_t));</td></tr>
<tr><th id="565">565</th><td></td></tr>
<tr><th id="566">566</th><td>  Pack8bitColMajorForAvx2Packer(src_ptr, input_xor, zerobuf, src_stride,</td></tr>
<tr><th id="567">567</th><td>                                remaining_src_cols, src_rows, packed_ptr,</td></tr>
<tr><th id="568">568</th><td>                                sums_ptr, trailing_buf);</td></tr>
<tr><th id="569">569</th><td></td></tr>
<tr><th id="570">570</th><td>  <b>constexpr</b> <em>int</em> kChunkedRowMask = kNumRowChunks * Layout::kRows - <var>1</var>;</td></tr>
<tr><th id="571">571</th><td>  <em>const</em> <em>bool</em> trailing_data = (src_rows &amp; kChunkedRowMask) &gt; <var>0</var>;</td></tr>
<tr><th id="572">572</th><td>  <i>// If the number of source rows is not a multiple of kChunkedRowMask, there</i></td></tr>
<tr><th id="573">573</th><td><i>  // will be data in the trailing buffer,</i></td></tr>
<tr><th id="574">574</th><td>  <b>if</b> (trailing_data) {</td></tr>
<tr><th id="575">575</th><td>    <em>const</em> <em>int</em> non_trailing_rows = src_rows &amp; ~kChunkedRowMask;</td></tr>
<tr><th id="576">576</th><td>    <i>// Destination "rows" are padded to next highest multiple of Layout::kRows.</i></td></tr>
<tr><th id="577">577</th><td>    <em>const</em> <em>int</em> dst_rows = (src_rows + <var>3</var>) &amp; ~<var>3</var>;</td></tr>
<tr><th id="578">578</th><td>    <em>const</em> <em>int</em> trailing_rows = dst_rows - non_trailing_rows;</td></tr>
<tr><th id="579">579</th><td>    memcpy(packed_ptr + Layout::kCols * non_trailing_rows, trailing_buf,</td></tr>
<tr><th id="580">580</th><td>           Layout::kCols * trailing_rows * <b>sizeof</b>(std::int8_t));</td></tr>
<tr><th id="581">581</th><td>  }</td></tr>
<tr><th id="582">582</th><td>}</td></tr>
<tr><th id="583">583</th><td></td></tr>
<tr><th id="584">584</th><td><em>void</em> PackFloatColMajorForAvx2(<em>const</em> <em>float</em>* src_ptr, <em>const</em> <em>float</em>* zerobuf,</td></tr>
<tr><th id="585">585</th><td>                              <em>int</em> src_stride, <em>int</em> remaining_src_cols,</td></tr>
<tr><th id="586">586</th><td>                              <em>int</em> src_rows, <em>float</em>* packed_ptr) {</td></tr>
<tr><th id="587">587</th><td>  profiler::ScopeLabel label(<q>"Pack kAvx2Fma float"</q>);</td></tr>
<tr><th id="588">588</th><td>  <em>static</em> <b>constexpr</b> <em>int</em> kPackCols = <var>8</var>;  <i>// Source cols packed together.</i></td></tr>
<tr><th id="589">589</th><td>  <em>static</em> <b>constexpr</b> <em>int</em> kPackRows = <var>8</var>;  <i>// Short input is padded.</i></td></tr>
<tr><th id="590">590</th><td>  <em>float</em> trailing_buf[(kPackRows - <var>1</var>) * kPackCols];</td></tr>
<tr><th id="591">591</th><td>  <b>if</b> (remaining_src_cols &lt; <var>8</var>) {</td></tr>
<tr><th id="592">592</th><td>    memset(trailing_buf, <var>0</var>, <b>sizeof</b>(trailing_buf));</td></tr>
<tr><th id="593">593</th><td>  }</td></tr>
<tr><th id="594">594</th><td>  PackFloatColMajorForAvxCommonPacker&lt;PackImplFloatAvx2, Path::kAvx2Fma&gt;(</td></tr>
<tr><th id="595">595</th><td>      src_ptr, zerobuf, src_stride, remaining_src_cols, src_rows, packed_ptr,</td></tr>
<tr><th id="596">596</th><td>      trailing_buf);</td></tr>
<tr><th id="597">597</th><td></td></tr>
<tr><th id="598">598</th><td>  <em>const</em> <em>int</em> trailing_rows = src_rows &amp; (kPackRows - <var>1</var>);</td></tr>
<tr><th id="599">599</th><td>  <b>if</b> (trailing_rows &gt; <var>0</var>) {</td></tr>
<tr><th id="600">600</th><td>    <em>const</em> <em>int</em> non_trailing_rows = src_rows &amp; ~(kPackRows - <var>1</var>);</td></tr>
<tr><th id="601">601</th><td>    memcpy(packed_ptr + kPackCols * non_trailing_rows, trailing_buf,</td></tr>
<tr><th id="602">602</th><td>           kPackCols * trailing_rows * <b>sizeof</b>(<em>float</em>));</td></tr>
<tr><th id="603">603</th><td>  }</td></tr>
<tr><th id="604">604</th><td>}</td></tr>
<tr><th id="605">605</th><td></td></tr>
<tr><th id="606">606</th><td><em>void</em> Pack8bitRowMajorForAvx2(<em>const</em> std::uint8_t* src_ptr, <em>int</em> src_stride,</td></tr>
<tr><th id="607">607</th><td>                             <em>int</em> src_zero_point, std::int8_t* packed_ptr,</td></tr>
<tr><th id="608">608</th><td>                             <em>int</em> packed_stride, <em>int</em> start_col, <em>int</em> end_col,</td></tr>
<tr><th id="609">609</th><td>                             <em>int</em> src_cols, <em>int</em> block_row, <em>int</em> src_rows,</td></tr>
<tr><th id="610">610</th><td>                             <em>int</em> input_xor, std::int32_t* sums) {</td></tr>
<tr><th id="611">611</th><td>  <em>int</em> col = start_col;</td></tr>
<tr><th id="612">612</th><td>  <em>int</em> src_end_col = std::min(end_col, src_cols);</td></tr>
<tr><th id="613">613</th><td></td></tr>
<tr><th id="614">614</th><td>  <b>for</b> (; col &lt;= src_end_col - <var>8</var>; col += <var>8</var>) {</td></tr>
<tr><th id="615">615</th><td>    std::int8_t* dst_ptr = packed_ptr;</td></tr>
<tr><th id="616">616</th><td>    __m128i val0, val1, val2, val3;</td></tr>
<tr><th id="617">617</th><td>    __m128i input_xor_dup = _mm_set1_epi8(input_xor);</td></tr>
<tr><th id="618">618</th><td>    <i>// Load a 4x8 block.</i></td></tr>
<tr><th id="619">619</th><td>    <b>if</b> (block_row + <var>4</var> &lt;= src_rows) {</td></tr>
<tr><th id="620">620</th><td>      val0 = _mm_loadu_si64(src_ptr + <var>0</var> * src_stride);</td></tr>
<tr><th id="621">621</th><td>      val1 = _mm_loadu_si64(src_ptr + <var>1</var> * src_stride);</td></tr>
<tr><th id="622">622</th><td>      val2 = _mm_loadu_si64(src_ptr + <var>2</var> * src_stride);</td></tr>
<tr><th id="623">623</th><td>      val3 = _mm_loadu_si64(src_ptr + <var>3</var> * src_stride);</td></tr>
<tr><th id="624">624</th><td>    } <b>else</b> {</td></tr>
<tr><th id="625">625</th><td>      val0 = _mm_set1_epi8(src_zero_point);</td></tr>
<tr><th id="626">626</th><td>      val1 = val0;</td></tr>
<tr><th id="627">627</th><td>      val2 = val0;</td></tr>
<tr><th id="628">628</th><td>      val3 = val0;</td></tr>
<tr><th id="629">629</th><td>      <b>if</b> (block_row + <var>0</var> &lt; src_rows)</td></tr>
<tr><th id="630">630</th><td>        val0 = _mm_loadu_si64(src_ptr + <var>0</var> * src_stride);</td></tr>
<tr><th id="631">631</th><td>      <b>if</b> (block_row + <var>1</var> &lt; src_rows)</td></tr>
<tr><th id="632">632</th><td>        val1 = _mm_loadu_si64(src_ptr + <var>1</var> * src_stride);</td></tr>
<tr><th id="633">633</th><td>      <b>if</b> (block_row + <var>2</var> &lt; src_rows)</td></tr>
<tr><th id="634">634</th><td>        val2 = _mm_loadu_si64(src_ptr + <var>2</var> * src_stride);</td></tr>
<tr><th id="635">635</th><td>      <b>if</b> (block_row + <var>3</var> &lt; src_rows)</td></tr>
<tr><th id="636">636</th><td>        val3 = _mm_loadu_si64(src_ptr + <var>3</var> * src_stride);</td></tr>
<tr><th id="637">637</th><td>    }</td></tr>
<tr><th id="638">638</th><td>    <i>// Maybe xor the sign bit to convert from uint8 to int8.</i></td></tr>
<tr><th id="639">639</th><td>    val0 = _mm_xor_si128(val0, input_xor_dup);</td></tr>
<tr><th id="640">640</th><td>    val1 = _mm_xor_si128(val1, input_xor_dup);</td></tr>
<tr><th id="641">641</th><td>    val2 = _mm_xor_si128(val2, input_xor_dup);</td></tr>
<tr><th id="642">642</th><td>    val3 = _mm_xor_si128(val3, input_xor_dup);</td></tr>
<tr><th id="643">643</th><td>    <i>// Update the sums.</i></td></tr>
<tr><th id="644">644</th><td>    __m128i val16_0 = _mm_cvtepi8_epi16(val0);</td></tr>
<tr><th id="645">645</th><td>    __m128i val16_1 = _mm_cvtepi8_epi16(val1);</td></tr>
<tr><th id="646">646</th><td>    __m128i val16_2 = _mm_cvtepi8_epi16(val2);</td></tr>
<tr><th id="647">647</th><td>    __m128i val16_3 = _mm_cvtepi8_epi16(val3);</td></tr>
<tr><th id="648">648</th><td>    __m128i new_sum16 = _mm_add_epi16(_mm_add_epi16(val16_0, val16_1),</td></tr>
<tr><th id="649">649</th><td>                                      _mm_add_epi16(val16_2, val16_3));</td></tr>
<tr><th id="650">650</th><td>    __m256i sum =</td></tr>
<tr><th id="651">651</th><td>        _mm256_loadu_si256(<b>reinterpret_cast</b>&lt;<em>const</em> __m256i*&gt;(sums + col));</td></tr>
<tr><th id="652">652</th><td>    sum = _mm256_add_epi32(sum, _mm256_cvtepi16_epi32(new_sum16));</td></tr>
<tr><th id="653">653</th><td>    _mm256_storeu_si256(<b>reinterpret_cast</b>&lt;__m256i*&gt;(sums + col), sum);</td></tr>
<tr><th id="654">654</th><td>    <i>// Perform the transposition of 4x4 blocks</i></td></tr>
<tr><th id="655">655</th><td>    __m128i t2_val0 = _mm_unpacklo_epi8(val0, val1);</td></tr>
<tr><th id="656">656</th><td>    __m128i t2_val1 = _mm_unpacklo_epi8(val2, val3);</td></tr>
<tr><th id="657">657</th><td>    __m128i t4_val0 = _mm_unpacklo_epi16(t2_val0, t2_val1);</td></tr>
<tr><th id="658">658</th><td>    __m128i t4_val1 = _mm_unpackhi_epi16(t2_val0, t2_val1);</td></tr>
<tr><th id="659">659</th><td>    _mm_storeu_si128(<b>reinterpret_cast</b>&lt;__m128i*&gt;(dst_ptr), t4_val0);</td></tr>
<tr><th id="660">660</th><td>    _mm_storeu_si128(<b>reinterpret_cast</b>&lt;__m128i*&gt;(dst_ptr + <var>16</var>), t4_val1);</td></tr>
<tr><th id="661">661</th><td>    src_ptr += <var>8</var>;</td></tr>
<tr><th id="662">662</th><td>    packed_ptr += packed_stride * <var>8</var>;</td></tr>
<tr><th id="663">663</th><td>  }</td></tr>
<tr><th id="664">664</th><td>  <b>for</b> (; col &lt; src_end_col; col++) {</td></tr>
<tr><th id="665">665</th><td>    std::int32_t accum = <var>0</var>;</td></tr>
<tr><th id="666">666</th><td>    <b>for</b> (<em>int</em> r = <var>0</var>; r &lt; <var>4</var>; r++) {</td></tr>
<tr><th id="667">667</th><td>      std::int8_t packed_val;</td></tr>
<tr><th id="668">668</th><td>      <b>if</b> (block_row + r &lt; src_rows) {</td></tr>
<tr><th id="669">669</th><td>        packed_val = input_xor ^ src_ptr[r * src_stride];</td></tr>
<tr><th id="670">670</th><td>      } <b>else</b> {</td></tr>
<tr><th id="671">671</th><td>        packed_val = input_xor ^ src_zero_point;</td></tr>
<tr><th id="672">672</th><td>      }</td></tr>
<tr><th id="673">673</th><td>      accum += packed_val;</td></tr>
<tr><th id="674">674</th><td>      *packed_ptr++ = packed_val;</td></tr>
<tr><th id="675">675</th><td>    }</td></tr>
<tr><th id="676">676</th><td>    <b>if</b> (sums) {</td></tr>
<tr><th id="677">677</th><td>      sums[col] += accum;</td></tr>
<tr><th id="678">678</th><td>    }</td></tr>
<tr><th id="679">679</th><td>    src_ptr++;</td></tr>
<tr><th id="680">680</th><td>  }</td></tr>
<tr><th id="681">681</th><td>  <b>for</b> (; col &lt; end_col; col++) {</td></tr>
<tr><th id="682">682</th><td>    std::memset(packed_ptr, <var>0</var>, <var>4</var>);</td></tr>
<tr><th id="683">683</th><td>    packed_ptr += <var>4</var>;</td></tr>
<tr><th id="684">684</th><td>  }</td></tr>
<tr><th id="685">685</th><td>}</td></tr>
<tr><th id="686">686</th><td></td></tr>
<tr><th id="687">687</th><td><u>#<span data-ppcond="32">endif</span>  // RUY_PLATFORM_AVX2_FMA &amp;&amp; RUY_OPT(INTRINSICS)</u></td></tr>
<tr><th id="688">688</th><td></td></tr>
<tr><th id="689">689</th><td>}  <i>// namespace ruy</i></td></tr>
<tr><th id="690">690</th><td></td></tr>
</table><hr/><p id='footer'>
Generated on <em>2021-Aug-05</em> from project halide revision <em>v12.0.1</em>