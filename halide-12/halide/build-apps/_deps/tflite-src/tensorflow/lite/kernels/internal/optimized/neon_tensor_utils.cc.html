<!doctype html>
<html>
<head>
<meta name="viewport" content="width=device-width, initial-scale=1.0"><title>neon_tensor_utils.cc source code [halide/build-apps/_deps/tflite-src/tensorflow/lite/kernels/internal/optimized/neon_tensor_utils.cc] - Woboq Code Browser</title>
<link rel="stylesheet" href="../../../../../../../../.././data/qtcreator.css" title="QtCreator"/>
<link rel="alternate stylesheet" href="../../../../../../../../.././data/kdevelop.css" title="KDevelop"/>
<script type="text/javascript" src="../../../../../../../../.././data/jquery/jquery.min.js"></script>
<script type="text/javascript" src="../../../../../../../../.././data/jquery/jquery-ui.min.js"></script>
<script>var file = 'halide/build-apps/_deps/tflite-src/tensorflow/lite/kernels/internal/optimized/neon_tensor_utils.cc'; var root_path = '../../../../../../../../..'; var data_path = '../../../../../../../../.././data'; var ecma_script_api_version = 2;</script>
<script src='../../../../../../../../.././data/codebrowser.js'></script>
</head>
<body><div id='header'><h1 id='breadcrumb'><span>Browse the source code of </span><a href='../../../../../../../..'>halide</a>/<a href='../../../../../../..'>build-apps</a>/<a href='../../../../../..'>_deps</a>/<a href='../../../../..'>tflite-src</a>/<a href='../../../..'>tensorflow</a>/<a href='../../..'>lite</a>/<a href='../..'>kernels</a>/<a href='..'>internal</a>/<a href='./'>optimized</a>/<a href='neon_tensor_utils.cc.html'>neon_tensor_utils.cc</a></h1></div>
<hr/><div id='content'><table class="code">
<tr><th id="1">1</th><td><i>/* Copyright 2017 The TensorFlow Authors. All Rights Reserved.</i></td></tr>
<tr><th id="2">2</th><td><i></i></td></tr>
<tr><th id="3">3</th><td><i>Licensed under the Apache License, Version 2.0 (the "License");</i></td></tr>
<tr><th id="4">4</th><td><i>you may not use this file except in compliance with the License.</i></td></tr>
<tr><th id="5">5</th><td><i>You may obtain a copy of the License at</i></td></tr>
<tr><th id="6">6</th><td><i></i></td></tr>
<tr><th id="7">7</th><td><i>    <a href="http://www.apache.org/licenses/LICENSE-2.0">http://www.apache.org/licenses/LICENSE-2.0</a></i></td></tr>
<tr><th id="8">8</th><td><i></i></td></tr>
<tr><th id="9">9</th><td><i>Unless required by applicable law or agreed to in writing, software</i></td></tr>
<tr><th id="10">10</th><td><i>distributed under the License is distributed on an "AS IS" BASIS,</i></td></tr>
<tr><th id="11">11</th><td><i>WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</i></td></tr>
<tr><th id="12">12</th><td><i>See the License for the specific language governing permissions and</i></td></tr>
<tr><th id="13">13</th><td><i>limitations under the License.</i></td></tr>
<tr><th id="14">14</th><td><i>==============================================================================*/</i></td></tr>
<tr><th id="15">15</th><td><u>#include <a href="../../../../../../../../../include/sys/types.h.html">&lt;sys/types.h&gt;</a></u></td></tr>
<tr><th id="16">16</th><td></td></tr>
<tr><th id="17">17</th><td><u>#include &lt;algorithm&gt;</u></td></tr>
<tr><th id="18">18</th><td><u>#include &lt;cmath&gt;</u></td></tr>
<tr><th id="19">19</th><td><u>#include &lt;cstddef&gt;</u></td></tr>
<tr><th id="20">20</th><td><u>#include &lt;cstdint&gt;</u></td></tr>
<tr><th id="21">21</th><td><u>#include &lt;cstdlib&gt;</u></td></tr>
<tr><th id="22">22</th><td><u>#include &lt;cstring&gt;</u></td></tr>
<tr><th id="23">23</th><td><u>#include &lt;limits&gt;</u></td></tr>
<tr><th id="24">24</th><td><u>#include &lt;utility&gt;</u></td></tr>
<tr><th id="25">25</th><td></td></tr>
<tr><th id="26">26</th><td><u>#include <a href="../../../../../../../ruy/ruy/ruy.h.html">"ruy/ruy.h"</a>  // from @ruy</u></td></tr>
<tr><th id="27">27</th><td><u>#include <a href="../../cpu_backend_context.h.html">"tensorflow/lite/kernels/cpu_backend_context.h"</a></u></td></tr>
<tr><th id="28">28</th><td><u>#include <a href="../../cpu_backend_gemm.h.html">"tensorflow/lite/kernels/cpu_backend_gemm.h"</a></u></td></tr>
<tr><th id="29">29</th><td><u>#include <a href="../../cpu_backend_gemm_params.h.html">"tensorflow/lite/kernels/cpu_backend_gemm_params.h"</a></u></td></tr>
<tr><th id="30">30</th><td><u>#include <a href="../common.h.html">"tensorflow/lite/kernels/internal/common.h"</a></u></td></tr>
<tr><th id="31">31</th><td><u>#include <a href="../compatibility.h.html">"tensorflow/lite/kernels/internal/compatibility.h"</a></u></td></tr>
<tr><th id="32">32</th><td><u>#include <a href="../cppmath.h.html">"tensorflow/lite/kernels/internal/cppmath.h"</a></u></td></tr>
<tr><th id="33">33</th><td><u>#include <a href="cpu_check.h.html">"tensorflow/lite/kernels/internal/optimized/cpu_check.h"</a></u></td></tr>
<tr><th id="34">34</th><td><u>#include <a href="neon_tensor_utils_impl.h.html">"tensorflow/lite/kernels/internal/optimized/neon_tensor_utils_impl.h"</a></u></td></tr>
<tr><th id="35">35</th><td></td></tr>
<tr><th id="36">36</th><td><u>#<span data-ppcond="36">ifdef</span> <span class="macro" data-ref="_M/USE_NEON">USE_NEON</span></u></td></tr>
<tr><th id="37">37</th><td></td></tr>
<tr><th id="38">38</th><td><i>// aligned_alloc is available (via cstdlib/stdlib.h) with C++17/C11.</i></td></tr>
<tr><th id="39">39</th><td><u>#if __cplusplus &gt;= 201703L || __STDC_VERSION__ &gt;= 201112L</u></td></tr>
<tr><th id="40">40</th><td><u>#if !defined(__ANDROID__) || __ANDROID_API__ &gt;= 28</u></td></tr>
<tr><th id="41">41</th><td><i>// Neither Apple nor Windows provide aligned_alloc.</i></td></tr>
<tr><th id="42">42</th><td><u>#if !defined(__APPLE__) &amp;&amp; !defined(_WIN32)</u></td></tr>
<tr><th id="43">43</th><td><u>#define TFLITE_USE_STD_ALIGNED_ALLOC</u></td></tr>
<tr><th id="44">44</th><td><u>#endif</u></td></tr>
<tr><th id="45">45</th><td><u>#endif</u></td></tr>
<tr><th id="46">46</th><td><u>#endif</u></td></tr>
<tr><th id="47">47</th><td></td></tr>
<tr><th id="48">48</th><td><i>// Note: This is the same as ABSL_HAVE_BUILTIN, but can't include the header.</i></td></tr>
<tr><th id="49">49</th><td><u>#ifdef __has_builtin</u></td></tr>
<tr><th id="50">50</th><td><u>#define TFLITE_HAS_BUILTIN(x) __has_builtin(x)</u></td></tr>
<tr><th id="51">51</th><td><u>#else</u></td></tr>
<tr><th id="52">52</th><td><u>#define TFLITE_HAS_BUILTIN(x) 0</u></td></tr>
<tr><th id="53">53</th><td><u>#endif</u></td></tr>
<tr><th id="54">54</th><td></td></tr>
<tr><th id="55">55</th><td><i>// Note: This is the same as ABSL_PREDICT_FALSE, but can't include the header.</i></td></tr>
<tr><th id="56">56</th><td><u>#if TFLITE_HAS_BUILTIN(__builtin_expect) || \</u></td></tr>
<tr><th id="57">57</th><td><u>    (defined(__GNUC__) &amp;&amp; !defined(__clang__))</u></td></tr>
<tr><th id="58">58</th><td><u>#define TFLITE_UNLIKELY(x) (__builtin_expect(false || (x), false))</u></td></tr>
<tr><th id="59">59</th><td><u>#else</u></td></tr>
<tr><th id="60">60</th><td><u>#define TFLITE_UNLIKELY(x) (x)</u></td></tr>
<tr><th id="61">61</th><td><u>#endif</u></td></tr>
<tr><th id="62">62</th><td></td></tr>
<tr><th id="63">63</th><td><b>namespace</b> tflite {</td></tr>
<tr><th id="64">64</th><td><b>namespace</b> tensor_utils {</td></tr>
<tr><th id="65">65</th><td><b>namespace</b> {</td></tr>
<tr><th id="66">66</th><td></td></tr>
<tr><th id="67">67</th><td><b>constexpr</b> <em>int</em> kFloatValuesPerNeonVector = <var>4</var>;</td></tr>
<tr><th id="68">68</th><td><b>constexpr</b> <em>int</em> kInt16ValuesPerNeonVector = <var>8</var>;</td></tr>
<tr><th id="69">69</th><td><b>constexpr</b> <em>int</em> kInt8ValuesPerNeonVector = <var>16</var>;</td></tr>
<tr><th id="70">70</th><td><b>constexpr</b> <em>int</em> kNeonVectorAlignment = <var>4</var>;</td></tr>
<tr><th id="71">71</th><td><b>template</b> &lt;<em>int</em> PerNeonSize&gt;</td></tr>
<tr><th id="72">72</th><td><b>inline</b> <em>int</em> RoundDownVectors(<em>int</em> size) {</td></tr>
<tr><th id="73">73</th><td>  <b>return</b> size &amp; ~(PerNeonSize - <var>1</var>);</td></tr>
<tr><th id="74">74</th><td>}</td></tr>
<tr><th id="75">75</th><td></td></tr>
<tr><th id="76">76</th><td><i>// Allocates, at least, size bytes of uninitialized storage whose alignment is</i></td></tr>
<tr><th id="77">77</th><td><i>// specified by alignment. The size parameter must be an integral multiple of</i></td></tr>
<tr><th id="78">78</th><td><i>// alignment.</i></td></tr>
<tr><th id="79">79</th><td><i>// Caller is responsible by freeing the allocated memory by calling free on</i></td></tr>
<tr><th id="80">80</th><td><i>// the passed freeing_buffer pointer.</i></td></tr>
<tr><th id="81">81</th><td><b>inline</b> <em>void</em>* aligned_alloc(size_t alignment, size_t size,</td></tr>
<tr><th id="82">82</th><td>                           <em>void</em>** freeing_buffer) {</td></tr>
<tr><th id="83">83</th><td><u>#ifdef TFLITE_USE_STD_ALIGNED_ALLOC</u></td></tr>
<tr><th id="84">84</th><td>  *freeing_buffer = ::aligned_alloc(</td></tr>
<tr><th id="85">85</th><td>      alignment, (size + alignment - <var>1</var>) / alignment * alignment);</td></tr>
<tr><th id="86">86</th><td>  <b>return</b> *freeing_buffer;</td></tr>
<tr><th id="87">87</th><td><u>#else</u></td></tr>
<tr><th id="88">88</th><td>  *freeing_buffer = malloc(size + alignment);</td></tr>
<tr><th id="89">89</th><td>  <em>const</em> size_t offset = ((uintptr_t)*freeing_buffer) % alignment;  <i>// NOLINT</i></td></tr>
<tr><th id="90">90</th><td>  <b>return</b> offset == <var>0</var></td></tr>
<tr><th id="91">91</th><td>             ? *freeing_buffer</td></tr>
<tr><th id="92">92</th><td>             : ((<em>char</em>*)*freeing_buffer + (alignment - offset));  <i>// NOLINT</i></td></tr>
<tr><th id="93">93</th><td><u>#endif</u></td></tr>
<tr><th id="94">94</th><td>}</td></tr>
<tr><th id="95">95</th><td></td></tr>
<tr><th id="96">96</th><td><em>bool</em> HasSdotInstruction() {</td></tr>
<tr><th id="97">97</th><td>  <em>static</em> <em>const</em> <em>bool</em> has_dotprod = DetectArmNeonDotprod();</td></tr>
<tr><th id="98">98</th><td>  <b>return</b> has_dotprod;</td></tr>
<tr><th id="99">99</th><td>}</td></tr>
<tr><th id="100">100</th><td></td></tr>
<tr><th id="101">101</th><td><b>inline</b> <em>float</em> AccumulateNeonLane(<em>const</em> float32x4_t lane) {</td></tr>
<tr><th id="102">102</th><td><u>#ifdef __aarch64__</u></td></tr>
<tr><th id="103">103</th><td>  <b>return</b> vaddvq_f32(lane);</td></tr>
<tr><th id="104">104</th><td><u>#else</u></td></tr>
<tr><th id="105">105</th><td>  <b>return</b> vgetq_lane_f32(lane, <var>0</var>) + vgetq_lane_f32(lane, <var>1</var>) +</td></tr>
<tr><th id="106">106</th><td>         vgetq_lane_f32(lane, <var>2</var>) + vgetq_lane_f32(lane, <var>3</var>);</td></tr>
<tr><th id="107">107</th><td><u>#endif</u></td></tr>
<tr><th id="108">108</th><td>}</td></tr>
<tr><th id="109">109</th><td></td></tr>
<tr><th id="110">110</th><td><i>// Empirically determined breakpoints on when to use CpuBackendGemm vs.</i></td></tr>
<tr><th id="111">111</th><td><i>// standard MatrixBatchVectorMultiplyAccumulate. Briefly, if the batch size</i></td></tr>
<tr><th id="112">112</th><td><i>// is above 8 and the device does not have sdot, use CpuBackendGemm. Otherwise,</i></td></tr>
<tr><th id="113">113</th><td><i>// for large batch sizes, it makes sense to use CpuBackendGemm if the matrix</i></td></tr>
<tr><th id="114">114</th><td><i>// is not extremely rectangular.</i></td></tr>
<tr><th id="115">115</th><td><em>bool</em> UseCpuBackendGemm(<em>int</em> rows, <em>int</em> cols, <em>int</em> batch) {</td></tr>
<tr><th id="116">116</th><td>  <b>if</b> (!HasSdotInstruction()) {</td></tr>
<tr><th id="117">117</th><td>    <b>return</b> batch &gt;= <var>8</var>;</td></tr>
<tr><th id="118">118</th><td>  }</td></tr>
<tr><th id="119">119</th><td>  <b>if</b> (batch &lt; <var>16</var>) {</td></tr>
<tr><th id="120">120</th><td>    <b>return</b> <b>false</b>;</td></tr>
<tr><th id="121">121</th><td>  }</td></tr>
<tr><th id="122">122</th><td>  <b>constexpr</b> <em>int</em> kCpuBackendGemmThreshold = <var>2</var>;</td></tr>
<tr><th id="123">123</th><td>  <i>// Calculate "rectangularness" as a measure of how far from square the</i></td></tr>
<tr><th id="124">124</th><td><i>  // the LHS matrix is.</i></td></tr>
<tr><th id="125">125</th><td>  <em>int</em> row_rect = rows / cols;</td></tr>
<tr><th id="126">126</th><td>  <em>int</em> col_rect = cols / rows;</td></tr>
<tr><th id="127">127</th><td>  <em>int</em> rectangularness_lg2 =</td></tr>
<tr><th id="128">128</th><td>      row_rect &gt; <var>0</var> ? FloorLog2(row_rect) : FloorLog2(col_rect);</td></tr>
<tr><th id="129">129</th><td>  <em>int</em> batch_lg2 = FloorLog2(batch);</td></tr>
<tr><th id="130">130</th><td>  <i>// Large batch sizes move us above the threshold, but can be offset</i></td></tr>
<tr><th id="131">131</th><td><i>  // by significant rectangularness.</i></td></tr>
<tr><th id="132">132</th><td>  <em>int</em> batch_lg2_minus_rect_lg2 = batch_lg2 - rectangularness_lg2;</td></tr>
<tr><th id="133">133</th><td>  <b>return</b> batch_lg2_minus_rect_lg2 &gt; kCpuBackendGemmThreshold;</td></tr>
<tr><th id="134">134</th><td>}</td></tr>
<tr><th id="135">135</th><td></td></tr>
<tr><th id="136">136</th><td><b>inline</b> int32_t AccumulateNeonLane(<em>const</em> int32x4_t lane) {</td></tr>
<tr><th id="137">137</th><td><u>#ifdef __aarch64__</u></td></tr>
<tr><th id="138">138</th><td>  <b>return</b> vaddvq_s32(lane);</td></tr>
<tr><th id="139">139</th><td><u>#else</u></td></tr>
<tr><th id="140">140</th><td>  int64x2_t pairwiseAdded = vpaddlq_s32(lane);</td></tr>
<tr><th id="141">141</th><td>  <b>return</b> vgetq_lane_s64(pairwiseAdded, <var>0</var>) + vgetq_lane_s64(pairwiseAdded, <var>1</var>);</td></tr>
<tr><th id="142">142</th><td><u>#endif</u></td></tr>
<tr><th id="143">143</th><td>}</td></tr>
<tr><th id="144">144</th><td></td></tr>
<tr><th id="145">145</th><td><b>inline</b> int32x4x2_t MultiplyByQuantizedMultiplier2Rows(</td></tr>
<tr><th id="146">146</th><td>    int32x4x2_t input_val, int32 quantized_multiplier, <em>int</em> shift) {</td></tr>
<tr><th id="147">147</th><td>  <b>using</b> gemmlowp::RoundingDivideByPOT;</td></tr>
<tr><th id="148">148</th><td>  <b>using</b> gemmlowp::SaturatingRoundingDoublingHighMul;</td></tr>
<tr><th id="149">149</th><td>  <em>const</em> <em>int</em> left_shift = shift &gt; <var>0</var> ? shift : <var>0</var>;</td></tr>
<tr><th id="150">150</th><td>  <em>const</em> <em>int</em> right_shift = shift &gt; <var>0</var> ? <var>0</var> : -shift;</td></tr>
<tr><th id="151">151</th><td>  int32x4x2_t result;</td></tr>
<tr><th id="152">152</th><td>  <i>// The vector type support for SaturatingRoundingDoublingHighMulth in gemmlowp</i></td></tr>
<tr><th id="153">153</th><td><i>  // is limited to NEON.</i></td></tr>
<tr><th id="154">154</th><td><u>#ifdef GEMMLOWP_NEON</u></td></tr>
<tr><th id="155">155</th><td>  <em>const</em> int32x4_t left_shifted_one_dup = vdupq_n_s32(<var>1</var> &lt;&lt; left_shift);</td></tr>
<tr><th id="156">156</th><td>  result.val[<var>0</var>] =</td></tr>
<tr><th id="157">157</th><td>      RoundingDivideByPOT(SaturatingRoundingDoublingHighMul(</td></tr>
<tr><th id="158">158</th><td>                              vmulq_s32(input_val.val[<var>0</var>], left_shifted_one_dup),</td></tr>
<tr><th id="159">159</th><td>                              quantized_multiplier),</td></tr>
<tr><th id="160">160</th><td>                          right_shift);</td></tr>
<tr><th id="161">161</th><td>  result.val[<var>1</var>] =</td></tr>
<tr><th id="162">162</th><td>      RoundingDivideByPOT(SaturatingRoundingDoublingHighMul(</td></tr>
<tr><th id="163">163</th><td>                              vmulq_s32(input_val.val[<var>1</var>], left_shifted_one_dup),</td></tr>
<tr><th id="164">164</th><td>                              quantized_multiplier),</td></tr>
<tr><th id="165">165</th><td>                          right_shift);</td></tr>
<tr><th id="166">166</th><td><u>#else</u></td></tr>
<tr><th id="167">167</th><td>  <b>for</b> (<em>int</em> i = <var>0</var>; i &lt; <var>2</var>; ++i) {</td></tr>
<tr><th id="168">168</th><td>    int32_t vals[<var>4</var>];</td></tr>
<tr><th id="169">169</th><td>    vals[<var>0</var>] = RoundingDivideByPOT(</td></tr>
<tr><th id="170">170</th><td>        SaturatingRoundingDoublingHighMul(</td></tr>
<tr><th id="171">171</th><td>            vgetq_lane_s32(input_val.val[i], <var>0</var>) * (<var>1</var> &lt;&lt; left_shift),</td></tr>
<tr><th id="172">172</th><td>            quantized_multiplier),</td></tr>
<tr><th id="173">173</th><td>        right_shift);</td></tr>
<tr><th id="174">174</th><td>    vals[<var>1</var>] = RoundingDivideByPOT(</td></tr>
<tr><th id="175">175</th><td>        SaturatingRoundingDoublingHighMul(</td></tr>
<tr><th id="176">176</th><td>            vgetq_lane_s32(input_val.val[i], <var>1</var>) * (<var>1</var> &lt;&lt; left_shift),</td></tr>
<tr><th id="177">177</th><td>            quantized_multiplier),</td></tr>
<tr><th id="178">178</th><td>        right_shift);</td></tr>
<tr><th id="179">179</th><td>    vals[<var>2</var>] = RoundingDivideByPOT(</td></tr>
<tr><th id="180">180</th><td>        SaturatingRoundingDoublingHighMul(</td></tr>
<tr><th id="181">181</th><td>            vgetq_lane_s32(input_val.val[i], <var>2</var>) * (<var>1</var> &lt;&lt; left_shift),</td></tr>
<tr><th id="182">182</th><td>            quantized_multiplier),</td></tr>
<tr><th id="183">183</th><td>        right_shift);</td></tr>
<tr><th id="184">184</th><td>    vals[<var>3</var>] = RoundingDivideByPOT(</td></tr>
<tr><th id="185">185</th><td>        SaturatingRoundingDoublingHighMul(</td></tr>
<tr><th id="186">186</th><td>            vgetq_lane_s32(input_val.val[i], <var>3</var>) * (<var>1</var> &lt;&lt; left_shift),</td></tr>
<tr><th id="187">187</th><td>            quantized_multiplier),</td></tr>
<tr><th id="188">188</th><td>        right_shift);</td></tr>
<tr><th id="189">189</th><td></td></tr>
<tr><th id="190">190</th><td>    result.val[i] = vld1q_s32(<b>reinterpret_cast</b>&lt;int32_t*&gt;(&amp;vals));</td></tr>
<tr><th id="191">191</th><td>  }</td></tr>
<tr><th id="192">192</th><td><u>#endif</u></td></tr>
<tr><th id="193">193</th><td>  <b>return</b> result;</td></tr>
<tr><th id="194">194</th><td>}</td></tr>
<tr><th id="195">195</th><td></td></tr>
<tr><th id="196">196</th><td>}  <i>// namespace</i></td></tr>
<tr><th id="197">197</th><td></td></tr>
<tr><th id="198">198</th><td><em>void</em> NeonMatrixBatchVectorMultiplyAccumulate(<em>const</em> <em>float</em>* matrix, <em>int</em> m_rows,</td></tr>
<tr><th id="199">199</th><td>                                             <em>int</em> m_cols, <em>const</em> <em>float</em>* vector,</td></tr>
<tr><th id="200">200</th><td>                                             <em>int</em> n_batch, <em>float</em>* result) {</td></tr>
<tr><th id="201">201</th><td>  <i>// If v_size is not divisible by the vector size, then we need to process the</i></td></tr>
<tr><th id="202">202</th><td><i>  // final few elements sequentially. postamble_start shows the start index</i></td></tr>
<tr><th id="203">203</th><td><i>  // where this should happen.</i></td></tr>
<tr><th id="204">204</th><td>  <em>const</em> <em>int</em> postamble_start =</td></tr>
<tr><th id="205">205</th><td>      RoundDownVectors&lt;kFloatValuesPerNeonVector&gt;(m_cols);</td></tr>
<tr><th id="206">206</th><td></td></tr>
<tr><th id="207">207</th><td>  <b>for</b> (<em>int</em> b = <var>0</var>; b &lt; n_batch; b++) {</td></tr>
<tr><th id="208">208</th><td>    <em>float</em>* result_in_batch = result + b * m_rows;</td></tr>
<tr><th id="209">209</th><td>    <em>const</em> <em>float</em>* vector_in_batch = vector + b * m_cols;</td></tr>
<tr><th id="210">210</th><td>    <em>const</em> <em>float</em>* matrix_row = matrix;</td></tr>
<tr><th id="211">211</th><td></td></tr>
<tr><th id="212">212</th><td>    <i>// Main matrix by vector multiplication loop</i></td></tr>
<tr><th id="213">213</th><td>    <b>for</b> (<em>int</em> r = <var>0</var>; r &lt; m_rows; r++) {</td></tr>
<tr><th id="214">214</th><td>      float32x4_t acc_32x4 = vmovq_n_f32(<var>0.0</var>);</td></tr>
<tr><th id="215">215</th><td>      <em>int</em> c = <var>0</var>;</td></tr>
<tr><th id="216">216</th><td>      <b>for</b> (; c &lt; postamble_start; c += kFloatValuesPerNeonVector) {</td></tr>
<tr><th id="217">217</th><td>        <i>// Load 4 float values from vector and matrix row.</i></td></tr>
<tr><th id="218">218</th><td>        float32x4_t vector_f32x4 = vld1q_f32(vector_in_batch + c);</td></tr>
<tr><th id="219">219</th><td>        float32x4_t matrix_f32x4 = vld1q_f32(matrix_row + c);</td></tr>
<tr><th id="220">220</th><td>        <i>// Multiply the vector and matrix row and add to accumulator.</i></td></tr>
<tr><th id="221">221</th><td>        acc_32x4 = vmlaq_f32(acc_32x4, matrix_f32x4, vector_f32x4);</td></tr>
<tr><th id="222">222</th><td>      }</td></tr>
<tr><th id="223">223</th><td>      <i>// Add the 4 intermediate sum values to get the final dot-prod value for</i></td></tr>
<tr><th id="224">224</th><td><i>      // this column.</i></td></tr>
<tr><th id="225">225</th><td>      *result_in_batch += AccumulateNeonLane(acc_32x4);</td></tr>
<tr><th id="226">226</th><td>      <b>for</b> (; TFLITE_UNLIKELY(c &lt; m_cols); c++) {</td></tr>
<tr><th id="227">227</th><td>        *result_in_batch += matrix_row[c] * vector_in_batch[c];</td></tr>
<tr><th id="228">228</th><td>      }</td></tr>
<tr><th id="229">229</th><td>      matrix_row += m_cols;</td></tr>
<tr><th id="230">230</th><td>      ++result_in_batch;</td></tr>
<tr><th id="231">231</th><td>    }</td></tr>
<tr><th id="232">232</th><td>  }</td></tr>
<tr><th id="233">233</th><td>}</td></tr>
<tr><th id="234">234</th><td></td></tr>
<tr><th id="235">235</th><td><u>#ifdef __aarch64__</u></td></tr>
<tr><th id="236">236</th><td></td></tr>
<tr><th id="237">237</th><td><i>// We interleave vector data to make the dot product logic more efficient.</i></td></tr>
<tr><th id="238">238</th><td><i>// Suppose that vectors is:</i></td></tr>
<tr><th id="239">239</th><td><i>//     a0 a1 a2 a3 a4 a5 ...</i></td></tr>
<tr><th id="240">240</th><td><i>//     b0 b1 b2 b3 b4 b5 ...</i></td></tr>
<tr><th id="241">241</th><td><i>//     c0 c1 c2 c3 c4 c5 ...</i></td></tr>
<tr><th id="242">242</th><td><i>//     d0 d1 d2 d3 d4 d5 ...</i></td></tr>
<tr><th id="243">243</th><td><i>//     e0 e1 e2 e3 e4 e5 ...</i></td></tr>
<tr><th id="244">244</th><td><i>// This code interleaves them like this:</i></td></tr>
<tr><th id="245">245</th><td><i>//     a0 a1 a2 a3 b0 b1 b2 b3 c0 c1 c2 c3 d0 d1 d2 d3 a4 a5 a6 a7 b4 ...</i></td></tr>
<tr><th id="246">246</th><td><i>//     e0 e1 e2 e3 f0 f1 f2 f3 ...</i></td></tr>
<tr><th id="247">247</th><td><i>// Once the data is interleaved, each 16-byte read from the vectors pointer</i></td></tr>
<tr><th id="248">248</th><td><i>// contains 4 bytes from each of 4 vectors.</i></td></tr>
<tr><th id="249">249</th><td><em>const</em> int8_t* ShuffleVectors(<em>const</em> int8_t* vectors, <em>const</em> <em>int</em> n_batch,</td></tr>
<tr><th id="250">250</th><td>                             <em>const</em> <em>int</em> m_cols, <em>void</em>** shuffled_vectors_free) {</td></tr>
<tr><th id="251">251</th><td>  int8* shuffled_vectors = <b>reinterpret_cast</b>&lt;int8*&gt;(aligned_alloc(</td></tr>
<tr><th id="252">252</th><td>      kNeonVectorAlignment, n_batch * m_cols, shuffled_vectors_free));</td></tr>
<tr><th id="253">253</th><td></td></tr>
<tr><th id="254">254</th><td>  <b>for</b> (<em>int</em> i = <var>0</var>; i &lt; n_batch; i += <var>4</var>) {</td></tr>
<tr><th id="255">255</th><td>    int8* shuffled_vectors_ptr = shuffled_vectors + (i * m_cols);</td></tr>
<tr><th id="256">256</th><td>    <em>const</em> int8* unshuffled_vec0_ptr =</td></tr>
<tr><th id="257">257</th><td>        <b>reinterpret_cast</b>&lt;<em>const</em> int8*&gt;(vectors) + (i * m_cols);</td></tr>
<tr><th id="258">258</th><td>    <em>const</em> int8* unshuffled_vec1_ptr =</td></tr>
<tr><th id="259">259</th><td>        <b>reinterpret_cast</b>&lt;<em>const</em> int8*&gt;(vectors) + ((i + <var>1</var>) * m_cols);</td></tr>
<tr><th id="260">260</th><td>    <em>const</em> int8* unshuffled_vec2_ptr =</td></tr>
<tr><th id="261">261</th><td>        <b>reinterpret_cast</b>&lt;<em>const</em> int8*&gt;(vectors) + ((i + <var>2</var>) * m_cols);</td></tr>
<tr><th id="262">262</th><td>    <em>const</em> int8* unshuffled_vec3_ptr =</td></tr>
<tr><th id="263">263</th><td>        <b>reinterpret_cast</b>&lt;<em>const</em> int8*&gt;(vectors) + ((i + <var>3</var>) * m_cols);</td></tr>
<tr><th id="264">264</th><td>    <em>const</em> int8* <em>const</em> end_vec0_ptr = unshuffled_vec1_ptr;</td></tr>
<tr><th id="265">265</th><td></td></tr>
<tr><th id="266">266</th><td>    <b>while</b> (unshuffled_vec0_ptr != end_vec0_ptr) {</td></tr>
<tr><th id="267">267</th><td>      <b>asm</b> <em>volatile</em>(</td></tr>
<tr><th id="268">268</th><td>          <i>// This code path requires that (n_cols % 16) == 0 so we can safely</i></td></tr>
<tr><th id="269">269</th><td><i>          // read in 16-byte chunks from each row.</i></td></tr>
<tr><th id="270">270</th><td>          <q>"ld1 {v0.16b}, [%[unshuffled_vec0_ptr]], #16\n"</q></td></tr>
<tr><th id="271">271</th><td>          <q>"ld1 {v1.16b}, [%[unshuffled_vec1_ptr]], #16\n"</q></td></tr>
<tr><th id="272">272</th><td>          <q>"ld1 {v2.16b}, [%[unshuffled_vec2_ptr]], #16\n"</q></td></tr>
<tr><th id="273">273</th><td>          <q>"ld1 {v3.16b}, [%[unshuffled_vec3_ptr]], #16\n"</q></td></tr>
<tr><th id="274">274</th><td></td></tr>
<tr><th id="275">275</th><td>          <q>"st4 {v0.s, v1.s, v2.s, v3.s}[0], [%[shuffled_vectors_ptr]], #16\n"</q></td></tr>
<tr><th id="276">276</th><td>          <q>"st4 {v0.s, v1.s, v2.s, v3.s}[1], [%[shuffled_vectors_ptr]], #16\n"</q></td></tr>
<tr><th id="277">277</th><td>          <q>"st4 {v0.s, v1.s, v2.s, v3.s}[2], [%[shuffled_vectors_ptr]], #16\n"</q></td></tr>
<tr><th id="278">278</th><td>          <q>"st4 {v0.s, v1.s, v2.s, v3.s}[3], [%[shuffled_vectors_ptr]], #16\n"</q></td></tr>
<tr><th id="279">279</th><td></td></tr>
<tr><th id="280">280</th><td>          : [unshuffled_vec0_ptr] <q>"+r"</q>(unshuffled_vec0_ptr),</td></tr>
<tr><th id="281">281</th><td>            [unshuffled_vec1_ptr] <q>"+r"</q>(unshuffled_vec1_ptr),</td></tr>
<tr><th id="282">282</th><td>            [unshuffled_vec2_ptr] <q>"+r"</q>(unshuffled_vec2_ptr),</td></tr>
<tr><th id="283">283</th><td>            [unshuffled_vec3_ptr] <q>"+r"</q>(unshuffled_vec3_ptr),</td></tr>
<tr><th id="284">284</th><td>            [shuffled_vectors_ptr] <q>"+r"</q>(shuffled_vectors_ptr)</td></tr>
<tr><th id="285">285</th><td>          :</td></tr>
<tr><th id="286">286</th><td>          : <q>"v0"</q>, <q>"v1"</q>, <q>"v2"</q>, <q>"v3"</q>, <q>"cc"</q>, <q>"memory"</q>);</td></tr>
<tr><th id="287">287</th><td>    }</td></tr>
<tr><th id="288">288</th><td>  }</td></tr>
<tr><th id="289">289</th><td></td></tr>
<tr><th id="290">290</th><td>  <b>return</b> <b>reinterpret_cast</b>&lt;<em>const</em> int8_t*&gt;(shuffled_vectors);</td></tr>
<tr><th id="291">291</th><td>}</td></tr>
<tr><th id="292">292</th><td></td></tr>
<tr><th id="293">293</th><td><i>// Notes about the speed of this version vs. the baseline (from memory):</i></td></tr>
<tr><th id="294">294</th><td><i>// - With 256K of L1, we can keep a lot of vectors in cache.</i></td></tr>
<tr><th id="295">295</th><td><i>//   I recall a reasonable speedup just by rearranging the loop to have</i></td></tr>
<tr><th id="296">296</th><td><i>//   row on the outside and batch on the inside.</i></td></tr>
<tr><th id="297">297</th><td><i>// - I also recall getting a nice speedup from sdot.</i></td></tr>
<tr><th id="298">298</th><td><i>// - I tried many times to do better than the current implementation, using</i></td></tr>
<tr><th id="299">299</th><td><i>//   loop unrolling and instruction reordering to avoid stalls, etc.</i></td></tr>
<tr><th id="300">300</th><td><i>//   but I was not able to do significantly better. This code is, however,</i></td></tr>
<tr><th id="301">301</th><td><i>//   much worse than what the processor spec sheet suggests is possible.</i></td></tr>
<tr><th id="302">302</th><td><em>static</em> <em>void</em> DotprodMatrixBatchFourVectorMultiplyAccumulate(</td></tr>
<tr><th id="303">303</th><td>    <em>const</em> int8_t* <b>__restrict__</b> matrix, <em>const</em> <em>int</em> m_rows, <em>const</em> <em>int</em> m_cols,</td></tr>
<tr><th id="304">304</th><td>    <em>const</em> int8_t* vectors, <em>const</em> <em>float</em>* scaling_factors, <em>int</em> n_batch,</td></tr>
<tr><th id="305">305</th><td>    <em>float</em>* <b>__restrict__</b> result) {</td></tr>
<tr><th id="306">306</th><td>  <em>void</em>* shuffled_vectors_free;</td></tr>
<tr><th id="307">307</th><td></td></tr>
<tr><th id="308">308</th><td>  <em>const</em> int8_t* shuffled_vectors =</td></tr>
<tr><th id="309">309</th><td>      ShuffleVectors(vectors, n_batch, m_cols, &amp;shuffled_vectors_free);</td></tr>
<tr><th id="310">310</th><td></td></tr>
<tr><th id="311">311</th><td>  <b>for</b> (<em>int</em> row = <var>0</var>; row &lt; m_rows; row += <var>2</var>) {</td></tr>
<tr><th id="312">312</th><td>    <b>for</b> (<em>int</em> batch = <var>0</var>; batch &lt; n_batch; batch += <var>4</var>) {</td></tr>
<tr><th id="313">313</th><td>      <em>float</em>* result_ptr = result + (batch * m_rows) + row;</td></tr>
<tr><th id="314">314</th><td>      <em>const</em> int8* mat_ptr0 = matrix + (row * m_cols);</td></tr>
<tr><th id="315">315</th><td>      <em>const</em> int8* mat_ptr1 = matrix + ((row + <var>1</var>) * m_cols);</td></tr>
<tr><th id="316">316</th><td>      <em>const</em> int8* mat_ptr0_end = mat_ptr1;</td></tr>
<tr><th id="317">317</th><td>      <em>const</em> int8* vec_ptr = shuffled_vectors + (batch * m_cols);</td></tr>
<tr><th id="318">318</th><td>      <em>const</em> <em>float</em>* scaling_factors_ptr = scaling_factors + batch;</td></tr>
<tr><th id="319">319</th><td>      <em>const</em> uint64_t wide_rows = m_rows * <b>sizeof</b>(<em>float</em>);</td></tr>
<tr><th id="320">320</th><td>      <em>const</em> int8* mat_ptr2 = matrix + ((row + <var>2</var>) * m_cols);</td></tr>
<tr><th id="321">321</th><td>      <em>const</em> int8* mat_ptr3 = matrix + ((row + <var>3</var>) * m_cols);</td></tr>
<tr><th id="322">322</th><td></td></tr>
<tr><th id="323">323</th><td>      <b>asm</b> <em>volatile</em>(</td></tr>
<tr><th id="324">324</th><td>          <i>// Zero out the accumulator registers.</i></td></tr>
<tr><th id="325">325</th><td>          <q>"movi v0.4s, #0\n"</q></td></tr>
<tr><th id="326">326</th><td>          <q>"movi v1.4s, #0\n"</q></td></tr>
<tr><th id="327">327</th><td>          <q>"movi v2.4s, #0\n"</q></td></tr>
<tr><th id="328">328</th><td>          <q>"movi v3.4s, #0\n"</q></td></tr>
<tr><th id="329">329</th><td></td></tr>
<tr><th id="330">330</th><td>          <q>"1:\n"</q>  <i>// batch_cols_loop</i></td></tr>
<tr><th id="331">331</th><td></td></tr>
<tr><th id="332">332</th><td>          <i>// Read 16 more bytes from a pair of matrix rows.</i></td></tr>
<tr><th id="333">333</th><td>          <q>"ld1 {v12.16b}, [%[mat_ptr0]], #16\n"</q></td></tr>
<tr><th id="334">334</th><td></td></tr>
<tr><th id="335">335</th><td>          <i>// Prefetch two rows ahead.</i></td></tr>
<tr><th id="336">336</th><td>          <q>"prfm pldl1strm, [%[mat_ptr2]]\n"</q></td></tr>
<tr><th id="337">337</th><td>          <q>"prfm pldl1strm, [%[mat_ptr3]]\n"</q></td></tr>
<tr><th id="338">338</th><td></td></tr>
<tr><th id="339">339</th><td>          <i>// Read from input vectors 4 times; 64 bytes total.</i></td></tr>
<tr><th id="340">340</th><td><i>          // Each 16-byte register contains parts of 4 vectors; see the</i></td></tr>
<tr><th id="341">341</th><td><i>          // shuffle logic above.</i></td></tr>
<tr><th id="342">342</th><td><i></i></td></tr>
<tr><th id="343">343</th><td><i>          // From Benoit, places to look in the future:</i></td></tr>
<tr><th id="344">344</th><td><i>          // - Move load instructions further from sdot</i></td></tr>
<tr><th id="345">345</th><td><i>          // - Switch loop use-then-reload</i></td></tr>
<tr><th id="346">346</th><td><i>          // - Do partial unrolling to use register space better</i></td></tr>
<tr><th id="347">347</th><td>          <q>"ld1 {v8.16b}, [%[vec_ptr]], #16\n"</q></td></tr>
<tr><th id="348">348</th><td>          <q>".word 0x4f8ce100  // sdot v0.4s, v8.16b, v12.4b[0]\n"</q></td></tr>
<tr><th id="349">349</th><td>          <q>"ld1 {v9.16b}, [%[vec_ptr]], #16\n"</q></td></tr>
<tr><th id="350">350</th><td>          <q>".word 0x4face121  // sdot v1.4s, v9.16b, v12.4b[1]\n"</q></td></tr>
<tr><th id="351">351</th><td>          <q>"ld1 {v10.16b}, [%[vec_ptr]], #16\n"</q></td></tr>
<tr><th id="352">352</th><td>          <q>".word 0x4f8ce940  // sdot v0.4s, v10.16b, v12.4b[2]\n"</q></td></tr>
<tr><th id="353">353</th><td>          <q>"ld1 {v11.16b}, [%[vec_ptr]], #16\n"</q></td></tr>
<tr><th id="354">354</th><td>          <q>".word 0x4face961  // sdot v1.4s, v11.16b, v12.4b[3]\n"</q></td></tr>
<tr><th id="355">355</th><td></td></tr>
<tr><th id="356">356</th><td>          <i>// Update prefetch pointers.</i></td></tr>
<tr><th id="357">357</th><td>          <q>"add %[mat_ptr2], %[mat_ptr2], #16\n"</q></td></tr>
<tr><th id="358">358</th><td>          <q>"add %[mat_ptr3], %[mat_ptr3], #16\n"</q></td></tr>
<tr><th id="359">359</th><td></td></tr>
<tr><th id="360">360</th><td>          <i>// Re-use those vectors for the next row as well.</i></td></tr>
<tr><th id="361">361</th><td>          <q>"ld1 {v13.16b}, [%[mat_ptr1]], #16\n"</q></td></tr>
<tr><th id="362">362</th><td>          <q>".word 0x4f8de102  // sdot v2.4s, v8.16b, v13.4b[0]\n"</q></td></tr>
<tr><th id="363">363</th><td>          <q>".word 0x4fade123  // sdot v3.4s, v9.16b, v13.4b[1]\n"</q></td></tr>
<tr><th id="364">364</th><td>          <q>".word 0x4f8de942  // sdot v2.4s, v10.16b, v13.4b[2]\n"</q></td></tr>
<tr><th id="365">365</th><td>          <q>".word 0x4fade963  // sdot v3.4s, v11.16b, v13.4b[3]\n"</q></td></tr>
<tr><th id="366">366</th><td></td></tr>
<tr><th id="367">367</th><td>          <i>// If we're not done with these rows, continue.</i></td></tr>
<tr><th id="368">368</th><td>          <q>"cmp %[mat_ptr0], %[mat_ptr0_end]\n"</q></td></tr>
<tr><th id="369">369</th><td>          <q>"bne 1b\n"</q>  <i>// batch_cols_loop</i></td></tr>
<tr><th id="370">370</th><td></td></tr>
<tr><th id="371">371</th><td>          <i>// Done with the rows, sum the results.</i></td></tr>
<tr><th id="372">372</th><td>          <q>"add v0.4s, v0.4s, v1.4s\n"</q></td></tr>
<tr><th id="373">373</th><td>          <q>"add v2.4s, v2.4s, v3.4s\n"</q></td></tr>
<tr><th id="374">374</th><td></td></tr>
<tr><th id="375">375</th><td>          <i>// Convert the per-vector sums to floating point.</i></td></tr>
<tr><th id="376">376</th><td>          <q>"scvtf v0.4s, v0.4s\n"</q></td></tr>
<tr><th id="377">377</th><td>          <q>"scvtf v1.4s, v2.4s\n"</q></td></tr>
<tr><th id="378">378</th><td></td></tr>
<tr><th id="379">379</th><td>          <i>// Fetch scale factors.</i></td></tr>
<tr><th id="380">380</th><td>          <q>"ld1 {v4.4s}, [%[scaling_factors_ptr]]\n"</q></td></tr>
<tr><th id="381">381</th><td></td></tr>
<tr><th id="382">382</th><td>          <i>// Multiply scale factors times sums.</i></td></tr>
<tr><th id="383">383</th><td>          <q>"fmul v0.4s, v4.4s, v0.4s\n"</q></td></tr>
<tr><th id="384">384</th><td>          <q>"fmul v1.4s, v4.4s, v1.4s\n"</q></td></tr>
<tr><th id="385">385</th><td></td></tr>
<tr><th id="386">386</th><td>          <i>// Load previous result values.</i></td></tr>
<tr><th id="387">387</th><td><i>          // The result position is:</i></td></tr>
<tr><th id="388">388</th><td><i>          //   result[batch * m_rows + row]</i></td></tr>
<tr><th id="389">389</th><td><i>          // Here that is factored into:</i></td></tr>
<tr><th id="390">390</th><td><i>          //   result_ptr = result + row</i></td></tr>
<tr><th id="391">391</th><td><i>          //   *result_ptr = res[0]</i></td></tr>
<tr><th id="392">392</th><td><i>          //   (uint8*)result_ptr += (m_rows * sizeof(float))</i></td></tr>
<tr><th id="393">393</th><td><i>          //   *result_ptr = res[1]</i></td></tr>
<tr><th id="394">394</th><td><i>          //   ...</i></td></tr>
<tr><th id="395">395</th><td><i>          // Since we're reading two rows at a time, though, we read both</i></td></tr>
<tr><th id="396">396</th><td><i>          //   result[batch * m_rows + row]</i></td></tr>
<tr><th id="397">397</th><td><i>          // and</i></td></tr>
<tr><th id="398">398</th><td><i>          //   result[batch * m_rows + row + 1]</i></td></tr>
<tr><th id="399">399</th><td>          <q>"ld2 {v9.s, v10.s}[0], [%[result_ptr]], %[wide_rows]\n"</q></td></tr>
<tr><th id="400">400</th><td>          <q>"ld2 {v9.s, v10.s}[1], [%[result_ptr]], %[wide_rows]\n"</q></td></tr>
<tr><th id="401">401</th><td>          <q>"ld2 {v9.s, v10.s}[2], [%[result_ptr]], %[wide_rows]\n"</q></td></tr>
<tr><th id="402">402</th><td>          <q>"ld2 {v9.s, v10.s}[3], [%[result_ptr]], %[wide_rows]\n"</q></td></tr>
<tr><th id="403">403</th><td></td></tr>
<tr><th id="404">404</th><td>          <i>// Go back to the starting position (subtract wide_rows * 4).</i></td></tr>
<tr><th id="405">405</th><td>          <q>"sub %[result_ptr], %[result_ptr], %[wide_rows], lsl #2\n"</q></td></tr>
<tr><th id="406">406</th><td></td></tr>
<tr><th id="407">407</th><td>          <i>// Add previous result values.</i></td></tr>
<tr><th id="408">408</th><td>          <q>"fadd v9.4s, v9.4s, v0.4s\n"</q></td></tr>
<tr><th id="409">409</th><td>          <q>"fadd v10.4s, v10.4s, v1.4s\n"</q></td></tr>
<tr><th id="410">410</th><td></td></tr>
<tr><th id="411">411</th><td>          <i>// Store results.</i></td></tr>
<tr><th id="412">412</th><td>          <q>"st2 {v9.s, v10.s}[0], [%[result_ptr]], %[wide_rows]\n"</q></td></tr>
<tr><th id="413">413</th><td>          <q>"st2 {v9.s, v10.s}[1], [%[result_ptr]], %[wide_rows]\n"</q></td></tr>
<tr><th id="414">414</th><td>          <q>"st2 {v9.s, v10.s}[2], [%[result_ptr]], %[wide_rows]\n"</q></td></tr>
<tr><th id="415">415</th><td>          <q>"st2 {v9.s, v10.s}[3], [%[result_ptr]], %[wide_rows]\n"</q></td></tr>
<tr><th id="416">416</th><td>          : [mat_ptr0] <q>"+r"</q>(mat_ptr0), [mat_ptr1] <q>"+r"</q>(mat_ptr1),</td></tr>
<tr><th id="417">417</th><td>            [vec_ptr] <q>"+r"</q>(vec_ptr), [result_ptr] <q>"+r"</q>(result_ptr),</td></tr>
<tr><th id="418">418</th><td>            [mat_ptr2] <q>"+r"</q>(mat_ptr2), [mat_ptr3] <q>"+r"</q>(mat_ptr3)</td></tr>
<tr><th id="419">419</th><td>          : [mat_ptr0_end] <q>"r"</q>(mat_ptr0_end),</td></tr>
<tr><th id="420">420</th><td>            [scaling_factors_ptr] <q>"r"</q>(scaling_factors_ptr),</td></tr>
<tr><th id="421">421</th><td>            [wide_rows] <q>"r"</q>(wide_rows)</td></tr>
<tr><th id="422">422</th><td>          : <q>"x0"</q>, <q>"v0"</q>, <q>"v1"</q>, <q>"v2"</q>, <q>"v3"</q>, <q>"v4"</q>, <q>"v5"</q>, <q>"v6"</q>, <q>"v7"</q>, <q>"v8"</q>, <q>"v9"</q>,</td></tr>
<tr><th id="423">423</th><td>            <q>"v10"</q>, <q>"v11"</q>, <q>"v12"</q>, <q>"v13"</q>, <q>"cc"</q>, <q>"memory"</q>);</td></tr>
<tr><th id="424">424</th><td>    }</td></tr>
<tr><th id="425">425</th><td>  }</td></tr>
<tr><th id="426">426</th><td></td></tr>
<tr><th id="427">427</th><td>  free(shuffled_vectors_free);</td></tr>
<tr><th id="428">428</th><td>}</td></tr>
<tr><th id="429">429</th><td></td></tr>
<tr><th id="430">430</th><td><em>static</em> <em>void</em> DotprodMatrixBatchFourVectorMultiplyAccumulate(</td></tr>
<tr><th id="431">431</th><td>    <em>const</em> int8_t* <b>__restrict__</b> matrix, <em>const</em> <em>int</em> m_rows, <em>const</em> <em>int</em> m_cols,</td></tr>
<tr><th id="432">432</th><td>    <em>const</em> int8_t* vectors, <em>const</em> <em>float</em>* scaling_factors, <em>int</em> n_batch,</td></tr>
<tr><th id="433">433</th><td>    <em>float</em>* <b>__restrict__</b> result, <em>const</em> <em>float</em>* per_channel_scale,</td></tr>
<tr><th id="434">434</th><td>    <em>const</em> int32_t* input_offset, int32_t* row_sums) {</td></tr>
<tr><th id="435">435</th><td>  <em>void</em>* shuffled_vectors_free;</td></tr>
<tr><th id="436">436</th><td>  <em>const</em> int8_t* shuffled_vectors =</td></tr>
<tr><th id="437">437</th><td>      ShuffleVectors(vectors, n_batch, m_cols, &amp;shuffled_vectors_free);</td></tr>
<tr><th id="438">438</th><td></td></tr>
<tr><th id="439">439</th><td>  <b>for</b> (<em>int</em> row = <var>0</var>; row &lt; m_rows; row += <var>2</var>) {</td></tr>
<tr><th id="440">440</th><td>    <b>for</b> (<em>int</em> batch = <var>0</var>; batch &lt; n_batch; batch += <var>4</var>) {</td></tr>
<tr><th id="441">441</th><td>      <em>const</em> <em>float</em>* channel_scales_ptr = per_channel_scale + row;</td></tr>
<tr><th id="442">442</th><td>      int32_t* row_sums_ptr = row_sums ? row_sums + row : <b>nullptr</b>;</td></tr>
<tr><th id="443">443</th><td></td></tr>
<tr><th id="444">444</th><td>      <em>float</em>* result_ptr = result + (batch * m_rows) + row;</td></tr>
<tr><th id="445">445</th><td>      <em>const</em> int8* mat_ptr0 = matrix + (row * m_cols);</td></tr>
<tr><th id="446">446</th><td>      <em>const</em> int8* mat_ptr1 = matrix + ((row + <var>1</var>) * m_cols);</td></tr>
<tr><th id="447">447</th><td>      <em>const</em> int8* mat_ptr0_end = mat_ptr1;</td></tr>
<tr><th id="448">448</th><td>      <em>const</em> int8* vec_ptr = shuffled_vectors + (batch * m_cols);</td></tr>
<tr><th id="449">449</th><td>      <em>const</em> <em>float</em>* scaling_factors_ptr = scaling_factors + batch;</td></tr>
<tr><th id="450">450</th><td>      <em>const</em> uint64_t wide_rows = m_rows * <b>sizeof</b>(<em>float</em>);</td></tr>
<tr><th id="451">451</th><td>      <em>const</em> int32_t* batch_offsets_ptr = input_offset + batch;</td></tr>
<tr><th id="452">452</th><td>      <em>const</em> int32_t is_channel_scale_nullptr = per_channel_scale == <b>nullptr</b>;</td></tr>
<tr><th id="453">453</th><td>      <em>const</em> int32_t is_row_sums_nullptr = row_sums_ptr == <b>nullptr</b>;</td></tr>
<tr><th id="454">454</th><td>      <b>asm</b> <em>volatile</em>(</td></tr>
<tr><th id="455">455</th><td>          <q>"movi v0.4s, #0\n"</q></td></tr>
<tr><th id="456">456</th><td>          <q>"movi v1.4s, #0\n"</q></td></tr>
<tr><th id="457">457</th><td>          <q>"movi v2.4s, #0\n"</q></td></tr>
<tr><th id="458">458</th><td>          <q>"movi v3.4s, #0\n"</q></td></tr>
<tr><th id="459">459</th><td>          <i>// Load zero points.</i></td></tr>
<tr><th id="460">460</th><td>          <q>"ld1 {v7.4s}, [%[batch_offsets_ptr]]\n"</q></td></tr>
<tr><th id="461">461</th><td>          <q>"ld1 {v4.4s}, [%[scaling_factors_ptr]]\n"</q></td></tr>
<tr><th id="462">462</th><td>          <i>// Zero out zero point accumulators.</i></td></tr>
<tr><th id="463">463</th><td>          <q>"movi v14.4s, #0\n"</q></td></tr>
<tr><th id="464">464</th><td>          <q>"movi v15.4s, #0\n"</q></td></tr>
<tr><th id="465">465</th><td></td></tr>
<tr><th id="466">466</th><td>          <i>// Load per channel scales if not null.</i></td></tr>
<tr><th id="467">467</th><td>          <q>"cmp %w[is_channel_scale_nullptr], #0\n"</q></td></tr>
<tr><th id="468">468</th><td>          <q>"bne 1f\n"</q></td></tr>
<tr><th id="469">469</th><td>          <q>"ld1r {v16.4s}, [%[channel_scales_ptr]], #4\n"</q></td></tr>
<tr><th id="470">470</th><td>          <q>"ld1r {v17.4s}, [%[channel_scales_ptr]]\n"</q></td></tr>
<tr><th id="471">471</th><td>          <q>"fmul v16.4s, v16.4s, v4.4s\n"</q></td></tr>
<tr><th id="472">472</th><td>          <q>"fmul v17.4s, v17.4s, v4.4s\n"</q></td></tr>
<tr><th id="473">473</th><td>          <q>"b 2f\n"</q></td></tr>
<tr><th id="474">474</th><td>          <q>"1:\n"</q></td></tr>
<tr><th id="475">475</th><td>          <q>"mov v16.16b, v4.16b\n"</q></td></tr>
<tr><th id="476">476</th><td>          <q>"mov v17.16b, v4.16b\n"</q></td></tr>
<tr><th id="477">477</th><td>          <q>"2:\n"</q></td></tr>
<tr><th id="478">478</th><td>          <q>"ld1 {v12.16b}, [%[mat_ptr0]], #16\n"</q></td></tr>
<tr><th id="479">479</th><td>          <q>"ld1 {v8.16b}, [%[vec_ptr]], #16\n"</q></td></tr>
<tr><th id="480">480</th><td>          <q>".word 0x4f8ce100  // sdot v0.4s, v8.16b, v12.4b[0]\n"</q></td></tr>
<tr><th id="481">481</th><td>          <q>"ld1 {v9.16b}, [%[vec_ptr]], #16\n"</q></td></tr>
<tr><th id="482">482</th><td>          <q>".word 0x4face121  // sdot v1.4s, v9.16b, v12.4b[1]\n"</q></td></tr>
<tr><th id="483">483</th><td>          <q>"ld1 {v10.16b}, [%[vec_ptr]], #16\n"</q></td></tr>
<tr><th id="484">484</th><td>          <q>".word 0x4f8ce940  // sdot v0.4s, v10.16b, v12.4b[2]\n"</q></td></tr>
<tr><th id="485">485</th><td>          <q>"ld1 {v11.16b}, [%[vec_ptr]], #16\n"</q></td></tr>
<tr><th id="486">486</th><td>          <q>".word 0x4face961  // sdot v1.4s, v11.16b, v12.4b[3]\n"</q></td></tr>
<tr><th id="487">487</th><td>          <q>"ld1 {v13.16b}, [%[mat_ptr1]], #16\n"</q></td></tr>
<tr><th id="488">488</th><td>          <q>".word 0x4f8de102  // sdot v2.4s, v8.16b, v13.4b[0]\n"</q></td></tr>
<tr><th id="489">489</th><td>          <q>".word 0x4fade123  // sdot v3.4s, v9.16b, v13.4b[1]\n"</q></td></tr>
<tr><th id="490">490</th><td>          <q>".word 0x4f8de942  // sdot v2.4s, v10.16b, v13.4b[2]\n"</q></td></tr>
<tr><th id="491">491</th><td>          <q>".word 0x4fade963  // sdot v3.4s, v11.16b, v13.4b[3]\n"</q></td></tr>
<tr><th id="492">492</th><td>          <q>"cmp %w[is_row_sums_nullptr], #1\n"</q></td></tr>
<tr><th id="493">493</th><td>          <q>"bne 3f\n"</q></td></tr>
<tr><th id="494">494</th><td>          <i>// Accumulate row_sums for zero point calculations.</i></td></tr>
<tr><th id="495">495</th><td>          <q>"saddlp v12.8h, v12.16b\n"</q></td></tr>
<tr><th id="496">496</th><td>          <q>"saddlp v13.8h, v13.16b\n"</q></td></tr>
<tr><th id="497">497</th><td>          <q>"sadalp v14.4s, v12.8h\n"</q></td></tr>
<tr><th id="498">498</th><td>          <q>"sadalp v15.4s, v13.8h\n"</q></td></tr>
<tr><th id="499">499</th><td>          <q>"3:\n"</q></td></tr>
<tr><th id="500">500</th><td>          <q>"cmp %[mat_ptr0], %[mat_ptr0_end]\n"</q></td></tr>
<tr><th id="501">501</th><td>          <q>"bne 2b\n"</q></td></tr>
<tr><th id="502">502</th><td>          <q>"add v0.4s, v0.4s, v1.4s\n"</q></td></tr>
<tr><th id="503">503</th><td>          <q>"add v2.4s, v2.4s, v3.4s\n"</q></td></tr>
<tr><th id="504">504</th><td></td></tr>
<tr><th id="505">505</th><td>          <q>"cmp %w[is_row_sums_nullptr], #1\n"</q></td></tr>
<tr><th id="506">506</th><td>          <q>"bne 4f\n"</q></td></tr>
<tr><th id="507">507</th><td>          <i>// Calculate zero point offsets.</i></td></tr>
<tr><th id="508">508</th><td>          <q>"addv s14, v14.4s\n"</q></td></tr>
<tr><th id="509">509</th><td>          <q>"addv s15, v15.4s\n"</q></td></tr>
<tr><th id="510">510</th><td>          <q>"dup v14.4s, v14.s[0]\n"</q></td></tr>
<tr><th id="511">511</th><td>          <q>"dup v15.4s, v15.s[0]\n"</q></td></tr>
<tr><th id="512">512</th><td>          <q>"b 5f\n"</q></td></tr>
<tr><th id="513">513</th><td>          <q>"4:\n"</q></td></tr>
<tr><th id="514">514</th><td>          <q>"ld1r {v14.4s}, [%[row_sums_ptr]], #4\n"</q></td></tr>
<tr><th id="515">515</th><td>          <q>"ld1r {v15.4s}, [%[row_sums_ptr]]\n"</q></td></tr>
<tr><th id="516">516</th><td>          <q>"5:\n"</q></td></tr>
<tr><th id="517">517</th><td></td></tr>
<tr><th id="518">518</th><td>          <q>"mul v14.4s, v14.4s, v7.4s\n"</q></td></tr>
<tr><th id="519">519</th><td>          <q>"mul v15.4s, v15.4s, v7.4s\n"</q></td></tr>
<tr><th id="520">520</th><td>          <q>"sub v0.4s, v0.4s, v14.4s\n"</q></td></tr>
<tr><th id="521">521</th><td>          <q>"sub v2.4s, v2.4s, v15.4s\n"</q></td></tr>
<tr><th id="522">522</th><td></td></tr>
<tr><th id="523">523</th><td>          <q>"scvtf v0.4s, v0.4s\n"</q></td></tr>
<tr><th id="524">524</th><td>          <q>"scvtf v1.4s, v2.4s\n"</q></td></tr>
<tr><th id="525">525</th><td></td></tr>
<tr><th id="526">526</th><td>          <i>// Multiply scale.</i></td></tr>
<tr><th id="527">527</th><td>          <q>"fmul v0.4s, v16.4s, v0.4s\n"</q></td></tr>
<tr><th id="528">528</th><td>          <q>"fmul v1.4s, v17.4s, v1.4s\n"</q></td></tr>
<tr><th id="529">529</th><td></td></tr>
<tr><th id="530">530</th><td>          <q>"ld2 {v9.s, v10.s}[0], [%[result_ptr]], %[wide_rows]\n"</q></td></tr>
<tr><th id="531">531</th><td>          <q>"ld2 {v9.s, v10.s}[1], [%[result_ptr]], %[wide_rows]\n"</q></td></tr>
<tr><th id="532">532</th><td>          <q>"ld2 {v9.s, v10.s}[2], [%[result_ptr]], %[wide_rows]\n"</q></td></tr>
<tr><th id="533">533</th><td>          <q>"ld2 {v9.s, v10.s}[3], [%[result_ptr]], %[wide_rows]\n"</q></td></tr>
<tr><th id="534">534</th><td>          <q>"sub %[result_ptr], %[result_ptr], %[wide_rows], lsl #2\n"</q></td></tr>
<tr><th id="535">535</th><td>          <q>"fadd v9.4s, v9.4s, v0.4s\n"</q></td></tr>
<tr><th id="536">536</th><td>          <q>"fadd v10.4s, v10.4s, v1.4s\n"</q></td></tr>
<tr><th id="537">537</th><td>          <q>"st2 {v9.s, v10.s}[0], [%[result_ptr]], %[wide_rows]\n"</q></td></tr>
<tr><th id="538">538</th><td>          <q>"st2 {v9.s, v10.s}[1], [%[result_ptr]], %[wide_rows]\n"</q></td></tr>
<tr><th id="539">539</th><td>          <q>"st2 {v9.s, v10.s}[2], [%[result_ptr]], %[wide_rows]\n"</q></td></tr>
<tr><th id="540">540</th><td>          <q>"st2 {v9.s, v10.s}[3], [%[result_ptr]], %[wide_rows]\n"</q></td></tr>
<tr><th id="541">541</th><td>          : [mat_ptr0] <q>"+r"</q>(mat_ptr0), [mat_ptr1] <q>"+r"</q>(mat_ptr1),</td></tr>
<tr><th id="542">542</th><td>            [vec_ptr] <q>"+r"</q>(vec_ptr), [result_ptr] <q>"+r"</q>(result_ptr),</td></tr>
<tr><th id="543">543</th><td>            [row_sums_ptr] <q>"+r"</q>(row_sums_ptr),</td></tr>
<tr><th id="544">544</th><td>            [channel_scales_ptr] <q>"+r"</q>(channel_scales_ptr)</td></tr>
<tr><th id="545">545</th><td>          : [mat_ptr0_end] <q>"r"</q>(mat_ptr0_end),</td></tr>
<tr><th id="546">546</th><td>            [scaling_factors_ptr] <q>"r"</q>(scaling_factors_ptr),</td></tr>
<tr><th id="547">547</th><td>            [wide_rows] <q>"r"</q>(wide_rows),</td></tr>
<tr><th id="548">548</th><td>            [batch_offsets_ptr] <q>"r"</q>(batch_offsets_ptr),</td></tr>
<tr><th id="549">549</th><td>            [is_channel_scale_nullptr] <q>"r"</q>(is_channel_scale_nullptr),</td></tr>
<tr><th id="550">550</th><td>            [is_row_sums_nullptr] <q>"r"</q>(is_row_sums_nullptr)</td></tr>
<tr><th id="551">551</th><td>          : <q>"x0"</q>, <q>"v0"</q>, <q>"v1"</q>, <q>"v2"</q>, <q>"v3"</q>, <q>"v4"</q>, <q>"v5"</q>, <q>"v6"</q>, <q>"v7"</q>, <q>"v8"</q>, <q>"v9"</q>,</td></tr>
<tr><th id="552">552</th><td>            <q>"v10"</q>, <q>"v11"</q>, <q>"v12"</q>, <q>"v13"</q>, <q>"v14"</q>, <q>"v15"</q>, <q>"v16"</q>, <q>"v17"</q>, <q>"w0"</q>, <q>"w1"</q>,</td></tr>
<tr><th id="553">553</th><td>            <q>"cc"</q>, <q>"memory"</q>);</td></tr>
<tr><th id="554">554</th><td>    }</td></tr>
<tr><th id="555">555</th><td>  }</td></tr>
<tr><th id="556">556</th><td></td></tr>
<tr><th id="557">557</th><td>  free(shuffled_vectors_free);</td></tr>
<tr><th id="558">558</th><td>}</td></tr>
<tr><th id="559">559</th><td></td></tr>
<tr><th id="560">560</th><td><em>static</em> <em>void</em> DotprodMatrixBatchFourVectorMultiplyAccumulate(</td></tr>
<tr><th id="561">561</th><td>    <em>const</em> int8_t* <b>__restrict__</b> matrix, <em>const</em> <em>int</em> m_rows, <em>const</em> <em>int</em> m_cols,</td></tr>
<tr><th id="562">562</th><td>    <em>const</em> int8_t* vectors, <em>const</em> <em>float</em>* scaling_factors, <em>int</em> n_batch,</td></tr>
<tr><th id="563">563</th><td>    <em>float</em>* <b>__restrict__</b> result, <em>const</em> <em>float</em>* per_channel_scale,</td></tr>
<tr><th id="564">564</th><td>    <em>const</em> int32_t* input_offset) {</td></tr>
<tr><th id="565">565</th><td>  DotprodMatrixBatchFourVectorMultiplyAccumulate(</td></tr>
<tr><th id="566">566</th><td>      matrix, m_rows, m_cols, vectors, scaling_factors, n_batch, result,</td></tr>
<tr><th id="567">567</th><td>      per_channel_scale, input_offset, <b>nullptr</b>);</td></tr>
<tr><th id="568">568</th><td>}</td></tr>
<tr><th id="569">569</th><td></td></tr>
<tr><th id="570">570</th><td><i>// The DotprodMatrixBatchFourVectorMultiplyAccumulate kernel processes 4</i></td></tr>
<tr><th id="571">571</th><td><i>// vectors in the same time as the baseline processes 1 vector. However, it</i></td></tr>
<tr><th id="572">572</th><td><i>// requires 4 vectors of input.</i></td></tr>
<tr><th id="573">573</th><td><i>//</i></td></tr>
<tr><th id="574">574</th><td><i>// To take advantage of this speed difference, we add some zero-valued</i></td></tr>
<tr><th id="575">575</th><td><i>// vectors to the batch so that n_batch is a multiple of 4. Then we execute</i></td></tr>
<tr><th id="576">576</th><td><i>// DotprodMatrixBatchPaddedFourVectorMultiplyAccumulate on that padded batch,</i></td></tr>
<tr><th id="577">577</th><td><i>// then extract just the results we want at the end (ignoring the extra padding</i></td></tr>
<tr><th id="578">578</th><td><i>// outputs).</i></td></tr>
<tr><th id="579">579</th><td><i>//</i></td></tr>
<tr><th id="580">580</th><td><i>// The relative cost of the padding is large when the matrix is smaller than</i></td></tr>
<tr><th id="581">581</th><td><i>// 128x128, so we don't use this code path on small matrices. On larger</i></td></tr>
<tr><th id="582">582</th><td><i>// matrices, the computation cost dwarfs the padding cost, making this code</i></td></tr>
<tr><th id="583">583</th><td><i>// viable.</i></td></tr>
<tr><th id="584">584</th><td><i>//</i></td></tr>
<tr><th id="585">585</th><td><i>// If we ignore the cost of padding, this kernel is:</i></td></tr>
<tr><th id="586">586</th><td><i>//    1x the speed of NeonMatrixBatchVectorMultiplyImpl for n_batch = 1</i></td></tr>
<tr><th id="587">587</th><td><i>//    2x the speed of NeonMatrixBatchVectorMultiplyImpl for n_batch = 2</i></td></tr>
<tr><th id="588">588</th><td><i>//    3x the speed of NeonMatrixBatchVectorMultiplyImpl for n_batch = 3</i></td></tr>
<tr><th id="589">589</th><td><i>//    ...</i></td></tr>
<tr><th id="590">590</th><td><i>//</i></td></tr>
<tr><th id="591">591</th><td><i>// We don't use this kernel when n_batch = 1 because the baseline kernel</i></td></tr>
<tr><th id="592">592</th><td><i>// is fine for that case.</i></td></tr>
<tr><th id="593">593</th><td><em>void</em> DotprodMatrixBatchPaddedFourVectorMultiplyAccumulate(</td></tr>
<tr><th id="594">594</th><td>    <em>const</em> int8_t* <b>__restrict__</b> matrix, <em>const</em> <em>int</em> m_rows, <em>const</em> <em>int</em> m_cols,</td></tr>
<tr><th id="595">595</th><td>    <em>const</em> int8_t* vectors, <em>const</em> <em>float</em>* scaling_factors, <em>int</em> n_batch,</td></tr>
<tr><th id="596">596</th><td>    <em>float</em>* <b>__restrict__</b> result, <em>const</em> <em>float</em>* per_channel_scale,</td></tr>
<tr><th id="597">597</th><td>    <em>const</em> int32_t* input_offset, int32_t* row_sums) {</td></tr>
<tr><th id="598">598</th><td>  <i>// Round to the nearest multiple of 4.</i></td></tr>
<tr><th id="599">599</th><td>  <em>int</em> batch_round_up = n_batch;</td></tr>
<tr><th id="600">600</th><td>  <b>if</b> (n_batch % <var>4</var> != <var>0</var>) {</td></tr>
<tr><th id="601">601</th><td>    batch_round_up += (<var>4</var> - n_batch % <var>4</var>);</td></tr>
<tr><th id="602">602</th><td>  }</td></tr>
<tr><th id="603">603</th><td>  TFLITE_CHECK_LE(n_batch, batch_round_up);</td></tr>
<tr><th id="604">604</th><td></td></tr>
<tr><th id="605">605</th><td>  <em>void</em>* padded_vectors_free;</td></tr>
<tr><th id="606">606</th><td>  <em>const</em> <em>int</em> padded_vectors_size = batch_round_up * m_cols;</td></tr>
<tr><th id="607">607</th><td>  int8_t* padded_vectors = <b>reinterpret_cast</b>&lt;int8_t*&gt;(aligned_alloc(</td></tr>
<tr><th id="608">608</th><td>      kNeonVectorAlignment, padded_vectors_size, &amp;padded_vectors_free));</td></tr>
<tr><th id="609">609</th><td>  memset(padded_vectors, <var>0</var>, padded_vectors_size);</td></tr>
<tr><th id="610">610</th><td></td></tr>
<tr><th id="611">611</th><td>  <em>void</em>* padded_result_free;</td></tr>
<tr><th id="612">612</th><td>  <em>const</em> <em>int</em> result_size = n_batch * m_rows * <b>sizeof</b>(<em>float</em>);</td></tr>
<tr><th id="613">613</th><td>  <em>const</em> <em>int</em> padded_result_size = batch_round_up * m_rows * <b>sizeof</b>(<em>float</em>);</td></tr>
<tr><th id="614">614</th><td>  <em>float</em>* padded_result = <b>reinterpret_cast</b>&lt;<em>float</em>*&gt;(aligned_alloc(</td></tr>
<tr><th id="615">615</th><td>      kNeonVectorAlignment, padded_result_size, &amp;padded_result_free));</td></tr>
<tr><th id="616">616</th><td>  memcpy(padded_result, result, result_size);</td></tr>
<tr><th id="617">617</th><td>  memset(<b>reinterpret_cast</b>&lt;<em>char</em>*&gt;(padded_result) + result_size, <var>0</var>,</td></tr>
<tr><th id="618">618</th><td>         padded_result_size - result_size);</td></tr>
<tr><th id="619">619</th><td></td></tr>
<tr><th id="620">620</th><td>  <i>// Copy the input into the padded data structure.</i></td></tr>
<tr><th id="621">621</th><td>  TFLITE_CHECK_LE(n_batch * m_cols, padded_vectors_size);</td></tr>
<tr><th id="622">622</th><td>  memcpy(padded_vectors, vectors, n_batch * m_cols);</td></tr>
<tr><th id="623">623</th><td></td></tr>
<tr><th id="624">624</th><td>  <em>void</em>* padded_scaling_factors_free;</td></tr>
<tr><th id="625">625</th><td>  <em>const</em> <em>int</em> padded_scaling_factors_size = batch_round_up * <b>sizeof</b>(<em>float</em>);</td></tr>
<tr><th id="626">626</th><td>  <em>float</em>* padded_scaling_factors = <b>reinterpret_cast</b>&lt;<em>float</em>*&gt;(</td></tr>
<tr><th id="627">627</th><td>      aligned_alloc(kNeonVectorAlignment, padded_scaling_factors_size,</td></tr>
<tr><th id="628">628</th><td>                    &amp;padded_scaling_factors_free));</td></tr>
<tr><th id="629">629</th><td>  TFLITE_CHECK_LE(n_batch * <b>sizeof</b>(<em>float</em>), padded_scaling_factors_size);</td></tr>
<tr><th id="630">630</th><td>  TFLITE_CHECK_LE(batch_round_up * <b>sizeof</b>(<em>float</em>), padded_scaling_factors_size);</td></tr>
<tr><th id="631">631</th><td>  memset(padded_scaling_factors, <var>0</var>, batch_round_up * <b>sizeof</b>(<em>float</em>));</td></tr>
<tr><th id="632">632</th><td>  memcpy(padded_scaling_factors, scaling_factors, n_batch * <b>sizeof</b>(<em>float</em>));</td></tr>
<tr><th id="633">633</th><td></td></tr>
<tr><th id="634">634</th><td>  <b>if</b> (input_offset != <b>nullptr</b>) {</td></tr>
<tr><th id="635">635</th><td>    <em>void</em>* padded_input_offset_free;</td></tr>
<tr><th id="636">636</th><td>    <em>const</em> <em>int</em> padded_input_offset_size = batch_round_up * <b>sizeof</b>(int32_t);</td></tr>
<tr><th id="637">637</th><td>    int32_t* padded_input_offset = <b>reinterpret_cast</b>&lt;int32_t*&gt;(</td></tr>
<tr><th id="638">638</th><td>        aligned_alloc(kNeonVectorAlignment, padded_input_offset_size,</td></tr>
<tr><th id="639">639</th><td>                      &amp;padded_input_offset_free));</td></tr>
<tr><th id="640">640</th><td>    TFLITE_CHECK_LE(n_batch * <b>sizeof</b>(int32_t), padded_input_offset_size);</td></tr>
<tr><th id="641">641</th><td>    TFLITE_CHECK_LE(batch_round_up * <b>sizeof</b>(int32_t), padded_input_offset_size);</td></tr>
<tr><th id="642">642</th><td>    memset(padded_input_offset, <var>0</var>, batch_round_up * <b>sizeof</b>(int32_t));</td></tr>
<tr><th id="643">643</th><td>    memcpy(padded_input_offset, input_offset, n_batch * <b>sizeof</b>(int32_t));</td></tr>
<tr><th id="644">644</th><td></td></tr>
<tr><th id="645">645</th><td>    <i>// Call the main kernel.</i></td></tr>
<tr><th id="646">646</th><td>    DotprodMatrixBatchFourVectorMultiplyAccumulate(</td></tr>
<tr><th id="647">647</th><td>        matrix, m_rows, m_cols, padded_vectors, padded_scaling_factors,</td></tr>
<tr><th id="648">648</th><td>        batch_round_up, padded_result, per_channel_scale, padded_input_offset,</td></tr>
<tr><th id="649">649</th><td>        row_sums);</td></tr>
<tr><th id="650">650</th><td></td></tr>
<tr><th id="651">651</th><td>    free(padded_input_offset_free);</td></tr>
<tr><th id="652">652</th><td>  } <b>else</b> {</td></tr>
<tr><th id="653">653</th><td>    <i>// Call the main kernel.</i></td></tr>
<tr><th id="654">654</th><td>    DotprodMatrixBatchFourVectorMultiplyAccumulate(</td></tr>
<tr><th id="655">655</th><td>        matrix, m_rows, m_cols, padded_vectors, padded_scaling_factors,</td></tr>
<tr><th id="656">656</th><td>        batch_round_up, padded_result);</td></tr>
<tr><th id="657">657</th><td>  }</td></tr>
<tr><th id="658">658</th><td>  memcpy(result, padded_result, result_size);</td></tr>
<tr><th id="659">659</th><td></td></tr>
<tr><th id="660">660</th><td>  free(padded_result_free);</td></tr>
<tr><th id="661">661</th><td>  free(padded_vectors_free);</td></tr>
<tr><th id="662">662</th><td>  free(padded_scaling_factors_free);</td></tr>
<tr><th id="663">663</th><td>}</td></tr>
<tr><th id="664">664</th><td></td></tr>
<tr><th id="665">665</th><td><em>void</em> DotprodMatrixBatchPaddedFourVectorMultiplyAccumulate(</td></tr>
<tr><th id="666">666</th><td>    <em>const</em> int8_t* <b>__restrict__</b> matrix, <em>const</em> <em>int</em> m_rows, <em>const</em> <em>int</em> m_cols,</td></tr>
<tr><th id="667">667</th><td>    <em>const</em> int8_t* vectors, <em>const</em> <em>float</em>* scaling_factors, <em>int</em> n_batch,</td></tr>
<tr><th id="668">668</th><td>    <em>float</em>* <b>__restrict__</b> result) {</td></tr>
<tr><th id="669">669</th><td>  DotprodMatrixBatchPaddedFourVectorMultiplyAccumulate(</td></tr>
<tr><th id="670">670</th><td>      matrix, m_rows, m_cols, vectors, scaling_factors, n_batch, result,</td></tr>
<tr><th id="671">671</th><td>      <i>/*per_channel_scale=*/</i><b>nullptr</b>, <i>/*input_offset=*/</i><b>nullptr</b>,</td></tr>
<tr><th id="672">672</th><td>      <i>/*row_sums=*/</i><b>nullptr</b>);</td></tr>
<tr><th id="673">673</th><td>}</td></tr>
<tr><th id="674">674</th><td></td></tr>
<tr><th id="675">675</th><td><em>static</em> <em>void</em> DotprodSparseMatrixBatchVectorMultiplyAccumulate(</td></tr>
<tr><th id="676">676</th><td>    <em>const</em> int8_t* <b>__restrict__</b> matrix, <em>const</em> uint8_t* ledger, <em>const</em> <em>int</em> m_rows,</td></tr>
<tr><th id="677">677</th><td>    <em>const</em> <em>int</em> m_cols, <em>const</em> int8_t* <b>__restrict__</b> vectors,</td></tr>
<tr><th id="678">678</th><td>    <em>const</em> <em>float</em>* scaling_factors, <em>int</em> n_batch, <em>float</em>* <b>__restrict__</b> result) {</td></tr>
<tr><th id="679">679</th><td>  <em>const</em> uint8_t* ledger_ptr = ledger;</td></tr>
<tr><th id="680">680</th><td>  <em>const</em> int8* mat_ptr = matrix;</td></tr>
<tr><th id="681">681</th><td></td></tr>
<tr><th id="682">682</th><td>  <b>for</b> (<em>int</em> row = <var>0</var>; row &lt; m_rows; row++) {</td></tr>
<tr><th id="683">683</th><td>    <em>int</em> num_nonzero_chunks = *ledger_ptr;</td></tr>
<tr><th id="684">684</th><td>    ledger_ptr++;</td></tr>
<tr><th id="685">685</th><td>    <em>const</em> uint8* ledger_start = ledger_ptr;</td></tr>
<tr><th id="686">686</th><td>    <em>const</em> uint8* ledger_end = ledger_ptr + num_nonzero_chunks;</td></tr>
<tr><th id="687">687</th><td>    <em>const</em> int8* mat_start = mat_ptr;</td></tr>
<tr><th id="688">688</th><td></td></tr>
<tr><th id="689">689</th><td>    <b>for</b> (<em>int</em> batch = <var>0</var>; batch &lt; n_batch; batch++) {</td></tr>
<tr><th id="690">690</th><td>      <em>const</em> int8* vec_ptr = vectors + (batch * m_cols);</td></tr>
<tr><th id="691">691</th><td>      int64_t row_sum = <var>0</var>;</td></tr>
<tr><th id="692">692</th><td></td></tr>
<tr><th id="693">693</th><td>      mat_ptr = mat_start;</td></tr>
<tr><th id="694">694</th><td>      ledger_ptr = ledger_start;</td></tr>
<tr><th id="695">695</th><td></td></tr>
<tr><th id="696">696</th><td>      <b>if</b> (ledger_ptr != ledger_end) {</td></tr>
<tr><th id="697">697</th><td>        <b>asm</b> <em>volatile</em>(</td></tr>
<tr><th id="698">698</th><td>            <q>"movi v0.4s, #0\n"</q></td></tr>
<tr><th id="699">699</th><td>            <q>"movi v1.4s, #0\n"</q></td></tr>
<tr><th id="700">700</th><td>            <q>"movi v8.4s, #0\n"</q></td></tr>
<tr><th id="701">701</th><td>            <q>"mov x7, 0\n"</q></td></tr>
<tr><th id="702">702</th><td></td></tr>
<tr><th id="703">703</th><td>            <q>"1:\n"</q>  <i>// chunks_loop</i></td></tr>
<tr><th id="704">704</th><td></td></tr>
<tr><th id="705">705</th><td>            <i>// Single matrix chunk, 16 bytes</i></td></tr>
<tr><th id="706">706</th><td>            <q>"ld1 {v8.16b}, [%[mat_ptr]], #16\n"</q></td></tr>
<tr><th id="707">707</th><td></td></tr>
<tr><th id="708">708</th><td>            <i>// Read the next ledger index and increment.</i></td></tr>
<tr><th id="709">709</th><td>            <q>"ldrb w7, [%[ledger_ptr]], #1\n"</q></td></tr>
<tr><th id="710">710</th><td></td></tr>
<tr><th id="711">711</th><td>            <i>// Read 16 bytes of vector data from (vec_ptr + (ledger_index * 16))</i></td></tr>
<tr><th id="712">712</th><td>            <q>"add x8, %[vec_ptr], x7, lsl #4\n"</q></td></tr>
<tr><th id="713">713</th><td>            <q>"ld1 {v9.16b}, [x8]\n"</q></td></tr>
<tr><th id="714">714</th><td></td></tr>
<tr><th id="715">715</th><td>            <i>// Dot product of matrix row and vector.</i></td></tr>
<tr><th id="716">716</th><td>            <q>".word 0x4e889520  // sdot v0.4s, v9.16b, v8.16b\n"</q></td></tr>
<tr><th id="717">717</th><td></td></tr>
<tr><th id="718">718</th><td>            <q>"cmp %[ledger_ptr], %[ledger_end]\n"</q></td></tr>
<tr><th id="719">719</th><td>            <q>"blt 1b\n"</q>  <i>// chunks_loop</i></td></tr>
<tr><th id="720">720</th><td></td></tr>
<tr><th id="721">721</th><td>            <i>// Sum the 4 vector components into a 32-bit value.</i></td></tr>
<tr><th id="722">722</th><td>            <q>"addv s1, v0.4s\n"</q></td></tr>
<tr><th id="723">723</th><td>            <i>// row_sum is 64-bit, so we copy 64 bits of v1 into it.</i></td></tr>
<tr><th id="724">724</th><td><i>            // We have to be careful to cast this value to 32 bits in order</i></td></tr>
<tr><th id="725">725</th><td><i>            // to interpret the sign bit properly.</i></td></tr>
<tr><th id="726">726</th><td>            <q>"mov %[row_sum], v1.d[0]\n"</q></td></tr>
<tr><th id="727">727</th><td>            : [row_sum] <q>"=r"</q>(row_sum), [ledger_ptr] <q>"+r"</q>(ledger_ptr),</td></tr>
<tr><th id="728">728</th><td>              [mat_ptr] <q>"+r"</q>(mat_ptr), [vec_ptr] <q>"+r"</q>(vec_ptr)</td></tr>
<tr><th id="729">729</th><td>            : [ledger_end] <q>"r"</q>(ledger_end)</td></tr>
<tr><th id="730">730</th><td>            : <q>"x0"</q>, <q>"x1"</q>, <q>"x7"</q>, <q>"x8"</q>, <q>"v0"</q>, <q>"v1"</q>, <q>"v8"</q>, <q>"v9"</q>, <q>"cc"</q>, <q>"memory"</q>);</td></tr>
<tr><th id="731">731</th><td>      }</td></tr>
<tr><th id="732">732</th><td>      result[batch * m_rows + row] +=</td></tr>
<tr><th id="733">733</th><td>          <b>static_cast</b>&lt;int32&gt;(row_sum) * scaling_factors[batch];</td></tr>
<tr><th id="734">734</th><td>    }</td></tr>
<tr><th id="735">735</th><td>  }</td></tr>
<tr><th id="736">736</th><td>}</td></tr>
<tr><th id="737">737</th><td></td></tr>
<tr><th id="738">738</th><td><u>#endif  // __aarch64__</u></td></tr>
<tr><th id="739">739</th><td></td></tr>
<tr><th id="740">740</th><td><em>void</em> NeonMatrixBatchVectorMultiplyImpl(<em>const</em> int8_t* input, <em>const</em> int32_t* bias,</td></tr>
<tr><th id="741">741</th><td>                                       <em>const</em> int8_t* input_to_gate_weights,</td></tr>
<tr><th id="742">742</th><td>                                       int32_t n_batch, int32_t n_input,</td></tr>
<tr><th id="743">743</th><td>                                       int32_t n_output, int32_t output_zp,</td></tr>
<tr><th id="744">744</th><td>                                       int32_t* scratch) {</td></tr>
<tr><th id="745">745</th><td>  <i>// Assuming *matrix is kNeonVectorAlignment-byte aligned, every row of the</i></td></tr>
<tr><th id="746">746</th><td><i>  // matrix is also kNeonVectorAlignment-byte aligned as long as cols is a</i></td></tr>
<tr><th id="747">747</th><td><i>  // multiple of kNeonVectorAlignment. The assumption is currently satisfied by</i></td></tr>
<tr><th id="748">748</th><td><i>  // TFLite's 16-byte memory alignment scheme.</i></td></tr>
<tr><th id="749">749</th><td><i>  //</i></td></tr>
<tr><th id="750">750</th><td><i>  // Otherwise, we allocate an aligned memory block and set</i></td></tr>
<tr><th id="751">751</th><td><i>  // a flag to later copy rows from matrix to the block</i></td></tr>
<tr><th id="752">752</th><td><i>  // for aligned multiplication.</i></td></tr>
<tr><th id="753">753</th><td>  <em>bool</em> unaligned = <b>false</b>;</td></tr>
<tr><th id="754">754</th><td>  int8_t* aligned_row = <b>nullptr</b>;</td></tr>
<tr><th id="755">755</th><td>  <em>void</em>* aligned_row_free = <b>nullptr</b>;</td></tr>
<tr><th id="756">756</th><td>  <b>if</b> ((n_input &amp; (kNeonVectorAlignment - <var>1</var>)) != <var>0</var>) {</td></tr>
<tr><th id="757">757</th><td>    unaligned = <b>true</b>;</td></tr>
<tr><th id="758">758</th><td>    aligned_row =</td></tr>
<tr><th id="759">759</th><td>        (int8_t*)aligned_alloc(kNeonVectorAlignment, n_input,  <i>// NOLINT</i></td></tr>
<tr><th id="760">760</th><td>                               &amp;aligned_row_free);</td></tr>
<tr><th id="761">761</th><td>  }</td></tr>
<tr><th id="762">762</th><td>  <em>void</em>* aligned_vec_free = <b>nullptr</b>;</td></tr>
<tr><th id="763">763</th><td>  int8_t* aligned_vec =</td></tr>
<tr><th id="764">764</th><td>      (int8_t*)aligned_alloc(kNeonVectorAlignment, n_input,  <i>// NOLINT</i></td></tr>
<tr><th id="765">765</th><td>                             &amp;aligned_vec_free);</td></tr>
<tr><th id="766">766</th><td></td></tr>
<tr><th id="767">767</th><td>  <i>// If m_cols is not at least kInt8ValuesPerNeonVector, we cannot use the main</i></td></tr>
<tr><th id="768">768</th><td><i>  // vectorized loop, and we need to process sequentially. postamble_half_start</i></td></tr>
<tr><th id="769">769</th><td><i>  // shows the start index where this should happen. Between postamble_start and</i></td></tr>
<tr><th id="770">770</th><td><i>  // postamble_half_start we can still process kInt8ValuesPerNeonVector/2 in a</i></td></tr>
<tr><th id="771">771</th><td><i>  // vectorized form.</i></td></tr>
<tr><th id="772">772</th><td>  <em>const</em> <em>int</em> postamble_half_start =</td></tr>
<tr><th id="773">773</th><td>      RoundDownVectors&lt;kInt8ValuesPerNeonVector&gt;(n_input);</td></tr>
<tr><th id="774">774</th><td>  <em>const</em> <em>int</em> postamble_start =</td></tr>
<tr><th id="775">775</th><td>      RoundDownVectors&lt;(kInt8ValuesPerNeonVector / <var>2</var>)&gt;(n_input);</td></tr>
<tr><th id="776">776</th><td></td></tr>
<tr><th id="777">777</th><td>  <b>for</b> (<em>int</em> batch = <var>0</var>; batch &lt; n_batch; ++batch) {</td></tr>
<tr><th id="778">778</th><td>    <i>// Copy the vector data to an aligned vector.</i></td></tr>
<tr><th id="779">779</th><td>    memcpy(aligned_vec, input + batch * n_input, <b>sizeof</b>(int8_t) * n_input);</td></tr>
<tr><th id="780">780</th><td>    <i>// Compute dot-product for every column.</i></td></tr>
<tr><th id="781">781</th><td>    <b>for</b> (<em>int</em> row = <var>0</var>; row &lt; n_output; ++row) {</td></tr>
<tr><th id="782">782</th><td>      <i>// Get the address of the first element of the row.</i></td></tr>
<tr><th id="783">783</th><td>      int8_t* row_ptr =</td></tr>
<tr><th id="784">784</th><td>          (int8_t*)input_to_gate_weights + row * n_input;  <i>// NOLINT</i></td></tr>
<tr><th id="785">785</th><td>      <b>if</b> (unaligned) {</td></tr>
<tr><th id="786">786</th><td>        memcpy(aligned_row, row_ptr, <b>sizeof</b>(int8_t) * n_input);</td></tr>
<tr><th id="787">787</th><td>        row_ptr = aligned_row;</td></tr>
<tr><th id="788">788</th><td>      }</td></tr>
<tr><th id="789">789</th><td></td></tr>
<tr><th id="790">790</th><td>      <i>// Initialize the dot product sum for the row to 0.</i></td></tr>
<tr><th id="791">791</th><td>      int32x4_t dotprod_32x4 = vmovq_n_s32(<var>0</var>);</td></tr>
<tr><th id="792">792</th><td></td></tr>
<tr><th id="793">793</th><td>      <i>// For every block of 16 8-bit elements.</i></td></tr>
<tr><th id="794">794</th><td>      <em>int</em> col = <var>0</var>;</td></tr>
<tr><th id="795">795</th><td>      <b>for</b> (; col &lt; postamble_half_start; col += kInt8ValuesPerNeonVector) {</td></tr>
<tr><th id="796">796</th><td>        <i>// Load 16 8-bit values from the row and vector, each, to operate on.</i></td></tr>
<tr><th id="797">797</th><td><i>        // Here the assumption is that each buffer is 4-byte aligned. Otherwise,</i></td></tr>
<tr><th id="798">798</th><td><i>        // performance may suffer significantly.</i></td></tr>
<tr><th id="799">799</th><td>        TFLITE_DCHECK_EQ(  <i>// NOLINT</i></td></tr>
<tr><th id="800">800</th><td>            (uintptr_t)(&amp;row_ptr[col]) &amp; (kNeonVectorAlignment - <var>1</var>), <var>0</var>);</td></tr>
<tr><th id="801">801</th><td>        <em>const</em> int8x16_t s1_8x16 = vld1q_s8((<em>const</em> int8_t*)(aligned_vec + col));</td></tr>
<tr><th id="802">802</th><td>        <em>const</em> int8x16_t s2_8x16 = vld1q_s8((<em>const</em> int8_t*)(row_ptr + col));</td></tr>
<tr><th id="803">803</th><td>        <i>// Multiply the low bits (i.e. the lower 8 8bit numbers in the</i></td></tr>
<tr><th id="804">804</th><td><i>        // registers).</i></td></tr>
<tr><th id="805">805</th><td>        int16x8_t prod_16x8 =</td></tr>
<tr><th id="806">806</th><td>            vmull_s8(vget_low_s8(s1_8x16), vget_low_s8(s2_8x16));</td></tr>
<tr><th id="807">807</th><td>        <i>// Multiply the high bits (i.e. the higher 8 8bit numbers in the</i></td></tr>
<tr><th id="808">808</th><td><i>        // registers), and accumulate with the result of the low bits product.</i></td></tr>
<tr><th id="809">809</th><td><i>        // The assumption here is that overflow will not happen as we quantize</i></td></tr>
<tr><th id="810">810</th><td><i>        // our values to be in the range [-127, 127]. As such the sum of the 2</i></td></tr>
<tr><th id="811">811</th><td><i>        // products is always strictly smaller than 15-bits (32767 in absolute</i></td></tr>
<tr><th id="812">812</th><td><i>        // value).</i></td></tr>
<tr><th id="813">813</th><td>        prod_16x8 =</td></tr>
<tr><th id="814">814</th><td>            vmlal_s8(prod_16x8, vget_high_s8(s1_8x16), vget_high_s8(s2_8x16));</td></tr>
<tr><th id="815">815</th><td></td></tr>
<tr><th id="816">816</th><td>        dotprod_32x4 = vpadalq_s16(dotprod_32x4, prod_16x8);</td></tr>
<tr><th id="817">817</th><td>      }  <i>// for col</i></td></tr>
<tr><th id="818">818</th><td></td></tr>
<tr><th id="819">819</th><td>      <i>// Half iteration dealing only 8 elements</i></td></tr>
<tr><th id="820">820</th><td>      <b>if</b> (TFLITE_UNLIKELY(col &lt; postamble_start)) {</td></tr>
<tr><th id="821">821</th><td>        <i>// Load 8 8-bit values from the row and column each to operate on.</i></td></tr>
<tr><th id="822">822</th><td><i>        // Here the assumption is that each buffer is 4-bytes aligned.</i></td></tr>
<tr><th id="823">823</th><td><i>        // Otherwise, performance may suffer significantly.</i></td></tr>
<tr><th id="824">824</th><td>        TFLITE_DCHECK_EQ(  <i>// NOLINT</i></td></tr>
<tr><th id="825">825</th><td>            (uintptr_t)(&amp;row_ptr[col]) &amp; (kNeonVectorAlignment - <var>1</var>), <var>0</var>);</td></tr>
<tr><th id="826">826</th><td>        <em>const</em> int8x8_t s1_8x8 = vld1_s8((<em>const</em> int8_t*)(aligned_vec + col));</td></tr>
<tr><th id="827">827</th><td>        <em>const</em> int8x8_t s2_8x8 = vld1_s8((<em>const</em> int8_t*)(row_ptr + col));</td></tr>
<tr><th id="828">828</th><td>        <em>const</em> int16x8_t prod_16x8 = vmull_s8(s1_8x8, s2_8x8);</td></tr>
<tr><th id="829">829</th><td>        dotprod_32x4 = vpadalq_s16(dotprod_32x4, prod_16x8);</td></tr>
<tr><th id="830">830</th><td>        col += (kInt8ValuesPerNeonVector &gt;&gt; <var>1</var>);</td></tr>
<tr><th id="831">831</th><td>      }</td></tr>
<tr><th id="832">832</th><td>      <i>// Add the 4 intermediate sum values to get the final dot-prod value for</i></td></tr>
<tr><th id="833">833</th><td><i>      // this row.</i></td></tr>
<tr><th id="834">834</th><td>      int32_t dotprod = AccumulateNeonLane(dotprod_32x4);</td></tr>
<tr><th id="835">835</th><td>      <i>// Postamble loop.</i></td></tr>
<tr><th id="836">836</th><td>      <b>for</b> (; TFLITE_UNLIKELY(col &lt; n_input); ++col) {</td></tr>
<tr><th id="837">837</th><td>        dotprod += row_ptr[col] * aligned_vec[col];</td></tr>
<tr><th id="838">838</th><td>      }  <i>// for col</i></td></tr>
<tr><th id="839">839</th><td></td></tr>
<tr><th id="840">840</th><td>      dotprod += bias[row];</td></tr>
<tr><th id="841">841</th><td>      scratch[batch * n_output + row] = dotprod;</td></tr>
<tr><th id="842">842</th><td>    }  <i>// for row</i></td></tr>
<tr><th id="843">843</th><td>  }    <i>// for batch</i></td></tr>
<tr><th id="844">844</th><td></td></tr>
<tr><th id="845">845</th><td>  <b>if</b> (unaligned) {</td></tr>
<tr><th id="846">846</th><td>    free(aligned_row_free);</td></tr>
<tr><th id="847">847</th><td>  }</td></tr>
<tr><th id="848">848</th><td>  free(aligned_vec_free);</td></tr>
<tr><th id="849">849</th><td>}</td></tr>
<tr><th id="850">850</th><td></td></tr>
<tr><th id="851">851</th><td><b>inline</b> <em>void</em> NeonMatrixBatchVectorAccumulateImpl(</td></tr>
<tr><th id="852">852</th><td>    int32_t multiplier, int32_t shift, int32_t n_batch, int32_t n_output,</td></tr>
<tr><th id="853">853</th><td>    int32_t output_zp, int32_t* scratch, int16_t* output) {</td></tr>
<tr><th id="854">854</th><td>  <em>int</em> i = <var>0</var>;</td></tr>
<tr><th id="855">855</th><td>  <em>const</em> <em>int</em> total_size = n_batch * n_output;</td></tr>
<tr><th id="856">856</th><td></td></tr>
<tr><th id="857">857</th><td>  <em>const</em> int32_t output_min = std::numeric_limits&lt;int16_t&gt;::min();</td></tr>
<tr><th id="858">858</th><td>  <em>const</em> int32_t output_max = std::numeric_limits&lt;int16_t&gt;::max();</td></tr>
<tr><th id="859">859</th><td></td></tr>
<tr><th id="860">860</th><td>  <em>const</em> int32x4_t output_zp_dup = vdupq_n_s32(output_zp);</td></tr>
<tr><th id="861">861</th><td>  <em>const</em> int32x4_t max_val_dup = vdupq_n_s32(output_max);</td></tr>
<tr><th id="862">862</th><td>  <em>const</em> int32x4_t min_val_dup = vdupq_n_s32(output_min);</td></tr>
<tr><th id="863">863</th><td></td></tr>
<tr><th id="864">864</th><td>  <b>using</b> gemmlowp::RoundingDivideByPOT;</td></tr>
<tr><th id="865">865</th><td>  <b>using</b> gemmlowp::SaturatingRoundingDoublingHighMul;</td></tr>
<tr><th id="866">866</th><td></td></tr>
<tr><th id="867">867</th><td>  <b>for</b> (; i &lt;= total_size - <var>8</var>; i += <var>8</var>) {</td></tr>
<tr><th id="868">868</th><td>    int32x4x2_t scratch_val;</td></tr>
<tr><th id="869">869</th><td>    scratch_val.val[<var>0</var>] = vld1q_s32(scratch + i);</td></tr>
<tr><th id="870">870</th><td>    scratch_val.val[<var>1</var>] = vld1q_s32(scratch + i + <var>4</var>);</td></tr>
<tr><th id="871">871</th><td>    <em>const</em> int16x8_t output_val = vld1q_s16(output + i);</td></tr>
<tr><th id="872">872</th><td>    <em>const</em> int32x4_t first_half = vmovl_s16(vget_low_s16(output_val));</td></tr>
<tr><th id="873">873</th><td>    <em>const</em> int32x4_t second_half = vmovl_s16(vget_high_s16(output_val));</td></tr>
<tr><th id="874">874</th><td></td></tr>
<tr><th id="875">875</th><td>    int32x4x2_t temp_val =</td></tr>
<tr><th id="876">876</th><td>        MultiplyByQuantizedMultiplier2Rows(scratch_val, multiplier, shift);</td></tr>
<tr><th id="877">877</th><td></td></tr>
<tr><th id="878">878</th><td>    temp_val.val[<var>0</var>] =</td></tr>
<tr><th id="879">879</th><td>        vaddq_s32(vaddq_s32(temp_val.val[<var>0</var>], first_half), output_zp_dup);</td></tr>
<tr><th id="880">880</th><td>    temp_val.val[<var>1</var>] =</td></tr>
<tr><th id="881">881</th><td>        vaddq_s32(vaddq_s32(temp_val.val[<var>1</var>], second_half), output_zp_dup);</td></tr>
<tr><th id="882">882</th><td>    temp_val.val[<var>0</var>] =</td></tr>
<tr><th id="883">883</th><td>        vmaxq_s32(vminq_s32(temp_val.val[<var>0</var>], max_val_dup), min_val_dup);</td></tr>
<tr><th id="884">884</th><td>    temp_val.val[<var>1</var>] =</td></tr>
<tr><th id="885">885</th><td>        vmaxq_s32(vminq_s32(temp_val.val[<var>1</var>], max_val_dup), min_val_dup);</td></tr>
<tr><th id="886">886</th><td>    <em>const</em> int16x8_t result =</td></tr>
<tr><th id="887">887</th><td>        vcombine_s16(vqmovn_s32(temp_val.val[<var>0</var>]), vqmovn_s32(temp_val.val[<var>1</var>]));</td></tr>
<tr><th id="888">888</th><td>    vst1q_s16(output + i, result);</td></tr>
<tr><th id="889">889</th><td>  }</td></tr>
<tr><th id="890">890</th><td>  <b>for</b> (; TFLITE_UNLIKELY(i &lt; total_size); ++i) {</td></tr>
<tr><th id="891">891</th><td>    int32_t temp = MultiplyByQuantizedMultiplier(scratch[i], multiplier, shift);</td></tr>
<tr><th id="892">892</th><td>    temp += output_zp;</td></tr>
<tr><th id="893">893</th><td>    temp += output[i];</td></tr>
<tr><th id="894">894</th><td>    <b>if</b> (temp &gt; output_max) {</td></tr>
<tr><th id="895">895</th><td>      temp = output_max;</td></tr>
<tr><th id="896">896</th><td>    }</td></tr>
<tr><th id="897">897</th><td>    <b>if</b> (temp &lt; output_min) {</td></tr>
<tr><th id="898">898</th><td>      temp = output_min;</td></tr>
<tr><th id="899">899</th><td>    }</td></tr>
<tr><th id="900">900</th><td>    output[i] = <b>static_cast</b>&lt;int16_t&gt;(temp);</td></tr>
<tr><th id="901">901</th><td>  }</td></tr>
<tr><th id="902">902</th><td>}</td></tr>
<tr><th id="903">903</th><td></td></tr>
<tr><th id="904">904</th><td><b>inline</b> <em>void</em> NeonMatrixBatchVectorAccumulateImpl(</td></tr>
<tr><th id="905">905</th><td>    int32_t multiplier, int32_t shift, int32_t n_batch, int32_t n_output,</td></tr>
<tr><th id="906">906</th><td>    int32_t output_zp, int32_t* scratch, int8_t* output) {</td></tr>
<tr><th id="907">907</th><td>  <em>int</em> i = <var>0</var>;</td></tr>
<tr><th id="908">908</th><td>  <em>const</em> <em>int</em> total_size = n_batch * n_output;</td></tr>
<tr><th id="909">909</th><td></td></tr>
<tr><th id="910">910</th><td>  <em>const</em> int32_t output_min = std::numeric_limits&lt;int8_t&gt;::min();</td></tr>
<tr><th id="911">911</th><td>  <em>const</em> int32_t output_max = std::numeric_limits&lt;int8_t&gt;::max();</td></tr>
<tr><th id="912">912</th><td></td></tr>
<tr><th id="913">913</th><td>  <em>const</em> int32x4_t output_zp_dup = vdupq_n_s32(output_zp);</td></tr>
<tr><th id="914">914</th><td>  <em>const</em> int32x4_t max_val_dup = vdupq_n_s32(output_max);</td></tr>
<tr><th id="915">915</th><td>  <em>const</em> int32x4_t min_val_dup = vdupq_n_s32(output_min);</td></tr>
<tr><th id="916">916</th><td></td></tr>
<tr><th id="917">917</th><td>  <b>using</b> gemmlowp::RoundingDivideByPOT;</td></tr>
<tr><th id="918">918</th><td>  <b>using</b> gemmlowp::SaturatingRoundingDoublingHighMul;</td></tr>
<tr><th id="919">919</th><td></td></tr>
<tr><th id="920">920</th><td>  <b>for</b> (; i &lt;= total_size - <var>16</var>; i += <var>16</var>) {</td></tr>
<tr><th id="921">921</th><td>    int32x4x4_t scratch_val;</td></tr>
<tr><th id="922">922</th><td>    scratch_val.val[<var>0</var>] = vld1q_s32(scratch + i);</td></tr>
<tr><th id="923">923</th><td>    scratch_val.val[<var>1</var>] = vld1q_s32(scratch + i + <var>4</var>);</td></tr>
<tr><th id="924">924</th><td>    scratch_val.val[<var>2</var>] = vld1q_s32(scratch + i + <var>8</var>);</td></tr>
<tr><th id="925">925</th><td>    scratch_val.val[<var>3</var>] = vld1q_s32(scratch + i + <var>12</var>);</td></tr>
<tr><th id="926">926</th><td></td></tr>
<tr><th id="927">927</th><td>    <em>const</em> int8x16_t output_val = vld1q_s8(output + i);</td></tr>
<tr><th id="928">928</th><td>    <em>const</em> int16x8_t first_half = vmovl_s8(vget_low_s8(output_val));</td></tr>
<tr><th id="929">929</th><td>    <em>const</em> int16x8_t second_half = vmovl_s8(vget_high_s8(output_val));</td></tr>
<tr><th id="930">930</th><td>    <em>const</em> int32x4_t output_val_1 = vmovl_s16(vget_low_s16(first_half));</td></tr>
<tr><th id="931">931</th><td>    <em>const</em> int32x4_t output_val_2 = vmovl_s16(vget_high_s16(first_half));</td></tr>
<tr><th id="932">932</th><td>    <em>const</em> int32x4_t output_val_3 = vmovl_s16(vget_low_s16(second_half));</td></tr>
<tr><th id="933">933</th><td>    <em>const</em> int32x4_t output_val_4 = vmovl_s16(vget_high_s16(second_half));</td></tr>
<tr><th id="934">934</th><td></td></tr>
<tr><th id="935">935</th><td>    int32x4x4_t temp_val =</td></tr>
<tr><th id="936">936</th><td>        MultiplyByQuantizedMultiplier4Rows(scratch_val, multiplier, shift);</td></tr>
<tr><th id="937">937</th><td></td></tr>
<tr><th id="938">938</th><td>    temp_val.val[<var>0</var>] =</td></tr>
<tr><th id="939">939</th><td>        vaddq_s32(vaddq_s32(temp_val.val[<var>0</var>], output_val_1), output_zp_dup);</td></tr>
<tr><th id="940">940</th><td>    temp_val.val[<var>1</var>] =</td></tr>
<tr><th id="941">941</th><td>        vaddq_s32(vaddq_s32(temp_val.val[<var>1</var>], output_val_2), output_zp_dup);</td></tr>
<tr><th id="942">942</th><td>    temp_val.val[<var>2</var>] =</td></tr>
<tr><th id="943">943</th><td>        vaddq_s32(vaddq_s32(temp_val.val[<var>2</var>], output_val_3), output_zp_dup);</td></tr>
<tr><th id="944">944</th><td>    temp_val.val[<var>3</var>] =</td></tr>
<tr><th id="945">945</th><td>        vaddq_s32(vaddq_s32(temp_val.val[<var>3</var>], output_val_4), output_zp_dup);</td></tr>
<tr><th id="946">946</th><td></td></tr>
<tr><th id="947">947</th><td>    temp_val.val[<var>0</var>] =</td></tr>
<tr><th id="948">948</th><td>        vmaxq_s32(vminq_s32(temp_val.val[<var>0</var>], max_val_dup), min_val_dup);</td></tr>
<tr><th id="949">949</th><td>    temp_val.val[<var>1</var>] =</td></tr>
<tr><th id="950">950</th><td>        vmaxq_s32(vminq_s32(temp_val.val[<var>1</var>], max_val_dup), min_val_dup);</td></tr>
<tr><th id="951">951</th><td>    temp_val.val[<var>2</var>] =</td></tr>
<tr><th id="952">952</th><td>        vmaxq_s32(vminq_s32(temp_val.val[<var>2</var>], max_val_dup), min_val_dup);</td></tr>
<tr><th id="953">953</th><td>    temp_val.val[<var>3</var>] =</td></tr>
<tr><th id="954">954</th><td>        vmaxq_s32(vminq_s32(temp_val.val[<var>3</var>], max_val_dup), min_val_dup);</td></tr>
<tr><th id="955">955</th><td></td></tr>
<tr><th id="956">956</th><td>    <em>const</em> int16x8_t result_1 =</td></tr>
<tr><th id="957">957</th><td>        vcombine_s16(vqmovn_s32(temp_val.val[<var>0</var>]), vqmovn_s32(temp_val.val[<var>1</var>]));</td></tr>
<tr><th id="958">958</th><td>    <em>const</em> int16x8_t result_2 =</td></tr>
<tr><th id="959">959</th><td>        vcombine_s16(vqmovn_s32(temp_val.val[<var>2</var>]), vqmovn_s32(temp_val.val[<var>3</var>]));</td></tr>
<tr><th id="960">960</th><td>    <em>const</em> int8x16_t result =</td></tr>
<tr><th id="961">961</th><td>        vcombine_s8(vqmovn_s16(result_1), vqmovn_s16(result_2));</td></tr>
<tr><th id="962">962</th><td>    vst1q_s8(output + i, result);</td></tr>
<tr><th id="963">963</th><td>  }</td></tr>
<tr><th id="964">964</th><td>  <b>for</b> (; TFLITE_UNLIKELY(i &lt; total_size); ++i) {</td></tr>
<tr><th id="965">965</th><td>    int32_t temp = MultiplyByQuantizedMultiplier(scratch[i], multiplier, shift);</td></tr>
<tr><th id="966">966</th><td>    temp += output_zp;</td></tr>
<tr><th id="967">967</th><td>    temp += output[i];</td></tr>
<tr><th id="968">968</th><td>    <b>if</b> (temp &gt; output_max) {</td></tr>
<tr><th id="969">969</th><td>      temp = output_max;</td></tr>
<tr><th id="970">970</th><td>    }</td></tr>
<tr><th id="971">971</th><td>    <b>if</b> (temp &lt; output_min) {</td></tr>
<tr><th id="972">972</th><td>      temp = output_min;</td></tr>
<tr><th id="973">973</th><td>    }</td></tr>
<tr><th id="974">974</th><td>    output[i] = <b>static_cast</b>&lt;int8_t&gt;(temp);</td></tr>
<tr><th id="975">975</th><td>  }</td></tr>
<tr><th id="976">976</th><td>}</td></tr>
<tr><th id="977">977</th><td></td></tr>
<tr><th id="978">978</th><td><em>void</em> NeonCpuBackendGemm(<em>const</em> int8_t* input, <em>const</em> int32_t* bias,</td></tr>
<tr><th id="979">979</th><td>                        <em>const</em> int8_t* input_to_gate_weights, int32_t n_batch,</td></tr>
<tr><th id="980">980</th><td>                        int32_t n_input, int32_t n_output, int32_t output_zp,</td></tr>
<tr><th id="981">981</th><td>                        int32_t* scratch, CpuBackendContext* context) {</td></tr>
<tr><th id="982">982</th><td>  <b>using</b> ::tflite::cpu_backend_gemm::Gemm;</td></tr>
<tr><th id="983">983</th><td>  <b>using</b> ::tflite::cpu_backend_gemm::GemmParams;</td></tr>
<tr><th id="984">984</th><td>  <b>using</b> ::tflite::cpu_backend_gemm::MatrixParams;</td></tr>
<tr><th id="985">985</th><td></td></tr>
<tr><th id="986">986</th><td>  MatrixParams&lt;int8_t&gt; lhs_params;</td></tr>
<tr><th id="987">987</th><td>  lhs_params.order = cpu_backend_gemm::Order::kRowMajor;</td></tr>
<tr><th id="988">988</th><td>  lhs_params.rows = n_output;</td></tr>
<tr><th id="989">989</th><td>  lhs_params.cols = n_input;</td></tr>
<tr><th id="990">990</th><td>  lhs_params.cache_policy = cpu_backend_gemm::CachePolicy::kCacheIfLargeSpeedup;</td></tr>
<tr><th id="991">991</th><td></td></tr>
<tr><th id="992">992</th><td>  MatrixParams&lt;int8_t&gt; rhs_params;</td></tr>
<tr><th id="993">993</th><td>  rhs_params.order = cpu_backend_gemm::Order::kColMajor;</td></tr>
<tr><th id="994">994</th><td>  rhs_params.rows = n_input;</td></tr>
<tr><th id="995">995</th><td>  rhs_params.cols = n_batch;</td></tr>
<tr><th id="996">996</th><td></td></tr>
<tr><th id="997">997</th><td>  MatrixParams&lt;int32_t&gt; dst_params;</td></tr>
<tr><th id="998">998</th><td>  dst_params.order = cpu_backend_gemm::Order::kColMajor;</td></tr>
<tr><th id="999">999</th><td>  dst_params.rows = n_output;</td></tr>
<tr><th id="1000">1000</th><td>  dst_params.cols = n_batch;</td></tr>
<tr><th id="1001">1001</th><td></td></tr>
<tr><th id="1002">1002</th><td>  GemmParams&lt;int32, int32&gt; gemm_params;</td></tr>
<tr><th id="1003">1003</th><td>  <b>if</b> (bias) {</td></tr>
<tr><th id="1004">1004</th><td>    gemm_params.bias = bias;</td></tr>
<tr><th id="1005">1005</th><td>  }</td></tr>
<tr><th id="1006">1006</th><td>  cpu_backend_gemm::Gemm(lhs_params, input_to_gate_weights, rhs_params, input,</td></tr>
<tr><th id="1007">1007</th><td>                         dst_params, scratch, gemm_params, context);</td></tr>
<tr><th id="1008">1008</th><td>}</td></tr>
<tr><th id="1009">1009</th><td></td></tr>
<tr><th id="1010">1010</th><td><em>void</em> NeonMatrixBatchVectorMultiplyAccumulate(</td></tr>
<tr><th id="1011">1011</th><td>    <em>const</em> int8_t* input, <em>const</em> int32_t* bias,</td></tr>
<tr><th id="1012">1012</th><td>    <em>const</em> int8_t* input_to_gate_weights, int32_t multiplier, int32_t shift,</td></tr>
<tr><th id="1013">1013</th><td>    int32_t n_batch, int32_t n_input, int32_t n_output, int32_t output_zp,</td></tr>
<tr><th id="1014">1014</th><td>    int32_t* scratch, int16_t* output, CpuBackendContext* context) {</td></tr>
<tr><th id="1015">1015</th><td><u>#ifdef TFLITE_WITH_RUY_GEMV</u></td></tr>
<tr><th id="1016">1016</th><td>  NeonCpuBackendGemm(input, bias, input_to_gate_weights, n_batch, n_input,</td></tr>
<tr><th id="1017">1017</th><td>                     n_output, output_zp, scratch, context);</td></tr>
<tr><th id="1018">1018</th><td><u>#else</u></td></tr>
<tr><th id="1019">1019</th><td>  NeonMatrixBatchVectorMultiplyImpl(input, bias, input_to_gate_weights, n_batch,</td></tr>
<tr><th id="1020">1020</th><td>                                    n_input, n_output, output_zp, scratch);</td></tr>
<tr><th id="1021">1021</th><td><u>#endif</u></td></tr>
<tr><th id="1022">1022</th><td>  NeonMatrixBatchVectorAccumulateImpl(multiplier, shift, n_batch, n_output,</td></tr>
<tr><th id="1023">1023</th><td>                                      output_zp, scratch, output);</td></tr>
<tr><th id="1024">1024</th><td>}</td></tr>
<tr><th id="1025">1025</th><td></td></tr>
<tr><th id="1026">1026</th><td><em>void</em> NeonMatrixBatchVectorMultiplyAccumulate(</td></tr>
<tr><th id="1027">1027</th><td>    <em>const</em> int8_t* input, <em>const</em> int32_t* bias,</td></tr>
<tr><th id="1028">1028</th><td>    <em>const</em> int8_t* input_to_gate_weights, int32_t multiplier, int32_t shift,</td></tr>
<tr><th id="1029">1029</th><td>    int32_t n_batch, int32_t n_input, int32_t n_output, int32_t output_zp,</td></tr>
<tr><th id="1030">1030</th><td>    int32_t* scratch, int8_t* output, CpuBackendContext* context) {</td></tr>
<tr><th id="1031">1031</th><td><u>#ifdef TFLITE_WITH_RUY_GEMV</u></td></tr>
<tr><th id="1032">1032</th><td>  NeonCpuBackendGemm(input, bias, input_to_gate_weights, n_batch, n_input,</td></tr>
<tr><th id="1033">1033</th><td>                     n_output, output_zp, scratch, context);</td></tr>
<tr><th id="1034">1034</th><td><u>#else</u></td></tr>
<tr><th id="1035">1035</th><td>  NeonMatrixBatchVectorMultiplyImpl(input, bias, input_to_gate_weights, n_batch,</td></tr>
<tr><th id="1036">1036</th><td>                                    n_input, n_output, output_zp, scratch);</td></tr>
<tr><th id="1037">1037</th><td><u>#endif</u></td></tr>
<tr><th id="1038">1038</th><td>  NeonMatrixBatchVectorAccumulateImpl(multiplier, shift, n_batch, n_output,</td></tr>
<tr><th id="1039">1039</th><td>                                      output_zp, scratch, output);</td></tr>
<tr><th id="1040">1040</th><td>}</td></tr>
<tr><th id="1041">1041</th><td></td></tr>
<tr><th id="1042">1042</th><td><em>void</em> NeonMatrixBatchVectorMultiplyAccumulate(<em>const</em> int8_t* <b>__restrict__</b> matrix,</td></tr>
<tr><th id="1043">1043</th><td>                                             <em>const</em> <em>int</em> m_rows, <em>const</em> <em>int</em> m_cols,</td></tr>
<tr><th id="1044">1044</th><td>                                             <em>const</em> int8_t* <b>__restrict__</b> vectors,</td></tr>
<tr><th id="1045">1045</th><td>                                             <em>const</em> <em>float</em>* scaling_factors,</td></tr>
<tr><th id="1046">1046</th><td>                                             <em>int</em> n_batch,</td></tr>
<tr><th id="1047">1047</th><td>                                             <em>float</em>* <b>__restrict__</b> result) {</td></tr>
<tr><th id="1048">1048</th><td><u>#ifdef __aarch64__</u></td></tr>
<tr><th id="1049">1049</th><td>  <b>if</b> (HasSdotInstruction() &amp;&amp; m_cols % <var>16</var> == <var>0</var> &amp;&amp; m_rows % <var>2</var> == <var>0</var> &amp;&amp;</td></tr>
<tr><th id="1050">1050</th><td>      m_rows &gt;= n_batch) {</td></tr>
<tr><th id="1051">1051</th><td>    <b>if</b> (n_batch % <var>4</var> == <var>0</var>) {</td></tr>
<tr><th id="1052">1052</th><td>      <i>// Benchmarks suggest that it's always better to use the batch code</i></td></tr>
<tr><th id="1053">1053</th><td><i>      // when we can, even on small matrices.</i></td></tr>
<tr><th id="1054">1054</th><td>      DotprodMatrixBatchFourVectorMultiplyAccumulate(</td></tr>
<tr><th id="1055">1055</th><td>          matrix, m_rows, m_cols, vectors, scaling_factors, n_batch, result);</td></tr>
<tr><th id="1056">1056</th><td>      <b>return</b>;</td></tr>
<tr><th id="1057">1057</th><td>    } <b>else</b> <b>if</b> (n_batch &gt;= <var>2</var> &amp;&amp; m_rows * m_cols &gt;= <var>128</var> * <var>128</var>) {</td></tr>
<tr><th id="1058">1058</th><td>      DotprodMatrixBatchPaddedFourVectorMultiplyAccumulate(</td></tr>
<tr><th id="1059">1059</th><td>          matrix, m_rows, m_cols, vectors, scaling_factors, n_batch, result);</td></tr>
<tr><th id="1060">1060</th><td>      <b>return</b>;</td></tr>
<tr><th id="1061">1061</th><td>    }</td></tr>
<tr><th id="1062">1062</th><td>  }</td></tr>
<tr><th id="1063">1063</th><td><u>#endif  // __aarch64__</u></td></tr>
<tr><th id="1064">1064</th><td></td></tr>
<tr><th id="1065">1065</th><td>  <i>// Assuming *matrix is kNeonVectorAlignment-byte aligned, every row of the</i></td></tr>
<tr><th id="1066">1066</th><td><i>  // matrix is also kNeonVectorAlignment-byte aligned as long as cols is a</i></td></tr>
<tr><th id="1067">1067</th><td><i>  // multiple of kNeonVectorAlignment. The assumption is currently satisfied by</i></td></tr>
<tr><th id="1068">1068</th><td><i>  // TFLite's 16-byte memory alignment scheme.</i></td></tr>
<tr><th id="1069">1069</th><td><i>  //</i></td></tr>
<tr><th id="1070">1070</th><td><i>  // Otherwise, we allocate an aligned memory block and set</i></td></tr>
<tr><th id="1071">1071</th><td><i>  // a flag to later copy rows from matrix to the block</i></td></tr>
<tr><th id="1072">1072</th><td><i>  // for aligned multiplication.</i></td></tr>
<tr><th id="1073">1073</th><td>  <em>bool</em> unaligned = <b>false</b>;</td></tr>
<tr><th id="1074">1074</th><td>  int8_t* aligned_row = <b>nullptr</b>;</td></tr>
<tr><th id="1075">1075</th><td>  <em>void</em>* aligned_row_free = <b>nullptr</b>;</td></tr>
<tr><th id="1076">1076</th><td>  <b>if</b> ((m_cols &amp; (kNeonVectorAlignment - <var>1</var>)) != <var>0</var>) {</td></tr>
<tr><th id="1077">1077</th><td>    unaligned = <b>true</b>;</td></tr>
<tr><th id="1078">1078</th><td>    aligned_row =</td></tr>
<tr><th id="1079">1079</th><td>        (int8_t*)aligned_alloc(kNeonVectorAlignment, m_cols,  <i>// NOLINT</i></td></tr>
<tr><th id="1080">1080</th><td>                               &amp;aligned_row_free);</td></tr>
<tr><th id="1081">1081</th><td>  }</td></tr>
<tr><th id="1082">1082</th><td>  <em>void</em>* aligned_vec_free = <b>nullptr</b>;</td></tr>
<tr><th id="1083">1083</th><td>  int8_t* aligned_vec =</td></tr>
<tr><th id="1084">1084</th><td>      (int8_t*)aligned_alloc(kNeonVectorAlignment, m_cols,  <i>// NOLINT</i></td></tr>
<tr><th id="1085">1085</th><td>                             &amp;aligned_vec_free);</td></tr>
<tr><th id="1086">1086</th><td></td></tr>
<tr><th id="1087">1087</th><td>  <i>// If m_cols is not at least kInt8ValuesPerNeonVector, we cannot use the main</i></td></tr>
<tr><th id="1088">1088</th><td><i>  // vectorized loop, and we need to process sequentially. postamble_half_start</i></td></tr>
<tr><th id="1089">1089</th><td><i>  // shows the start index where this should happen. Between postamble_start and</i></td></tr>
<tr><th id="1090">1090</th><td><i>  // postamble_half_start we can still process kInt8ValuesPerNeonVector/2 in a</i></td></tr>
<tr><th id="1091">1091</th><td><i>  // vectorized form.</i></td></tr>
<tr><th id="1092">1092</th><td>  <em>const</em> <em>int</em> postamble_half_start =</td></tr>
<tr><th id="1093">1093</th><td>      RoundDownVectors&lt;kInt8ValuesPerNeonVector&gt;(m_cols);</td></tr>
<tr><th id="1094">1094</th><td>  <em>const</em> <em>int</em> postamble_start =</td></tr>
<tr><th id="1095">1095</th><td>      RoundDownVectors&lt;(kInt8ValuesPerNeonVector / <var>2</var>)&gt;(m_cols);</td></tr>
<tr><th id="1096">1096</th><td></td></tr>
<tr><th id="1097">1097</th><td>  <b>for</b> (<em>int</em> batch = <var>0</var>; batch &lt; n_batch; ++batch) {</td></tr>
<tr><th id="1098">1098</th><td>    <em>const</em> <em>float</em> batch_scaling_factor = scaling_factors[batch];</td></tr>
<tr><th id="1099">1099</th><td>    <i>// Copy the vector data to an aligned vector.</i></td></tr>
<tr><th id="1100">1100</th><td>    memcpy(aligned_vec, vectors + batch * m_cols, <b>sizeof</b>(int8_t) * m_cols);</td></tr>
<tr><th id="1101">1101</th><td>    <i>// Compute dot-product for every column.</i></td></tr>
<tr><th id="1102">1102</th><td>    <b>for</b> (<em>int</em> row = <var>0</var>; row &lt; m_rows; ++row) {</td></tr>
<tr><th id="1103">1103</th><td>      <i>// Get the address of the first element of the row.</i></td></tr>
<tr><th id="1104">1104</th><td>      int8_t* row_ptr = (int8_t*)matrix + row * m_cols;  <i>// NOLINT</i></td></tr>
<tr><th id="1105">1105</th><td>      <b>if</b> (unaligned) {</td></tr>
<tr><th id="1106">1106</th><td>        memcpy(aligned_row, row_ptr, <b>sizeof</b>(int8_t) * m_cols);</td></tr>
<tr><th id="1107">1107</th><td>        row_ptr = aligned_row;</td></tr>
<tr><th id="1108">1108</th><td>      }</td></tr>
<tr><th id="1109">1109</th><td></td></tr>
<tr><th id="1110">1110</th><td>      <i>// Initialize the dot product sum for the row to 0.</i></td></tr>
<tr><th id="1111">1111</th><td>      int32x4_t dotprod_32x4 = vmovq_n_s32(<var>0</var>);</td></tr>
<tr><th id="1112">1112</th><td></td></tr>
<tr><th id="1113">1113</th><td>      <i>// Prefetch the row to cache.</i></td></tr>
<tr><th id="1114">1114</th><td>      __builtin_prefetch(row_ptr, <var>0</var> <i>/* prefetch for read */</i>,</td></tr>
<tr><th id="1115">1115</th><td>                         <var>3</var> <i>/* temporal locality */</i>);</td></tr>
<tr><th id="1116">1116</th><td></td></tr>
<tr><th id="1117">1117</th><td>      <i>// For every block of 16 8-bit elements.</i></td></tr>
<tr><th id="1118">1118</th><td>      <em>int</em> col = <var>0</var>;</td></tr>
<tr><th id="1119">1119</th><td>      <b>for</b> (; col &lt; postamble_half_start; col += kInt8ValuesPerNeonVector) {</td></tr>
<tr><th id="1120">1120</th><td>        <i>// Load 16 8-bit values from the row and vector, each, to operate on.</i></td></tr>
<tr><th id="1121">1121</th><td><i>        // Here the assumption is that each buffer is 4-byte aligned. Otherwise,</i></td></tr>
<tr><th id="1122">1122</th><td><i>        // performance may suffer significantly.</i></td></tr>
<tr><th id="1123">1123</th><td>        TFLITE_DCHECK_EQ(  <i>// NOLINT</i></td></tr>
<tr><th id="1124">1124</th><td>            (uintptr_t)(&amp;row_ptr[col]) &amp; (kNeonVectorAlignment - <var>1</var>), <var>0</var>);</td></tr>
<tr><th id="1125">1125</th><td>        <em>const</em> int8x16_t s1_8x16 = vld1q_s8((<em>const</em> int8_t*)(aligned_vec + col));</td></tr>
<tr><th id="1126">1126</th><td>        <em>const</em> int8x16_t s2_8x16 = vld1q_s8((<em>const</em> int8_t*)(row_ptr + col));</td></tr>
<tr><th id="1127">1127</th><td>        <i>// Multiply the low bits (i.e. the lower 8 8bit numbers in the</i></td></tr>
<tr><th id="1128">1128</th><td><i>        // registers).</i></td></tr>
<tr><th id="1129">1129</th><td>        int16x8_t prod_16x8 =</td></tr>
<tr><th id="1130">1130</th><td>            vmull_s8(vget_low_s8(s1_8x16), vget_low_s8(s2_8x16));</td></tr>
<tr><th id="1131">1131</th><td>        <i>// Multiply the high bits (i.e. the higher 8 8bit numbers in the</i></td></tr>
<tr><th id="1132">1132</th><td><i>        // registers), and accumulate with the result of the low bits product.</i></td></tr>
<tr><th id="1133">1133</th><td><i>        // The assumption here is that overflow will not happen as we quantize</i></td></tr>
<tr><th id="1134">1134</th><td><i>        // our values to be in the range [-127, 127]. As such the sum of the 2</i></td></tr>
<tr><th id="1135">1135</th><td><i>        // products is always strictly smaller than 15-bits (32767 in absolute</i></td></tr>
<tr><th id="1136">1136</th><td><i>        // value).</i></td></tr>
<tr><th id="1137">1137</th><td>        prod_16x8 =</td></tr>
<tr><th id="1138">1138</th><td>            vmlal_s8(prod_16x8, vget_high_s8(s1_8x16), vget_high_s8(s2_8x16));</td></tr>
<tr><th id="1139">1139</th><td></td></tr>
<tr><th id="1140">1140</th><td>        dotprod_32x4 = vpadalq_s16(dotprod_32x4, prod_16x8);</td></tr>
<tr><th id="1141">1141</th><td>      }  <i>// for col</i></td></tr>
<tr><th id="1142">1142</th><td></td></tr>
<tr><th id="1143">1143</th><td>      <i>// Half iteration dealing only 8 elements</i></td></tr>
<tr><th id="1144">1144</th><td>      <b>if</b> (TFLITE_UNLIKELY(col &lt; postamble_start)) {</td></tr>
<tr><th id="1145">1145</th><td>        <i>// Load 8 8-bit values from the row and column each to operate on.</i></td></tr>
<tr><th id="1146">1146</th><td><i>        // Here the assumption is that each buffer is 4-bytes aligned.</i></td></tr>
<tr><th id="1147">1147</th><td><i>        // Otherwise, performance may suffer significantly.</i></td></tr>
<tr><th id="1148">1148</th><td>        TFLITE_DCHECK_EQ(  <i>// NOLINT</i></td></tr>
<tr><th id="1149">1149</th><td>            (uintptr_t)(&amp;row_ptr[col]) &amp; (kNeonVectorAlignment - <var>1</var>), <var>0</var>);</td></tr>
<tr><th id="1150">1150</th><td>        <em>const</em> int8x8_t s1_8x8 = vld1_s8((<em>const</em> int8_t*)(aligned_vec + col));</td></tr>
<tr><th id="1151">1151</th><td>        <em>const</em> int8x8_t s2_8x8 = vld1_s8((<em>const</em> int8_t*)(row_ptr + col));</td></tr>
<tr><th id="1152">1152</th><td>        <em>const</em> int16x8_t prod_16x8 = vmull_s8(s1_8x8, s2_8x8);</td></tr>
<tr><th id="1153">1153</th><td>        dotprod_32x4 = vpadalq_s16(dotprod_32x4, prod_16x8);</td></tr>
<tr><th id="1154">1154</th><td>        col += (kInt8ValuesPerNeonVector &gt;&gt; <var>1</var>);</td></tr>
<tr><th id="1155">1155</th><td>      }</td></tr>
<tr><th id="1156">1156</th><td>      <i>// Add the 4 intermediate sum values to get the final dot-prod value for</i></td></tr>
<tr><th id="1157">1157</th><td><i>      // this row.</i></td></tr>
<tr><th id="1158">1158</th><td>      int32_t dotprod = AccumulateNeonLane(dotprod_32x4);</td></tr>
<tr><th id="1159">1159</th><td>      <i>// Postamble loop.</i></td></tr>
<tr><th id="1160">1160</th><td>      <b>for</b> (; TFLITE_UNLIKELY(col &lt; m_cols); ++col) {</td></tr>
<tr><th id="1161">1161</th><td>        dotprod += row_ptr[col] * aligned_vec[col];</td></tr>
<tr><th id="1162">1162</th><td>      }  <i>// for col</i></td></tr>
<tr><th id="1163">1163</th><td></td></tr>
<tr><th id="1164">1164</th><td>      *result += dotprod * batch_scaling_factor;</td></tr>
<tr><th id="1165">1165</th><td>      ++result;</td></tr>
<tr><th id="1166">1166</th><td>    }  <i>// for row</i></td></tr>
<tr><th id="1167">1167</th><td>  }    <i>// for batch</i></td></tr>
<tr><th id="1168">1168</th><td></td></tr>
<tr><th id="1169">1169</th><td>  <b>if</b> (unaligned) {</td></tr>
<tr><th id="1170">1170</th><td>    free(aligned_row_free);</td></tr>
<tr><th id="1171">1171</th><td>  }</td></tr>
<tr><th id="1172">1172</th><td>  free(aligned_vec_free);</td></tr>
<tr><th id="1173">1173</th><td>}</td></tr>
<tr><th id="1174">1174</th><td></td></tr>
<tr><th id="1175">1175</th><td><em>void</em> NeonMatrixBatchVectorMultiplyAccumulate(<em>const</em> int8_t* <b>__restrict__</b> matrix,</td></tr>
<tr><th id="1176">1176</th><td>                                             <em>const</em> <em>int</em> m_rows, <em>const</em> <em>int</em> m_cols,</td></tr>
<tr><th id="1177">1177</th><td>                                             <em>const</em> int8_t* <b>__restrict__</b> vectors,</td></tr>
<tr><th id="1178">1178</th><td>                                             <em>const</em> <em>float</em>* scaling_factors,</td></tr>
<tr><th id="1179">1179</th><td>                                             <em>int</em> n_batch, int32_t* scratch,</td></tr>
<tr><th id="1180">1180</th><td>                                             <em>float</em>* <b>__restrict__</b> result,</td></tr>
<tr><th id="1181">1181</th><td>                                             CpuBackendContext* context) {</td></tr>
<tr><th id="1182">1182</th><td>  <b>if</b> (m_rows % <var>4</var> == <var>0</var>) {</td></tr>
<tr><th id="1183">1183</th><td>    <em>const</em> int32_t* bias = <b>static_cast</b>&lt;<em>const</em> int32_t*&gt;(<b>nullptr</b>);</td></tr>
<tr><th id="1184">1184</th><td>    NeonCpuBackendGemm(vectors, bias, matrix, n_batch, m_cols, m_rows,</td></tr>
<tr><th id="1185">1185</th><td>                       <i>/*output_zp =*/</i><var>0</var>, scratch, context);</td></tr>
<tr><th id="1186">1186</th><td></td></tr>
<tr><th id="1187">1187</th><td>    <i>// Multiply by float scaling factors and write to result</i></td></tr>
<tr><th id="1188">1188</th><td>    <em>const</em> <em>int</em> total_size = n_batch * m_rows;</td></tr>
<tr><th id="1189">1189</th><td>    <em>int</em> i = <var>0</var>;</td></tr>
<tr><th id="1190">1190</th><td>    <b>for</b> (; i &lt;= total_size - <var>8</var>; i += <var>8</var>, result += <var>8</var>) {</td></tr>
<tr><th id="1191">1191</th><td>      <em>const</em> <em>float</em> batch_scaling_factor0 = scaling_factors[i / m_rows];</td></tr>
<tr><th id="1192">1192</th><td>      <em>const</em> <em>float</em> batch_scaling_factor1 = scaling_factors[(i + <var>4</var>) / m_rows];</td></tr>
<tr><th id="1193">1193</th><td>      <em>const</em> float32x4_t scaling_factor0 = vdupq_n_f32(batch_scaling_factor0);</td></tr>
<tr><th id="1194">1194</th><td>      <em>const</em> float32x4_t scaling_factor1 = vdupq_n_f32(batch_scaling_factor1);</td></tr>
<tr><th id="1195">1195</th><td>      <em>const</em> int32x4_t scratch_val0 = vld1q_s32(scratch + i);</td></tr>
<tr><th id="1196">1196</th><td>      <em>const</em> int32x4_t scratch_val1 = vld1q_s32(scratch + i + <var>4</var>);</td></tr>
<tr><th id="1197">1197</th><td>      <em>const</em> float32x4_t float_val0 = vcvtq_f32_s32(scratch_val0);</td></tr>
<tr><th id="1198">1198</th><td>      <em>const</em> float32x4_t float_val1 = vcvtq_f32_s32(scratch_val1);</td></tr>
<tr><th id="1199">1199</th><td>      <em>const</em> float32x4_t result0 =</td></tr>
<tr><th id="1200">1200</th><td>          vmlaq_f32(vld1q_f32(result), float_val0, scaling_factor0);</td></tr>
<tr><th id="1201">1201</th><td>      <em>const</em> float32x4_t result1 =</td></tr>
<tr><th id="1202">1202</th><td>          vmlaq_f32(vld1q_f32(result + <var>4</var>), float_val1, scaling_factor1);</td></tr>
<tr><th id="1203">1203</th><td>      vst1q_f32(result, result0);</td></tr>
<tr><th id="1204">1204</th><td>      vst1q_f32(result + <var>4</var>, result1);</td></tr>
<tr><th id="1205">1205</th><td>    }</td></tr>
<tr><th id="1206">1206</th><td>    scratch += i;</td></tr>
<tr><th id="1207">1207</th><td>    <b>for</b> (; TFLITE_UNLIKELY(i &lt; total_size); i++) {</td></tr>
<tr><th id="1208">1208</th><td>      <em>const</em> <em>float</em> batch_scaling_factor = scaling_factors[i / m_rows];</td></tr>
<tr><th id="1209">1209</th><td>      int32_t x = *(scratch++);</td></tr>
<tr><th id="1210">1210</th><td>      *result += x * batch_scaling_factor;</td></tr>
<tr><th id="1211">1211</th><td>      ++result;</td></tr>
<tr><th id="1212">1212</th><td>    }</td></tr>
<tr><th id="1213">1213</th><td>    <b>return</b>;</td></tr>
<tr><th id="1214">1214</th><td>  }</td></tr>
<tr><th id="1215">1215</th><td>  NeonMatrixBatchVectorMultiplyAccumulate(matrix, m_rows, m_cols, vectors,</td></tr>
<tr><th id="1216">1216</th><td>                                          scaling_factors, n_batch, result);</td></tr>
<tr><th id="1217">1217</th><td>}</td></tr>
<tr><th id="1218">1218</th><td></td></tr>
<tr><th id="1219">1219</th><td><em>void</em> NeonMatrixScalarMultiplyAccumulate(<em>const</em> int8_t* matrix, int32_t scalar,</td></tr>
<tr><th id="1220">1220</th><td>                                        int32_t n_row, int32_t n_col,</td></tr>
<tr><th id="1221">1221</th><td>                                        int32_t* output) {</td></tr>
<tr><th id="1222">1222</th><td>  <i>// Processing multiple rows at the same time actually makes it slower. :(</i></td></tr>
<tr><th id="1223">1223</th><td>  <b>for</b> (<em>int</em> i = <var>0</var>; i &lt; n_row; ++i) {</td></tr>
<tr><th id="1224">1224</th><td>    int32x4_t row_sum = vdupq_n_s32(<var>0</var>);</td></tr>
<tr><th id="1225">1225</th><td>    <em>int</em> j = <var>0</var>;</td></tr>
<tr><th id="1226">1226</th><td>    <em>const</em> int8_t* row_ptr = matrix + i * n_col;</td></tr>
<tr><th id="1227">1227</th><td>    <b>for</b> (; j &lt;= n_col - kInt8ValuesPerNeonVector;</td></tr>
<tr><th id="1228">1228</th><td>         j += kInt8ValuesPerNeonVector) {</td></tr>
<tr><th id="1229">1229</th><td>      <em>const</em> int8x16_t input_value = vld1q_s8(row_ptr + j);</td></tr>
<tr><th id="1230">1230</th><td>      int16x8_t temp = vmovl_s8(vget_low_s8(input_value));</td></tr>
<tr><th id="1231">1231</th><td>      temp = vaddw_s8(temp, vget_high_s8(input_value));</td></tr>
<tr><th id="1232">1232</th><td>      row_sum = vpadalq_s16(row_sum, temp);</td></tr>
<tr><th id="1233">1233</th><td>    }</td></tr>
<tr><th id="1234">1234</th><td>    int32_t sum = AccumulateNeonLane(row_sum);</td></tr>
<tr><th id="1235">1235</th><td>    <b>for</b> (; TFLITE_UNLIKELY(j &lt; n_col); ++j) {</td></tr>
<tr><th id="1236">1236</th><td>      sum += *(row_ptr + j);</td></tr>
<tr><th id="1237">1237</th><td>    }</td></tr>
<tr><th id="1238">1238</th><td>    output[i] += sum * scalar;</td></tr>
<tr><th id="1239">1239</th><td>  }</td></tr>
<tr><th id="1240">1240</th><td>}</td></tr>
<tr><th id="1241">1241</th><td></td></tr>
<tr><th id="1242">1242</th><td><em>void</em> NeonMatrixBatchVectorMultiplyAccumulateImpl(</td></tr>
<tr><th id="1243">1243</th><td>    <em>const</em> int8_t* <b>__restrict__</b> matrix, <em>const</em> <em>int</em> m_rows, <em>const</em> <em>int</em> m_cols,</td></tr>
<tr><th id="1244">1244</th><td>    <em>const</em> int8_t* <b>__restrict__</b> vectors, <em>const</em> <em>float</em>* scaling_factors,</td></tr>
<tr><th id="1245">1245</th><td>    <em>int</em> n_batch, <em>float</em>* <b>__restrict__</b> result, <em>const</em> <em>float</em>* per_channel_scale,</td></tr>
<tr><th id="1246">1246</th><td>    <em>const</em> int32_t* input_offset, int32_t* row_sums) {</td></tr>
<tr><th id="1247">1247</th><td><u>#ifdef __aarch64__</u></td></tr>
<tr><th id="1248">1248</th><td>  <b>if</b> (HasSdotInstruction() &amp;&amp; m_cols % <var>16</var> == <var>0</var> &amp;&amp; m_rows % <var>2</var> == <var>0</var> &amp;&amp;</td></tr>
<tr><th id="1249">1249</th><td>      m_rows &gt;= n_batch) {</td></tr>
<tr><th id="1250">1250</th><td>    <b>if</b> (n_batch % <var>4</var> == <var>0</var>) {</td></tr>
<tr><th id="1251">1251</th><td>      DotprodMatrixBatchFourVectorMultiplyAccumulate(</td></tr>
<tr><th id="1252">1252</th><td>          matrix, m_rows, m_cols, vectors, scaling_factors, n_batch, result,</td></tr>
<tr><th id="1253">1253</th><td>          per_channel_scale, input_offset, row_sums);</td></tr>
<tr><th id="1254">1254</th><td>      <b>return</b>;</td></tr>
<tr><th id="1255">1255</th><td>    } <b>else</b> <b>if</b> (n_batch &gt;= <var>2</var> &amp;&amp; m_rows * m_cols &gt;= <var>128</var> * <var>128</var>) {</td></tr>
<tr><th id="1256">1256</th><td>      DotprodMatrixBatchPaddedFourVectorMultiplyAccumulate(</td></tr>
<tr><th id="1257">1257</th><td>          matrix, m_rows, m_cols, vectors, scaling_factors, n_batch, result,</td></tr>
<tr><th id="1258">1258</th><td>          per_channel_scale, input_offset, row_sums);</td></tr>
<tr><th id="1259">1259</th><td>      <b>return</b>;</td></tr>
<tr><th id="1260">1260</th><td>    }</td></tr>
<tr><th id="1261">1261</th><td>  }</td></tr>
<tr><th id="1262">1262</th><td><u>#endif  // __aarch64__</u></td></tr>
<tr><th id="1263">1263</th><td></td></tr>
<tr><th id="1264">1264</th><td>  <em>bool</em> unaligned = <b>false</b>;</td></tr>
<tr><th id="1265">1265</th><td>  int8_t* aligned_row = <b>nullptr</b>;</td></tr>
<tr><th id="1266">1266</th><td>  <em>void</em>* aligned_row_free = <b>nullptr</b>;</td></tr>
<tr><th id="1267">1267</th><td>  <b>if</b> ((m_cols &amp; (kNeonVectorAlignment - <var>1</var>)) != <var>0</var>) {</td></tr>
<tr><th id="1268">1268</th><td>    unaligned = <b>true</b>;</td></tr>
<tr><th id="1269">1269</th><td>    aligned_row =</td></tr>
<tr><th id="1270">1270</th><td>        (int8_t*)aligned_alloc(kNeonVectorAlignment, m_cols,  <i>// NOLINT</i></td></tr>
<tr><th id="1271">1271</th><td>                               &amp;aligned_row_free);</td></tr>
<tr><th id="1272">1272</th><td>  }</td></tr>
<tr><th id="1273">1273</th><td>  <em>void</em>* aligned_vec_free = <b>nullptr</b>;</td></tr>
<tr><th id="1274">1274</th><td>  int8_t* aligned_vec =</td></tr>
<tr><th id="1275">1275</th><td>      (int8_t*)aligned_alloc(kNeonVectorAlignment, m_cols,  <i>// NOLINT</i></td></tr>
<tr><th id="1276">1276</th><td>                             &amp;aligned_vec_free);</td></tr>
<tr><th id="1277">1277</th><td></td></tr>
<tr><th id="1278">1278</th><td>  <em>const</em> <em>int</em> postamble_half_start =</td></tr>
<tr><th id="1279">1279</th><td>      RoundDownVectors&lt;kInt8ValuesPerNeonVector&gt;(m_cols);</td></tr>
<tr><th id="1280">1280</th><td>  <em>const</em> <em>int</em> postamble_start =</td></tr>
<tr><th id="1281">1281</th><td>      RoundDownVectors&lt;(kInt8ValuesPerNeonVector / <var>2</var>)&gt;(m_cols);</td></tr>
<tr><th id="1282">1282</th><td></td></tr>
<tr><th id="1283">1283</th><td>  int32_t* row_sums_ptr = row_sums;</td></tr>
<tr><th id="1284">1284</th><td>  <b>if</b> (row_sums == <b>nullptr</b>) {</td></tr>
<tr><th id="1285">1285</th><td>    row_sums_ptr = <b>static_cast</b>&lt;int32_t*&gt;(malloc(<b>sizeof</b>(int32_t) * m_rows));</td></tr>
<tr><th id="1286">1286</th><td>    NeonReductionSumVector(matrix, row_sums_ptr, m_rows, m_cols);</td></tr>
<tr><th id="1287">1287</th><td>  }</td></tr>
<tr><th id="1288">1288</th><td></td></tr>
<tr><th id="1289">1289</th><td>  <b>for</b> (<em>int</em> batch = <var>0</var>; batch &lt; n_batch; ++batch) {</td></tr>
<tr><th id="1290">1290</th><td>    <em>const</em> <em>float</em> batch_scaling_factor = scaling_factors[batch];</td></tr>
<tr><th id="1291">1291</th><td>    <em>const</em> <em>int</em> batch_input_offset = input_offset[batch];</td></tr>
<tr><th id="1292">1292</th><td>    memcpy(aligned_vec, vectors + batch * m_cols, <b>sizeof</b>(int8_t) * m_cols);</td></tr>
<tr><th id="1293">1293</th><td>    <b>for</b> (<em>int</em> row = <var>0</var>; row &lt; m_rows; ++row) {</td></tr>
<tr><th id="1294">1294</th><td>      int8_t* row_ptr = (int8_t*)matrix + row * m_cols;  <i>// NOLINT</i></td></tr>
<tr><th id="1295">1295</th><td>      <b>if</b> (unaligned) {</td></tr>
<tr><th id="1296">1296</th><td>        memcpy(aligned_row, row_ptr, <b>sizeof</b>(int8_t) * m_cols);</td></tr>
<tr><th id="1297">1297</th><td>        row_ptr = aligned_row;</td></tr>
<tr><th id="1298">1298</th><td>      }</td></tr>
<tr><th id="1299">1299</th><td>      <em>float</em> scale = batch_scaling_factor;</td></tr>
<tr><th id="1300">1300</th><td>      <b>if</b> (per_channel_scale) {</td></tr>
<tr><th id="1301">1301</th><td>        scale *= per_channel_scale[row];</td></tr>
<tr><th id="1302">1302</th><td>      }</td></tr>
<tr><th id="1303">1303</th><td>      <i>// Initialize the dot product sum for the row to 0.</i></td></tr>
<tr><th id="1304">1304</th><td>      int32x4_t dotprod_32x4 = vmovq_n_s32(<var>0</var>);</td></tr>
<tr><th id="1305">1305</th><td></td></tr>
<tr><th id="1306">1306</th><td>      <i>// Prefetch the row to cache.</i></td></tr>
<tr><th id="1307">1307</th><td>      __builtin_prefetch(row_ptr, <var>0</var> <i>/* prefetch for read */</i>,</td></tr>
<tr><th id="1308">1308</th><td>                         <var>3</var> <i>/* temporal locality */</i>);</td></tr>
<tr><th id="1309">1309</th><td></td></tr>
<tr><th id="1310">1310</th><td>      <i>// For every block of 16 8-bit elements.</i></td></tr>
<tr><th id="1311">1311</th><td>      <em>int</em> col = <var>0</var>;</td></tr>
<tr><th id="1312">1312</th><td>      <b>for</b> (; col &lt; postamble_half_start; col += kInt8ValuesPerNeonVector) {</td></tr>
<tr><th id="1313">1313</th><td>        <i>// Load 16 8-bit values from the row and vector, each, to operate on.</i></td></tr>
<tr><th id="1314">1314</th><td><i>        // Here the assumption is that each buffer is 4-byte aligned. Otherwise,</i></td></tr>
<tr><th id="1315">1315</th><td><i>        // performance may suffer significantly.</i></td></tr>
<tr><th id="1316">1316</th><td>        TFLITE_DCHECK_EQ(  <i>// NOLINT</i></td></tr>
<tr><th id="1317">1317</th><td>            (uintptr_t)(&amp;row_ptr[col]) &amp; (kNeonVectorAlignment - <var>1</var>), <var>0</var>);</td></tr>
<tr><th id="1318">1318</th><td>        <em>const</em> int8x16_t s1_8x16 = vld1q_s8((<em>const</em> int8_t*)(aligned_vec + col));</td></tr>
<tr><th id="1319">1319</th><td>        <em>const</em> int8x16_t s2_8x16 = vld1q_s8((<em>const</em> int8_t*)(row_ptr + col));</td></tr>
<tr><th id="1320">1320</th><td>        <i>// Multiply the low bits (i.e. the lower 8 8bit numbers in the</i></td></tr>
<tr><th id="1321">1321</th><td><i>        // registers).</i></td></tr>
<tr><th id="1322">1322</th><td>        int16x8_t prod_16x8 =</td></tr>
<tr><th id="1323">1323</th><td>            vmull_s8(vget_low_s8(s1_8x16), vget_low_s8(s2_8x16));</td></tr>
<tr><th id="1324">1324</th><td>        <i>// Multiply the high bits (i.e. the higher 8 8bit numbers in the</i></td></tr>
<tr><th id="1325">1325</th><td><i>        // registers), and accumulate with the result of the low bits product.</i></td></tr>
<tr><th id="1326">1326</th><td><i>        // The assumption here is that overflow will not happen as we quantize</i></td></tr>
<tr><th id="1327">1327</th><td><i>        // our values to be in the range [-127, 127]. As such the sum of the 2</i></td></tr>
<tr><th id="1328">1328</th><td><i>        // products is always strictly smaller than 15-bits (32767 in absolute</i></td></tr>
<tr><th id="1329">1329</th><td><i>        // value).</i></td></tr>
<tr><th id="1330">1330</th><td>        prod_16x8 =</td></tr>
<tr><th id="1331">1331</th><td>            vmlal_s8(prod_16x8, vget_high_s8(s1_8x16), vget_high_s8(s2_8x16));</td></tr>
<tr><th id="1332">1332</th><td>        dotprod_32x4 = vpadalq_s16(dotprod_32x4, prod_16x8);</td></tr>
<tr><th id="1333">1333</th><td>      }  <i>// for col</i></td></tr>
<tr><th id="1334">1334</th><td></td></tr>
<tr><th id="1335">1335</th><td>      <i>// Half iteration dealing only 8 elements</i></td></tr>
<tr><th id="1336">1336</th><td>      <b>if</b> (TFLITE_UNLIKELY(col &lt; postamble_start)) {</td></tr>
<tr><th id="1337">1337</th><td>        <i>// Load 8 8-bit values from the row and column each to operate on.</i></td></tr>
<tr><th id="1338">1338</th><td><i>        // Here the assumption is that each buffer is 4-bytes aligned.</i></td></tr>
<tr><th id="1339">1339</th><td><i>        // Otherwise, performance may suffer significantly.</i></td></tr>
<tr><th id="1340">1340</th><td>        TFLITE_DCHECK_EQ(  <i>// NOLINT</i></td></tr>
<tr><th id="1341">1341</th><td>            (uintptr_t)(&amp;row_ptr[col]) &amp; (kNeonVectorAlignment - <var>1</var>), <var>0</var>);</td></tr>
<tr><th id="1342">1342</th><td>        <em>const</em> int8x8_t s1_8x8 = vld1_s8((<em>const</em> int8_t*)(aligned_vec + col));</td></tr>
<tr><th id="1343">1343</th><td>        <em>const</em> int8x8_t s2_8x8 = vld1_s8((<em>const</em> int8_t*)(row_ptr + col));</td></tr>
<tr><th id="1344">1344</th><td>        <em>const</em> int16x8_t prod_16x8 = vmull_s8(s1_8x8, s2_8x8);</td></tr>
<tr><th id="1345">1345</th><td>        dotprod_32x4 = vpadalq_s16(dotprod_32x4, prod_16x8);</td></tr>
<tr><th id="1346">1346</th><td>        col += (kInt8ValuesPerNeonVector &gt;&gt; <var>1</var>);</td></tr>
<tr><th id="1347">1347</th><td>      }</td></tr>
<tr><th id="1348">1348</th><td></td></tr>
<tr><th id="1349">1349</th><td>      int32_t dotprod = AccumulateNeonLane(dotprod_32x4);</td></tr>
<tr><th id="1350">1350</th><td></td></tr>
<tr><th id="1351">1351</th><td>      <i>// Postamble loop.</i></td></tr>
<tr><th id="1352">1352</th><td>      <b>for</b> (; TFLITE_UNLIKELY(col &lt; m_cols); ++col) {</td></tr>
<tr><th id="1353">1353</th><td>        dotprod += row_ptr[col] * aligned_vec[col];</td></tr>
<tr><th id="1354">1354</th><td>      }  <i>// for col</i></td></tr>
<tr><th id="1355">1355</th><td>      dotprod -= row_sums_ptr[row] * batch_input_offset;</td></tr>
<tr><th id="1356">1356</th><td>      *result += dotprod * scale;</td></tr>
<tr><th id="1357">1357</th><td>      ++result;</td></tr>
<tr><th id="1358">1358</th><td>    }  <i>// for row</i></td></tr>
<tr><th id="1359">1359</th><td>  }    <i>// for batch</i></td></tr>
<tr><th id="1360">1360</th><td></td></tr>
<tr><th id="1361">1361</th><td>  <b>if</b> (row_sums == <b>nullptr</b>) {</td></tr>
<tr><th id="1362">1362</th><td>    free(row_sums_ptr);</td></tr>
<tr><th id="1363">1363</th><td>  }</td></tr>
<tr><th id="1364">1364</th><td>  <b>if</b> (unaligned) {</td></tr>
<tr><th id="1365">1365</th><td>    free(aligned_row_free);</td></tr>
<tr><th id="1366">1366</th><td>  }</td></tr>
<tr><th id="1367">1367</th><td>  free(aligned_vec_free);</td></tr>
<tr><th id="1368">1368</th><td>}</td></tr>
<tr><th id="1369">1369</th><td></td></tr>
<tr><th id="1370">1370</th><td><em>void</em> NeonMatrixBatchVectorMultiplyAccumulate(</td></tr>
<tr><th id="1371">1371</th><td>    <em>const</em> int8_t* <b>__restrict__</b> matrix, <em>const</em> <em>int</em> m_rows, <em>const</em> <em>int</em> m_cols,</td></tr>
<tr><th id="1372">1372</th><td>    <em>const</em> int8_t* <b>__restrict__</b> vectors, <em>const</em> <em>float</em>* scaling_factors,</td></tr>
<tr><th id="1373">1373</th><td>    <em>int</em> n_batch, <em>float</em>* <b>__restrict__</b> result, <em>const</em> <em>float</em>* per_channel_scale,</td></tr>
<tr><th id="1374">1374</th><td>    <em>const</em> int32_t* input_offset, int32_t* scratch, int32_t* row_sums,</td></tr>
<tr><th id="1375">1375</th><td>    <em>bool</em>* compute_row_sums, CpuBackendContext* context) {</td></tr>
<tr><th id="1376">1376</th><td><u>#ifdef TFLITE_WITH_RUY_GEMV</u></td></tr>
<tr><th id="1377">1377</th><td>  <em>const</em> <em>bool</em> use_cpu_backend_gemm = <b>true</b>;</td></tr>
<tr><th id="1378">1378</th><td><u>#else</u></td></tr>
<tr><th id="1379">1379</th><td>  <em>const</em> <em>bool</em> use_cpu_backend_gemm = UseCpuBackendGemm(m_rows, m_cols, n_batch);</td></tr>
<tr><th id="1380">1380</th><td><u>#endif</u></td></tr>
<tr><th id="1381">1381</th><td>  <b>if</b> (input_offset == <b>nullptr</b>) {</td></tr>
<tr><th id="1382">1382</th><td>    <b>if</b> (use_cpu_backend_gemm &amp;&amp; context) {</td></tr>
<tr><th id="1383">1383</th><td>      NeonMatrixBatchVectorMultiplyAccumulate(matrix, m_rows, m_cols, vectors,</td></tr>
<tr><th id="1384">1384</th><td>                                              scaling_factors, n_batch, scratch,</td></tr>
<tr><th id="1385">1385</th><td>                                              result, context);</td></tr>
<tr><th id="1386">1386</th><td>      <b>return</b>;</td></tr>
<tr><th id="1387">1387</th><td>    }</td></tr>
<tr><th id="1388">1388</th><td>    NeonMatrixBatchVectorMultiplyAccumulate(matrix, m_rows, m_cols, vectors,</td></tr>
<tr><th id="1389">1389</th><td>                                            scaling_factors, n_batch, result);</td></tr>
<tr><th id="1390">1390</th><td>    <b>return</b>;</td></tr>
<tr><th id="1391">1391</th><td>  }</td></tr>
<tr><th id="1392">1392</th><td></td></tr>
<tr><th id="1393">1393</th><td>  <b>if</b> (compute_row_sums == <b>nullptr</b> || *compute_row_sums) {</td></tr>
<tr><th id="1394">1394</th><td>    NeonReductionSumVector(matrix, row_sums, m_rows, m_cols);</td></tr>
<tr><th id="1395">1395</th><td>    <b>if</b> (compute_row_sums) {</td></tr>
<tr><th id="1396">1396</th><td>      *compute_row_sums = <b>false</b>;</td></tr>
<tr><th id="1397">1397</th><td>    }</td></tr>
<tr><th id="1398">1398</th><td>  }</td></tr>
<tr><th id="1399">1399</th><td></td></tr>
<tr><th id="1400">1400</th><td>  <b>if</b> (use_cpu_backend_gemm) {</td></tr>
<tr><th id="1401">1401</th><td>    <b>if</b> (context != <b>nullptr</b> &amp;&amp; m_rows % <var>4</var> == <var>0</var>) {</td></tr>
<tr><th id="1402">1402</th><td>      <em>const</em> int32_t* bias = <b>static_cast</b>&lt;<em>const</em> int32_t*&gt;(<b>nullptr</b>);</td></tr>
<tr><th id="1403">1403</th><td>      NeonCpuBackendGemm(vectors, bias, matrix, n_batch, m_cols, m_rows, <var>0</var>,</td></tr>
<tr><th id="1404">1404</th><td>                         scratch, context);</td></tr>
<tr><th id="1405">1405</th><td></td></tr>
<tr><th id="1406">1406</th><td>      <i>// Multiply by float scaling factors and write to result</i></td></tr>
<tr><th id="1407">1407</th><td>      <em>const</em> <em>int</em> total_size = n_batch * m_rows;</td></tr>
<tr><th id="1408">1408</th><td>      <em>int</em> i = <var>0</var>;</td></tr>
<tr><th id="1409">1409</th><td>      int32_t* scratch_ptr = scratch;</td></tr>
<tr><th id="1410">1410</th><td>      <b>for</b> (; i &lt;= total_size - <var>8</var>; i += <var>8</var>, result += <var>8</var>) {</td></tr>
<tr><th id="1411">1411</th><td>        <em>const</em> <em>float</em> batch_scaling_factor0 = scaling_factors[i / m_rows];</td></tr>
<tr><th id="1412">1412</th><td>        <em>const</em> <em>float</em> batch_scaling_factor1 = scaling_factors[(i + <var>4</var>) / m_rows];</td></tr>
<tr><th id="1413">1413</th><td>        <em>const</em> <em>int</em> batch_input_offset0 = -input_offset[i / m_rows];</td></tr>
<tr><th id="1414">1414</th><td>        <em>const</em> <em>int</em> batch_input_offset1 = -input_offset[(i + <var>4</var>) / m_rows];</td></tr>
<tr><th id="1415">1415</th><td>        float32x4_t scaling_factor0 = vdupq_n_f32(batch_scaling_factor0);</td></tr>
<tr><th id="1416">1416</th><td>        float32x4_t scaling_factor1 = vdupq_n_f32(batch_scaling_factor1);</td></tr>
<tr><th id="1417">1417</th><td>        <b>if</b> (per_channel_scale) {</td></tr>
<tr><th id="1418">1418</th><td>          <em>const</em> float32x4_t per_channel_scale0 =</td></tr>
<tr><th id="1419">1419</th><td>              vld1q_f32(&amp;per_channel_scale[i % m_rows]);</td></tr>
<tr><th id="1420">1420</th><td>          <em>const</em> float32x4_t per_channel_scale1 =</td></tr>
<tr><th id="1421">1421</th><td>              vld1q_f32(&amp;per_channel_scale[(i + <var>4</var>) % m_rows]);</td></tr>
<tr><th id="1422">1422</th><td>          scaling_factor0 = vmulq_f32(scaling_factor0, per_channel_scale0);</td></tr>
<tr><th id="1423">1423</th><td>          scaling_factor1 = vmulq_f32(scaling_factor1, per_channel_scale1);</td></tr>
<tr><th id="1424">1424</th><td>        }</td></tr>
<tr><th id="1425">1425</th><td>        <em>const</em> int32x4_t input_offset0 = vdupq_n_s32(batch_input_offset0);</td></tr>
<tr><th id="1426">1426</th><td>        <em>const</em> int32x4_t input_offset1 = vdupq_n_s32(batch_input_offset1);</td></tr>
<tr><th id="1427">1427</th><td>        <em>const</em> int32x4_t row_sum0 = vld1q_s32(row_sums + (i % m_rows));</td></tr>
<tr><th id="1428">1428</th><td>        <em>const</em> int32x4_t row_sum1 = vld1q_s32(row_sums + ((i + <var>4</var>) % m_rows));</td></tr>
<tr><th id="1429">1429</th><td>        <em>const</em> int32x4_t scratch_val0 = vld1q_s32(scratch_ptr + i);</td></tr>
<tr><th id="1430">1430</th><td>        <em>const</em> int32x4_t scratch_val1 = vld1q_s32(scratch_ptr + i + <var>4</var>);</td></tr>
<tr><th id="1431">1431</th><td>        <em>const</em> int32x4_t dotprod0 =</td></tr>
<tr><th id="1432">1432</th><td>            vmlaq_s32(scratch_val0, row_sum0, input_offset0);</td></tr>
<tr><th id="1433">1433</th><td>        <em>const</em> int32x4_t dotprod1 =</td></tr>
<tr><th id="1434">1434</th><td>            vmlaq_s32(scratch_val1, row_sum1, input_offset1);</td></tr>
<tr><th id="1435">1435</th><td>        <em>const</em> float32x4_t float_val0 = vcvtq_f32_s32(dotprod0);</td></tr>
<tr><th id="1436">1436</th><td>        <em>const</em> float32x4_t float_val1 = vcvtq_f32_s32(dotprod1);</td></tr>
<tr><th id="1437">1437</th><td>        <em>const</em> float32x4_t result0 =</td></tr>
<tr><th id="1438">1438</th><td>            vmlaq_f32(vld1q_f32(result), float_val0, scaling_factor0);</td></tr>
<tr><th id="1439">1439</th><td>        <em>const</em> float32x4_t result1 =</td></tr>
<tr><th id="1440">1440</th><td>            vmlaq_f32(vld1q_f32(result + <var>4</var>), float_val1, scaling_factor1);</td></tr>
<tr><th id="1441">1441</th><td>        vst1q_f32(result, result0);</td></tr>
<tr><th id="1442">1442</th><td>        vst1q_f32(result + <var>4</var>, result1);</td></tr>
<tr><th id="1443">1443</th><td>      }</td></tr>
<tr><th id="1444">1444</th><td></td></tr>
<tr><th id="1445">1445</th><td>      scratch_ptr += i;</td></tr>
<tr><th id="1446">1446</th><td>      <b>for</b> (; TFLITE_UNLIKELY(i &lt; total_size); i++) {</td></tr>
<tr><th id="1447">1447</th><td>        <em>float</em> batch_scaling_factor = scaling_factors[i / m_rows];</td></tr>
<tr><th id="1448">1448</th><td>        <b>if</b> (per_channel_scale) {</td></tr>
<tr><th id="1449">1449</th><td>          batch_scaling_factor *= per_channel_scale[i % m_rows];</td></tr>
<tr><th id="1450">1450</th><td>        }</td></tr>
<tr><th id="1451">1451</th><td>        <em>const</em> int32_t zero_point = input_offset[i / m_rows];</td></tr>
<tr><th id="1452">1452</th><td>        int32_t dotprod = *(scratch_ptr++);</td></tr>
<tr><th id="1453">1453</th><td>        dotprod -= row_sums[i % m_rows] * zero_point;</td></tr>
<tr><th id="1454">1454</th><td>        *result += dotprod * batch_scaling_factor;</td></tr>
<tr><th id="1455">1455</th><td>        ++result;</td></tr>
<tr><th id="1456">1456</th><td>      }</td></tr>
<tr><th id="1457">1457</th><td>      <b>return</b>;</td></tr>
<tr><th id="1458">1458</th><td>    }</td></tr>
<tr><th id="1459">1459</th><td>  }</td></tr>
<tr><th id="1460">1460</th><td></td></tr>
<tr><th id="1461">1461</th><td>  NeonMatrixBatchVectorMultiplyAccumulateImpl(</td></tr>
<tr><th id="1462">1462</th><td>      matrix, m_rows, m_cols, vectors, scaling_factors, n_batch, result,</td></tr>
<tr><th id="1463">1463</th><td>      per_channel_scale, input_offset, row_sums);</td></tr>
<tr><th id="1464">1464</th><td>}</td></tr>
<tr><th id="1465">1465</th><td></td></tr>
<tr><th id="1466">1466</th><td><b>inline</b> int64x2x2_t MulAdd(int32x4_t acc, int32x4_t lhs, int32x4_t rhs) {</td></tr>
<tr><th id="1467">1467</th><td>  int64x2x2_t result;</td></tr>
<tr><th id="1468">1468</th><td>  <em>const</em> int64x2_t lhs_low = vmovl_s32(vget_low_s32(lhs));</td></tr>
<tr><th id="1469">1469</th><td>  <em>const</em> int64x2_t lhs_high = vmovl_s32(vget_high_s32(lhs));</td></tr>
<tr><th id="1470">1470</th><td>  <em>const</em> int64_t lhs_0 = vgetq_lane_s64(lhs_low, <var>0</var>);</td></tr>
<tr><th id="1471">1471</th><td>  <em>const</em> int64_t lhs_1 = vgetq_lane_s64(lhs_low, <var>1</var>);</td></tr>
<tr><th id="1472">1472</th><td>  <em>const</em> int64_t lhs_2 = vgetq_lane_s64(lhs_high, <var>0</var>);</td></tr>
<tr><th id="1473">1473</th><td>  <em>const</em> int64_t lhs_3 = vgetq_lane_s64(lhs_high, <var>1</var>);</td></tr>
<tr><th id="1474">1474</th><td></td></tr>
<tr><th id="1475">1475</th><td>  <em>const</em> int64x2_t rhs_low = vmovl_s32(vget_low_s32(rhs));</td></tr>
<tr><th id="1476">1476</th><td>  <em>const</em> int64x2_t rhs_high = vmovl_s32(vget_high_s32(rhs));</td></tr>
<tr><th id="1477">1477</th><td>  <em>const</em> int64_t rhs_0 = vgetq_lane_s64(rhs_low, <var>0</var>);</td></tr>
<tr><th id="1478">1478</th><td>  <em>const</em> int64_t rhs_1 = vgetq_lane_s64(rhs_low, <var>1</var>);</td></tr>
<tr><th id="1479">1479</th><td>  <em>const</em> int64_t rhs_2 = vgetq_lane_s64(rhs_high, <var>0</var>);</td></tr>
<tr><th id="1480">1480</th><td>  <em>const</em> int64_t rhs_3 = vgetq_lane_s64(rhs_high, <var>1</var>);</td></tr>
<tr><th id="1481">1481</th><td></td></tr>
<tr><th id="1482">1482</th><td>  <em>const</em> int64x2_t mul_0 = {lhs_0 * rhs_0, lhs_1 * rhs_1};</td></tr>
<tr><th id="1483">1483</th><td>  <em>const</em> int64x2_t mul_1 = {lhs_2 * rhs_2, lhs_3 * rhs_3};</td></tr>
<tr><th id="1484">1484</th><td></td></tr>
<tr><th id="1485">1485</th><td>  result.val[<var>0</var>] = vaddq_s64(vmovl_s32(vget_low_s32(acc)), mul_0);</td></tr>
<tr><th id="1486">1486</th><td>  result.val[<var>1</var>] = vaddq_s64(vmovl_s32(vget_high_s32(acc)), mul_1);</td></tr>
<tr><th id="1487">1487</th><td>  <b>return</b> result;</td></tr>
<tr><th id="1488">1488</th><td>}</td></tr>
<tr><th id="1489">1489</th><td></td></tr>
<tr><th id="1490">1490</th><td><em>void</em> NeonApplyLayerNorm(<em>const</em> int16_t* input, <em>const</em> int16_t* layer_norm_weights,</td></tr>
<tr><th id="1491">1491</th><td>                        <em>const</em> int32_t* bias, int32_t layer_norm_scale_a,</td></tr>
<tr><th id="1492">1492</th><td>                        int32_t layer_norm_scale_b, int32_t variance_limit,</td></tr>
<tr><th id="1493">1493</th><td>                        <em>int</em> n_batch, <em>int</em> n_input, int16_t* output) {</td></tr>
<tr><th id="1494">1494</th><td>  <em>const</em> int32 int16_max = std::numeric_limits&lt;int16&gt;::max();</td></tr>
<tr><th id="1495">1495</th><td>  <em>const</em> int32 int16_min = std::numeric_limits&lt;int16&gt;::min();</td></tr>
<tr><th id="1496">1496</th><td>  <em>const</em> int32 temp = <var>1048576</var> / n_input;</td></tr>
<tr><th id="1497">1497</th><td></td></tr>
<tr><th id="1498">1498</th><td>  <b>for</b> (<em>int</em> i = <var>0</var>; i &lt; n_batch; ++i) {</td></tr>
<tr><th id="1499">1499</th><td>    int64_t sum = <var>0</var>;</td></tr>
<tr><th id="1500">1500</th><td>    int64_t sum_sq = <var>0</var>;</td></tr>
<tr><th id="1501">1501</th><td></td></tr>
<tr><th id="1502">1502</th><td>    <em>int</em> j = <var>0</var>;</td></tr>
<tr><th id="1503">1503</th><td>    <b>for</b> (; j &lt;= n_input - <var>8</var>; j += <var>8</var>) {</td></tr>
<tr><th id="1504">1504</th><td>      <em>const</em> int32 index = i * n_input + j;</td></tr>
<tr><th id="1505">1505</th><td>      <em>const</em> int16x8_t val_s16 = vld1q_s16(input + index);</td></tr>
<tr><th id="1506">1506</th><td>      <em>const</em> int32x4_t val_s32_0 = vmovl_s16(vget_low_s16(val_s16));</td></tr>
<tr><th id="1507">1507</th><td>      <em>const</em> int32x4_t val_s32_1 = vmovl_s16(vget_high_s16(val_s16));</td></tr>
<tr><th id="1508">1508</th><td></td></tr>
<tr><th id="1509">1509</th><td>      sum += <b>static_cast</b>&lt;int64_t&gt;(AccumulateNeonLane(val_s32_0));</td></tr>
<tr><th id="1510">1510</th><td>      sum += <b>static_cast</b>&lt;int64_t&gt;(AccumulateNeonLane(val_s32_1));</td></tr>
<tr><th id="1511">1511</th><td></td></tr>
<tr><th id="1512">1512</th><td>      sum_sq += <b>static_cast</b>&lt;int64_t&gt;(</td></tr>
<tr><th id="1513">1513</th><td>          AccumulateNeonLane(vmulq_s32(val_s32_0, val_s32_0)));</td></tr>
<tr><th id="1514">1514</th><td>      sum_sq += <b>static_cast</b>&lt;int64_t&gt;(</td></tr>
<tr><th id="1515">1515</th><td>          AccumulateNeonLane(vmulq_s32(val_s32_1, val_s32_1)));</td></tr>
<tr><th id="1516">1516</th><td>    }</td></tr>
<tr><th id="1517">1517</th><td>    <b>for</b> (; TFLITE_UNLIKELY(j &lt; n_input); ++j) {</td></tr>
<tr><th id="1518">1518</th><td>      <em>const</em> int32 index = i * n_input + j;</td></tr>
<tr><th id="1519">1519</th><td>      int32 val = <b>static_cast</b>&lt;int32_t&gt;(input[index]);</td></tr>
<tr><th id="1520">1520</th><td>      sum += val;</td></tr>
<tr><th id="1521">1521</th><td>      sum_sq += val * val;</td></tr>
<tr><th id="1522">1522</th><td>    }</td></tr>
<tr><th id="1523">1523</th><td></td></tr>
<tr><th id="1524">1524</th><td>    int32_t mean =</td></tr>
<tr><th id="1525">1525</th><td>        <b>static_cast</b>&lt;int32_t&gt;(<b>static_cast</b>&lt;int64_t&gt;(sum) * <var>1024</var> / n_input);</td></tr>
<tr><th id="1526">1526</th><td>    <i>// TODO(jianlijianli): Avoids overflow but only works for POT n_input.</i></td></tr>
<tr><th id="1527">1527</th><td>    int64_t variance =</td></tr>
<tr><th id="1528">1528</th><td>        sum_sq * temp - <b>static_cast</b>&lt;int64_t&gt;(mean) * <b>static_cast</b>&lt;int64_t&gt;(mean);</td></tr>
<tr><th id="1529">1529</th><td>    int32_t variance2 = <b>static_cast</b>&lt;int32&gt;(variance / <var>1048576</var>);</td></tr>
<tr><th id="1530">1530</th><td>    <b>if</b> (variance2 &lt; <var>1</var>) {</td></tr>
<tr><th id="1531">1531</th><td>      variance2 = variance_limit;</td></tr>
<tr><th id="1532">1532</th><td>    }</td></tr>
<tr><th id="1533">1533</th><td>    int32_t stddev_inverse_a;</td></tr>
<tr><th id="1534">1534</th><td>    <em>int</em> stddev_inverse_b;</td></tr>
<tr><th id="1535">1535</th><td>    GetInvSqrtQuantizedMultiplierExp(variance2, <i>/*reverse_shift*/</i> -<var>1</var>,</td></tr>
<tr><th id="1536">1536</th><td>                                     &amp;stddev_inverse_a, &amp;stddev_inverse_b);</td></tr>
<tr><th id="1537">1537</th><td></td></tr>
<tr><th id="1538">1538</th><td>    j = <var>0</var>;</td></tr>
<tr><th id="1539">1539</th><td>    <em>const</em> int32x4_t mean_dup = vdupq_n_s32(mean);</td></tr>
<tr><th id="1540">1540</th><td>    <b>for</b> (; j &lt;= n_input - <var>16</var>; j += <var>16</var>) {</td></tr>
<tr><th id="1541">1541</th><td>      <i>// Load 16 items at once.</i></td></tr>
<tr><th id="1542">1542</th><td>      <em>const</em> int32 index = i * n_input + j;</td></tr>
<tr><th id="1543">1543</th><td>      <em>const</em> int16x8_t val_s16_0 = vld1q_s16(input + index);</td></tr>
<tr><th id="1544">1544</th><td>      <em>const</em> int16x8_t val_s16_1 = vld1q_s16(input + index + <var>8</var>);</td></tr>
<tr><th id="1545">1545</th><td></td></tr>
<tr><th id="1546">1546</th><td>      int32x4x4_t shifted;</td></tr>
<tr><th id="1547">1547</th><td>      shifted.val[<var>0</var>] = vsubq_s32(</td></tr>
<tr><th id="1548">1548</th><td>          vshlq_n_s32(vmovl_s16(vget_low_s16(val_s16_0)), <var>10</var>), mean_dup);</td></tr>
<tr><th id="1549">1549</th><td>      shifted.val[<var>1</var>] = vsubq_s32(</td></tr>
<tr><th id="1550">1550</th><td>          vshlq_n_s32(vmovl_s16(vget_high_s16(val_s16_0)), <var>10</var>), mean_dup);</td></tr>
<tr><th id="1551">1551</th><td>      shifted.val[<var>2</var>] = vsubq_s32(</td></tr>
<tr><th id="1552">1552</th><td>          vshlq_n_s32(vmovl_s16(vget_low_s16(val_s16_1)), <var>10</var>), mean_dup);</td></tr>
<tr><th id="1553">1553</th><td>      shifted.val[<var>3</var>] = vsubq_s32(</td></tr>
<tr><th id="1554">1554</th><td>          vshlq_n_s32(vmovl_s16(vget_high_s16(val_s16_1)), <var>10</var>), mean_dup);</td></tr>
<tr><th id="1555">1555</th><td></td></tr>
<tr><th id="1556">1556</th><td>      int32x4x4_t rescaled = MultiplyByQuantizedMultiplier4Rows(</td></tr>
<tr><th id="1557">1557</th><td>          shifted, stddev_inverse_a, stddev_inverse_b);</td></tr>
<tr><th id="1558">1558</th><td></td></tr>
<tr><th id="1559">1559</th><td>      <em>const</em> int32x4_t bias_0 = vld1q_s32(bias + j);</td></tr>
<tr><th id="1560">1560</th><td>      <em>const</em> int32x4_t bias_1 = vld1q_s32(bias + j + <var>4</var>);</td></tr>
<tr><th id="1561">1561</th><td>      <em>const</em> int32x4_t bias_2 = vld1q_s32(bias + j + <var>8</var>);</td></tr>
<tr><th id="1562">1562</th><td>      <em>const</em> int32x4_t bias_3 = vld1q_s32(bias + j + <var>12</var>);</td></tr>
<tr><th id="1563">1563</th><td></td></tr>
<tr><th id="1564">1564</th><td>      <em>const</em> int16x8_t layer_norm_weights_s16_0 =</td></tr>
<tr><th id="1565">1565</th><td>          vld1q_s16(layer_norm_weights + j);</td></tr>
<tr><th id="1566">1566</th><td>      <em>const</em> int16x8_t layer_norm_weights_s16_1 =</td></tr>
<tr><th id="1567">1567</th><td>          vld1q_s16(layer_norm_weights + j + <var>8</var>);</td></tr>
<tr><th id="1568">1568</th><td>      <em>const</em> int32x4_t layer_norm_weights_s32_0 =</td></tr>
<tr><th id="1569">1569</th><td>          vmovl_s16(vget_low_s16(layer_norm_weights_s16_0));</td></tr>
<tr><th id="1570">1570</th><td>      <em>const</em> int32x4_t layer_norm_weights_s32_1 =</td></tr>
<tr><th id="1571">1571</th><td>          vmovl_s16(vget_high_s16(layer_norm_weights_s16_0));</td></tr>
<tr><th id="1572">1572</th><td>      <em>const</em> int32x4_t layer_norm_weights_s32_2 =</td></tr>
<tr><th id="1573">1573</th><td>          vmovl_s16(vget_low_s16(layer_norm_weights_s16_1));</td></tr>
<tr><th id="1574">1574</th><td>      <em>const</em> int32x4_t layer_norm_weights_s32_3 =</td></tr>
<tr><th id="1575">1575</th><td>          vmovl_s16(vget_high_s16(layer_norm_weights_s16_1));</td></tr>
<tr><th id="1576">1576</th><td></td></tr>
<tr><th id="1577">1577</th><td>      int64x2x2_t val3_0 =</td></tr>
<tr><th id="1578">1578</th><td>          MulAdd(bias_0, rescaled.val[<var>0</var>], layer_norm_weights_s32_0);</td></tr>
<tr><th id="1579">1579</th><td>      int64x2x2_t val3_1 =</td></tr>
<tr><th id="1580">1580</th><td>          MulAdd(bias_1, rescaled.val[<var>1</var>], layer_norm_weights_s32_1);</td></tr>
<tr><th id="1581">1581</th><td>      int64x2x2_t val3_2 =</td></tr>
<tr><th id="1582">1582</th><td>          MulAdd(bias_2, rescaled.val[<var>2</var>], layer_norm_weights_s32_2);</td></tr>
<tr><th id="1583">1583</th><td>      int64x2x2_t val3_3 =</td></tr>
<tr><th id="1584">1584</th><td>          MulAdd(bias_3, rescaled.val[<var>3</var>], layer_norm_weights_s32_3);</td></tr>
<tr><th id="1585">1585</th><td></td></tr>
<tr><th id="1586">1586</th><td>      int32x4x4_t val4;</td></tr>
<tr><th id="1587">1587</th><td>      val4.val[<var>0</var>] = vcombine_s32(vmovn_s64(vrshrq_n_s64(val3_0.val[<var>0</var>], <var>10</var>)),</td></tr>
<tr><th id="1588">1588</th><td>                                 vmovn_s64(vrshrq_n_s64(val3_0.val[<var>1</var>], <var>10</var>)));</td></tr>
<tr><th id="1589">1589</th><td>      val4.val[<var>1</var>] = vcombine_s32(vmovn_s64(vrshrq_n_s64(val3_1.val[<var>0</var>], <var>10</var>)),</td></tr>
<tr><th id="1590">1590</th><td>                                 vmovn_s64(vrshrq_n_s64(val3_1.val[<var>1</var>], <var>10</var>)));</td></tr>
<tr><th id="1591">1591</th><td>      val4.val[<var>2</var>] = vcombine_s32(vmovn_s64(vrshrq_n_s64(val3_2.val[<var>0</var>], <var>10</var>)),</td></tr>
<tr><th id="1592">1592</th><td>                                 vmovn_s64(vrshrq_n_s64(val3_2.val[<var>1</var>], <var>10</var>)));</td></tr>
<tr><th id="1593">1593</th><td>      val4.val[<var>3</var>] = vcombine_s32(vmovn_s64(vrshrq_n_s64(val3_3.val[<var>0</var>], <var>10</var>)),</td></tr>
<tr><th id="1594">1594</th><td>                                 vmovn_s64(vrshrq_n_s64(val3_3.val[<var>1</var>], <var>10</var>)));</td></tr>
<tr><th id="1595">1595</th><td></td></tr>
<tr><th id="1596">1596</th><td>      int32x4x4_t val5_s32 = MultiplyByQuantizedMultiplier4Rows(</td></tr>
<tr><th id="1597">1597</th><td>          val4, layer_norm_scale_a, layer_norm_scale_b + <var>12</var>);</td></tr>
<tr><th id="1598">1598</th><td>      vst1_s16(output + index, vqmovn_s32(val5_s32.val[<var>0</var>]));</td></tr>
<tr><th id="1599">1599</th><td>      vst1_s16(output + index + <var>4</var>, vqmovn_s32(val5_s32.val[<var>1</var>]));</td></tr>
<tr><th id="1600">1600</th><td>      vst1_s16(output + index + <var>8</var>, vqmovn_s32(val5_s32.val[<var>2</var>]));</td></tr>
<tr><th id="1601">1601</th><td>      vst1_s16(output + index + <var>12</var>, vqmovn_s32(val5_s32.val[<var>3</var>]));</td></tr>
<tr><th id="1602">1602</th><td>    }</td></tr>
<tr><th id="1603">1603</th><td>    <b>for</b> (; TFLITE_UNLIKELY(j &lt; n_input); ++j) {</td></tr>
<tr><th id="1604">1604</th><td>      <em>const</em> int32 index = i * n_input + j;</td></tr>
<tr><th id="1605">1605</th><td>      int32 val = <b>static_cast</b>&lt;int32_t&gt;(input[index]);</td></tr>
<tr><th id="1606">1606</th><td>      int32 shifted = <var>1024</var> * val - mean;</td></tr>
<tr><th id="1607">1607</th><td>      int32 rescaled = MultiplyByQuantizedMultiplier(shifted, stddev_inverse_a,</td></tr>
<tr><th id="1608">1608</th><td>                                                     stddev_inverse_b);</td></tr>
<tr><th id="1609">1609</th><td>      <i>// TODO(jianlijianli): Saturate this.</i></td></tr>
<tr><th id="1610">1610</th><td>      int64_t val3 = rescaled * layer_norm_weights[j] + bias[j];</td></tr>
<tr><th id="1611">1611</th><td>      int32 val4 =</td></tr>
<tr><th id="1612">1612</th><td>          <b>static_cast</b>&lt;int32&gt;((val3 &gt; <var>0</var> ? val3 + <var>512</var> : val3 - <var>512</var>) / <var>1024</var>);</td></tr>
<tr><th id="1613">1613</th><td>      int32 val5 = MultiplyByQuantizedMultiplier(val4, layer_norm_scale_a,</td></tr>
<tr><th id="1614">1614</th><td>                                                 layer_norm_scale_b + <var>12</var>);</td></tr>
<tr><th id="1615">1615</th><td>      val5 = std::min(std::max(int16_min, val5), int16_max);</td></tr>
<tr><th id="1616">1616</th><td>      output[index] = <b>static_cast</b>&lt;int16_t&gt;(val5);</td></tr>
<tr><th id="1617">1617</th><td>    }</td></tr>
<tr><th id="1618">1618</th><td>  }</td></tr>
<tr><th id="1619">1619</th><td>}</td></tr>
<tr><th id="1620">1620</th><td></td></tr>
<tr><th id="1621">1621</th><td><em>void</em> NeonApplySigmoid(<em>const</em> int16_t* input, int32_t n_batch, int32_t n_input,</td></tr>
<tr><th id="1622">1622</th><td>                      int16_t* output) {</td></tr>
<tr><th id="1623">1623</th><td>  <b>for</b> (<em>int</em> batch = <var>0</var>; batch &lt; n_batch; ++batch) {</td></tr>
<tr><th id="1624">1624</th><td>    <em>int</em> i = <var>0</var>;</td></tr>
<tr><th id="1625">1625</th><td><u>#ifdef GEMMLOWP_NEON</u></td></tr>
<tr><th id="1626">1626</th><td>    <i>// F0 uses 0 integer bits, range [-1, 1].</i></td></tr>
<tr><th id="1627">1627</th><td><i>    // This is the return type of math functions such as tanh, logistic,</i></td></tr>
<tr><th id="1628">1628</th><td><i>    // whose range is in [-1, 1].</i></td></tr>
<tr><th id="1629">1629</th><td>    <b>using</b> F0 = gemmlowp::FixedPoint&lt;int16x8_t, <var>0</var>&gt;;</td></tr>
<tr><th id="1630">1630</th><td>    <i>// F3 uses 3 integer bits, range [-8, 8], the input range expected here.</i></td></tr>
<tr><th id="1631">1631</th><td>    <b>using</b> F3 = gemmlowp::FixedPoint&lt;int16x8_t, <var>3</var>&gt;;</td></tr>
<tr><th id="1632">1632</th><td></td></tr>
<tr><th id="1633">1633</th><td>    <b>for</b> (; i &lt;= n_input - <var>32</var>; i += <var>32</var>) {</td></tr>
<tr><th id="1634">1634</th><td>      <em>const</em> <em>int</em> index = batch * n_input + i;</td></tr>
<tr><th id="1635">1635</th><td>      F3 input0 = F3::FromRaw(vld1q_s16(input + index));</td></tr>
<tr><th id="1636">1636</th><td>      F3 input1 = F3::FromRaw(vld1q_s16(input + index + <var>8</var>));</td></tr>
<tr><th id="1637">1637</th><td>      F3 input2 = F3::FromRaw(vld1q_s16(input + index + <var>16</var>));</td></tr>
<tr><th id="1638">1638</th><td>      F3 input3 = F3::FromRaw(vld1q_s16(input + index + <var>24</var>));</td></tr>
<tr><th id="1639">1639</th><td>      F0 output0 = gemmlowp::logistic(input0);</td></tr>
<tr><th id="1640">1640</th><td>      F0 output1 = gemmlowp::logistic(input1);</td></tr>
<tr><th id="1641">1641</th><td>      F0 output2 = gemmlowp::logistic(input2);</td></tr>
<tr><th id="1642">1642</th><td>      F0 output3 = gemmlowp::logistic(input3);</td></tr>
<tr><th id="1643">1643</th><td>      vst1q_s16(output + index, output0.raw());</td></tr>
<tr><th id="1644">1644</th><td>      vst1q_s16(output + index + <var>8</var>, output1.raw());</td></tr>
<tr><th id="1645">1645</th><td>      vst1q_s16(output + index + <var>16</var>, output2.raw());</td></tr>
<tr><th id="1646">1646</th><td>      vst1q_s16(output + index + <var>24</var>, output3.raw());</td></tr>
<tr><th id="1647">1647</th><td>    }</td></tr>
<tr><th id="1648">1648</th><td><u>#endif  // GEMMLOWP_NEON</u></td></tr>
<tr><th id="1649">1649</th><td>    <b>using</b> F0_Scalar = gemmlowp::FixedPoint&lt;int16_t, <var>0</var>&gt;;</td></tr>
<tr><th id="1650">1650</th><td>    <b>using</b> F3_Scalar = gemmlowp::FixedPoint&lt;int16_t, <var>3</var>&gt;;</td></tr>
<tr><th id="1651">1651</th><td>    <b>for</b> (; i &lt; n_input; ++i) {</td></tr>
<tr><th id="1652">1652</th><td>      <em>const</em> <em>int</em> index = batch * n_input + i;</td></tr>
<tr><th id="1653">1653</th><td>      F3_Scalar input_f3 = F3_Scalar::FromRaw(input[index]);</td></tr>
<tr><th id="1654">1654</th><td>      F0_Scalar output_f0 = gemmlowp::logistic(input_f3);</td></tr>
<tr><th id="1655">1655</th><td>      output[index] = output_f0.raw();</td></tr>
<tr><th id="1656">1656</th><td>    }</td></tr>
<tr><th id="1657">1657</th><td>  }</td></tr>
<tr><th id="1658">1658</th><td>}</td></tr>
<tr><th id="1659">1659</th><td></td></tr>
<tr><th id="1660">1660</th><td><b>template</b> &lt;<em>int</em> IntegerBits&gt;</td></tr>
<tr><th id="1661">1661</th><td><em>void</em> NeonApplyTanhImpl(<em>const</em> int16_t* input, int32_t n_batch, int32_t n_input,</td></tr>
<tr><th id="1662">1662</th><td>                       int16_t* output) {</td></tr>
<tr><th id="1663">1663</th><td>  <b>for</b> (<em>int</em> batch = <var>0</var>; batch &lt; n_batch; ++batch) {</td></tr>
<tr><th id="1664">1664</th><td>    <em>int</em> i = <var>0</var>;</td></tr>
<tr><th id="1665">1665</th><td><u>#ifdef GEMMLOWP_NEON</u></td></tr>
<tr><th id="1666">1666</th><td>    <i>// F0 uses 0 integer bits, range [-1, 1].</i></td></tr>
<tr><th id="1667">1667</th><td><i>    // This is the return type of math functions such as tanh, logistic,</i></td></tr>
<tr><th id="1668">1668</th><td><i>    // whose range is in [-1, 1].</i></td></tr>
<tr><th id="1669">1669</th><td>    <b>using</b> F_In = gemmlowp::FixedPoint&lt;int16x8_t, IntegerBits&gt;;</td></tr>
<tr><th id="1670">1670</th><td>    <b>using</b> F_Out = gemmlowp::FixedPoint&lt;int16x8_t, <var>0</var>&gt;;</td></tr>
<tr><th id="1671">1671</th><td></td></tr>
<tr><th id="1672">1672</th><td>    <b>for</b> (; i &lt;= n_input - <var>32</var>; i += <var>32</var>) {</td></tr>
<tr><th id="1673">1673</th><td>      <em>const</em> <em>int</em> index = batch * n_input + i;</td></tr>
<tr><th id="1674">1674</th><td>      F_In input0 = F_In::FromRaw(vld1q_s16(input + index));</td></tr>
<tr><th id="1675">1675</th><td>      F_In input1 = F_In::FromRaw(vld1q_s16(input + index + <var>8</var>));</td></tr>
<tr><th id="1676">1676</th><td>      F_In input2 = F_In::FromRaw(vld1q_s16(input + index + <var>16</var>));</td></tr>
<tr><th id="1677">1677</th><td>      F_In input3 = F_In::FromRaw(vld1q_s16(input + index + <var>24</var>));</td></tr>
<tr><th id="1678">1678</th><td>      F_Out output0 = gemmlowp::tanh(input0);</td></tr>
<tr><th id="1679">1679</th><td>      F_Out output1 = gemmlowp::tanh(input1);</td></tr>
<tr><th id="1680">1680</th><td>      F_Out output2 = gemmlowp::tanh(input2);</td></tr>
<tr><th id="1681">1681</th><td>      F_Out output3 = gemmlowp::tanh(input3);</td></tr>
<tr><th id="1682">1682</th><td>      vst1q_s16(output + index, output0.raw());</td></tr>
<tr><th id="1683">1683</th><td>      vst1q_s16(output + index + <var>8</var>, output1.raw());</td></tr>
<tr><th id="1684">1684</th><td>      vst1q_s16(output + index + <var>16</var>, output2.raw());</td></tr>
<tr><th id="1685">1685</th><td>      vst1q_s16(output + index + <var>24</var>, output3.raw());</td></tr>
<tr><th id="1686">1686</th><td>    }</td></tr>
<tr><th id="1687">1687</th><td><u>#endif  // GEMMLOWP_NEON</u></td></tr>
<tr><th id="1688">1688</th><td>    <b>using</b> F_In_Scalar = gemmlowp::FixedPoint&lt;int16_t, IntegerBits&gt;;</td></tr>
<tr><th id="1689">1689</th><td>    <b>using</b> F_Out_Scalar = gemmlowp::FixedPoint&lt;int16_t, <var>0</var>&gt;;</td></tr>
<tr><th id="1690">1690</th><td>    <b>for</b> (; i &lt; n_input; ++i) {</td></tr>
<tr><th id="1691">1691</th><td>      <em>const</em> <em>int</em> index = batch * n_input + i;</td></tr>
<tr><th id="1692">1692</th><td>      F_In_Scalar input_in = F_In_Scalar::FromRaw(input[index]);</td></tr>
<tr><th id="1693">1693</th><td>      F_Out_Scalar output_out = gemmlowp::tanh(input_in);</td></tr>
<tr><th id="1694">1694</th><td>      output[index] = output_out.raw();</td></tr>
<tr><th id="1695">1695</th><td>    }</td></tr>
<tr><th id="1696">1696</th><td>  }</td></tr>
<tr><th id="1697">1697</th><td>}</td></tr>
<tr><th id="1698">1698</th><td></td></tr>
<tr><th id="1699">1699</th><td><em>void</em> NeonApplyTanh(int32_t integer_bits, <em>const</em> int16_t* input, int32_t n_batch,</td></tr>
<tr><th id="1700">1700</th><td>                   int32_t n_input, int16_t* output) {</td></tr>
<tr><th id="1701">1701</th><td>  assert(integer_bits &lt;= <var>6</var>);</td></tr>
<tr><th id="1702">1702</th><td><u>#define DISPATCH_TANH(i)                                   \</u></td></tr>
<tr><th id="1703">1703</th><td><u>  case i:                                                  \</u></td></tr>
<tr><th id="1704">1704</th><td><u>    NeonApplyTanhImpl&lt;i&gt;(input, n_batch, n_input, output); \</u></td></tr>
<tr><th id="1705">1705</th><td><u>    break;</u></td></tr>
<tr><th id="1706">1706</th><td>  <b>switch</b> (integer_bits) {</td></tr>
<tr><th id="1707">1707</th><td>    DISPATCH_TANH(<var>0</var>);</td></tr>
<tr><th id="1708">1708</th><td>    DISPATCH_TANH(<var>1</var>);</td></tr>
<tr><th id="1709">1709</th><td>    DISPATCH_TANH(<var>2</var>);</td></tr>
<tr><th id="1710">1710</th><td>    DISPATCH_TANH(<var>3</var>);</td></tr>
<tr><th id="1711">1711</th><td>    DISPATCH_TANH(<var>4</var>);</td></tr>
<tr><th id="1712">1712</th><td>    DISPATCH_TANH(<var>5</var>);</td></tr>
<tr><th id="1713">1713</th><td>    DISPATCH_TANH(<var>6</var>);</td></tr>
<tr><th id="1714">1714</th><td>    <b>default</b>:</td></tr>
<tr><th id="1715">1715</th><td>      <b>return</b>;</td></tr>
<tr><th id="1716">1716</th><td>  }</td></tr>
<tr><th id="1717">1717</th><td><u>#undef DISPATCH_TANH</u></td></tr>
<tr><th id="1718">1718</th><td>}</td></tr>
<tr><th id="1719">1719</th><td></td></tr>
<tr><th id="1720">1720</th><td><em>void</em> NeonCwiseMul(<em>const</em> int16_t* input_1, <em>const</em> int16_t* input_2, <em>int</em> n_batch,</td></tr>
<tr><th id="1721">1721</th><td>                  <em>int</em> n_input, <em>int</em> shift, int16_t* output) {</td></tr>
<tr><th id="1722">1722</th><td>  <b>for</b> (<em>int</em> batch = <var>0</var>; batch &lt; n_batch; ++batch) {</td></tr>
<tr><th id="1723">1723</th><td>    <em>int</em> i = <var>0</var>;</td></tr>
<tr><th id="1724">1724</th><td>    <b>for</b> (; i &lt;= n_input - <var>8</var>; i += <var>8</var>) {</td></tr>
<tr><th id="1725">1725</th><td>      <em>const</em> <em>int</em> index = batch * n_input + i;</td></tr>
<tr><th id="1726">1726</th><td>      <em>const</em> int16x8_t a = vld1q_s16(input_1 + index);</td></tr>
<tr><th id="1727">1727</th><td>      <em>const</em> int16x8_t b = vld1q_s16(input_2 + index);</td></tr>
<tr><th id="1728">1728</th><td>      <em>const</em> int32x4_t a_s32_0 = vmovl_s16(vget_low_s16(a));</td></tr>
<tr><th id="1729">1729</th><td>      <em>const</em> int32x4_t a_s32_1 = vmovl_s16(vget_high_s16(a));</td></tr>
<tr><th id="1730">1730</th><td>      <em>const</em> int32x4_t b_s32_0 = vmovl_s16(vget_low_s16(b));</td></tr>
<tr><th id="1731">1731</th><td>      <em>const</em> int32x4_t b_s32_1 = vmovl_s16(vget_high_s16(b));</td></tr>
<tr><th id="1732">1732</th><td></td></tr>
<tr><th id="1733">1733</th><td>      int32x4_t x_0 = vmulq_s32(a_s32_0, b_s32_0);</td></tr>
<tr><th id="1734">1734</th><td>      int32x4_t x_1 = vmulq_s32(a_s32_1, b_s32_1);</td></tr>
<tr><th id="1735">1735</th><td>      x_0 = gemmlowp::RoundingDivideByPOT(x_0, shift);</td></tr>
<tr><th id="1736">1736</th><td>      x_1 = gemmlowp::RoundingDivideByPOT(x_1, shift);</td></tr>
<tr><th id="1737">1737</th><td></td></tr>
<tr><th id="1738">1738</th><td>      <em>const</em> int16x8_t result = vcombine_s16(vmovn_s32(x_0), vmovn_s32(x_1));</td></tr>
<tr><th id="1739">1739</th><td>      vst1q_s16(output + index, result);</td></tr>
<tr><th id="1740">1740</th><td>    }</td></tr>
<tr><th id="1741">1741</th><td>    <b>for</b> (; TFLITE_UNLIKELY(i &lt; n_input); ++i) {</td></tr>
<tr><th id="1742">1742</th><td>      <em>const</em> <em>int</em> index = batch * n_input + i;</td></tr>
<tr><th id="1743">1743</th><td>      <em>const</em> int16_t a = input_1[index];</td></tr>
<tr><th id="1744">1744</th><td>      <em>const</em> int16_t b = input_2[index];</td></tr>
<tr><th id="1745">1745</th><td>      int64_t x = a * b;</td></tr>
<tr><th id="1746">1746</th><td>      <b>if</b> (x &gt; std::numeric_limits&lt;std::int32_t&gt;::max()) {</td></tr>
<tr><th id="1747">1747</th><td>        x = std::numeric_limits&lt;std::int32_t&gt;::max();</td></tr>
<tr><th id="1748">1748</th><td>      }</td></tr>
<tr><th id="1749">1749</th><td>      <em>const</em> int32_t value = <b>static_cast</b>&lt;int32_t&gt;(x);</td></tr>
<tr><th id="1750">1750</th><td>      output[index] =</td></tr>
<tr><th id="1751">1751</th><td>          <b>static_cast</b>&lt;int16_t&gt;(gemmlowp::RoundingDivideByPOT(value, shift));</td></tr>
<tr><th id="1752">1752</th><td>    }</td></tr>
<tr><th id="1753">1753</th><td>  }</td></tr>
<tr><th id="1754">1754</th><td>}</td></tr>
<tr><th id="1755">1755</th><td></td></tr>
<tr><th id="1756">1756</th><td><em>void</em> NeonCwiseMul(<em>const</em> int16_t* input_1, <em>const</em> int16_t* input_2,</td></tr>
<tr><th id="1757">1757</th><td>                  int32_t multiplier, <em>int</em> shift, <em>int</em> n_batch, <em>int</em> n_input,</td></tr>
<tr><th id="1758">1758</th><td>                  int32_t output_zp, int8_t* output) {</td></tr>
<tr><th id="1759">1759</th><td>  <em>const</em> int32_t output_min = std::numeric_limits&lt;int8_t&gt;::min();</td></tr>
<tr><th id="1760">1760</th><td>  <em>const</em> int32_t output_max = std::numeric_limits&lt;int8_t&gt;::max();</td></tr>
<tr><th id="1761">1761</th><td></td></tr>
<tr><th id="1762">1762</th><td>  <em>const</em> int32x4_t output_zp_dup = vdupq_n_s32(-output_zp);</td></tr>
<tr><th id="1763">1763</th><td>  <em>const</em> int32x4_t max_val_dup = vdupq_n_s32(output_max);</td></tr>
<tr><th id="1764">1764</th><td>  <em>const</em> int32x4_t min_val_dup = vdupq_n_s32(output_min);</td></tr>
<tr><th id="1765">1765</th><td></td></tr>
<tr><th id="1766">1766</th><td>  <b>for</b> (<em>int</em> batch = <var>0</var>; batch &lt; n_batch; ++batch) {</td></tr>
<tr><th id="1767">1767</th><td>    <em>int</em> i = <var>0</var>;</td></tr>
<tr><th id="1768">1768</th><td>    <b>for</b> (; i &lt;= n_input - <var>8</var>; i += <var>8</var>) {</td></tr>
<tr><th id="1769">1769</th><td>      <em>const</em> <em>int</em> index = batch * n_input + i;</td></tr>
<tr><th id="1770">1770</th><td>      <em>const</em> int16x8_t a = vld1q_s16(input_1 + index);</td></tr>
<tr><th id="1771">1771</th><td>      <em>const</em> int16x8_t b = vld1q_s16(input_2 + index);</td></tr>
<tr><th id="1772">1772</th><td>      <em>const</em> int32x4_t a_s32_0 = vmovl_s16(vget_low_s16(a));</td></tr>
<tr><th id="1773">1773</th><td>      <em>const</em> int32x4_t a_s32_1 = vmovl_s16(vget_high_s16(a));</td></tr>
<tr><th id="1774">1774</th><td>      <em>const</em> int32x4_t b_s32_0 = vmovl_s16(vget_low_s16(b));</td></tr>
<tr><th id="1775">1775</th><td>      <em>const</em> int32x4_t b_s32_1 = vmovl_s16(vget_high_s16(b));</td></tr>
<tr><th id="1776">1776</th><td></td></tr>
<tr><th id="1777">1777</th><td>      int32x4x2_t temp_val;</td></tr>
<tr><th id="1778">1778</th><td>      temp_val.val[<var>0</var>] = vmulq_s32(a_s32_0, b_s32_0);</td></tr>
<tr><th id="1779">1779</th><td>      temp_val.val[<var>1</var>] = vmulq_s32(a_s32_1, b_s32_1);</td></tr>
<tr><th id="1780">1780</th><td>      temp_val =</td></tr>
<tr><th id="1781">1781</th><td>          MultiplyByQuantizedMultiplier2Rows(temp_val, multiplier, shift);</td></tr>
<tr><th id="1782">1782</th><td></td></tr>
<tr><th id="1783">1783</th><td>      temp_val.val[<var>0</var>] = vaddq_s32(temp_val.val[<var>0</var>], output_zp_dup);</td></tr>
<tr><th id="1784">1784</th><td>      temp_val.val[<var>1</var>] = vaddq_s32(temp_val.val[<var>1</var>], output_zp_dup);</td></tr>
<tr><th id="1785">1785</th><td>      temp_val.val[<var>0</var>] =</td></tr>
<tr><th id="1786">1786</th><td>          vmaxq_s32(vminq_s32(temp_val.val[<var>0</var>], max_val_dup), min_val_dup);</td></tr>
<tr><th id="1787">1787</th><td>      temp_val.val[<var>1</var>] =</td></tr>
<tr><th id="1788">1788</th><td>          vmaxq_s32(vminq_s32(temp_val.val[<var>1</var>], max_val_dup), min_val_dup);</td></tr>
<tr><th id="1789">1789</th><td></td></tr>
<tr><th id="1790">1790</th><td>      <em>const</em> int16x8_t result =</td></tr>
<tr><th id="1791">1791</th><td>          vcombine_s16(vmovn_s32(temp_val.val[<var>0</var>]), vmovn_s32(temp_val.val[<var>1</var>]));</td></tr>
<tr><th id="1792">1792</th><td>      vst1_s8(output + index, vmovn_s16(result));</td></tr>
<tr><th id="1793">1793</th><td>    }</td></tr>
<tr><th id="1794">1794</th><td>    <b>for</b> (; TFLITE_UNLIKELY(i &lt; n_input); ++i) {</td></tr>
<tr><th id="1795">1795</th><td>      <em>const</em> <em>int</em> index = batch * n_input + i;</td></tr>
<tr><th id="1796">1796</th><td>      <em>const</em> int16_t a = input_1[index];</td></tr>
<tr><th id="1797">1797</th><td>      <em>const</em> int16_t b = input_2[index];</td></tr>
<tr><th id="1798">1798</th><td>      int32_t value = <b>static_cast</b>&lt;int32_t&gt;(a) * <b>static_cast</b>&lt;int32_t&gt;(b);</td></tr>
<tr><th id="1799">1799</th><td>      value = MultiplyByQuantizedMultiplier(value, multiplier, shift);</td></tr>
<tr><th id="1800">1800</th><td>      value -= output_zp;</td></tr>
<tr><th id="1801">1801</th><td>      value = std::min(std::max(-<var>128</var>, value), <var>127</var>);</td></tr>
<tr><th id="1802">1802</th><td></td></tr>
<tr><th id="1803">1803</th><td>      output[index] = <b>static_cast</b>&lt;int8&gt;(value);</td></tr>
<tr><th id="1804">1804</th><td>    }</td></tr>
<tr><th id="1805">1805</th><td>  }</td></tr>
<tr><th id="1806">1806</th><td>}</td></tr>
<tr><th id="1807">1807</th><td></td></tr>
<tr><th id="1808">1808</th><td><em>void</em> NeonCwiseAdd(<em>const</em> int16_t* input_1, <em>const</em> int16_t* input_2, <em>int</em> n_batch,</td></tr>
<tr><th id="1809">1809</th><td>                  <em>int</em> n_input, int16_t* output) {</td></tr>
<tr><th id="1810">1810</th><td>  <em>const</em> int32 int16_max = std::numeric_limits&lt;int16&gt;::max();</td></tr>
<tr><th id="1811">1811</th><td>  <em>const</em> int32 int16_min = std::numeric_limits&lt;int16&gt;::min();</td></tr>
<tr><th id="1812">1812</th><td>  <b>for</b> (<em>int</em> batch = <var>0</var>; batch &lt; n_batch; ++batch) {</td></tr>
<tr><th id="1813">1813</th><td>    <em>int</em> i = <var>0</var>;</td></tr>
<tr><th id="1814">1814</th><td>    <b>for</b> (; i &lt;= n_input - <var>8</var>; i += <var>8</var>) {</td></tr>
<tr><th id="1815">1815</th><td>      <em>const</em> <em>int</em> index = batch * n_input + i;</td></tr>
<tr><th id="1816">1816</th><td>      <em>const</em> int16x8_t a = vld1q_s16(input_1 + index);</td></tr>
<tr><th id="1817">1817</th><td>      <em>const</em> int16x8_t b = vld1q_s16(input_2 + index);</td></tr>
<tr><th id="1818">1818</th><td>      <em>const</em> int32x4_t a_s32_0 = vmovl_s16(vget_low_s16(a));</td></tr>
<tr><th id="1819">1819</th><td>      <em>const</em> int32x4_t a_s32_1 = vmovl_s16(vget_high_s16(a));</td></tr>
<tr><th id="1820">1820</th><td>      <em>const</em> int32x4_t b_s32_0 = vmovl_s16(vget_low_s16(b));</td></tr>
<tr><th id="1821">1821</th><td>      <em>const</em> int32x4_t b_s32_1 = vmovl_s16(vget_high_s16(b));</td></tr>
<tr><th id="1822">1822</th><td></td></tr>
<tr><th id="1823">1823</th><td>      <em>const</em> int32x4_t sum_0 = vaddq_s32(a_s32_0, b_s32_0);</td></tr>
<tr><th id="1824">1824</th><td>      <em>const</em> int32x4_t sum_1 = vaddq_s32(a_s32_1, b_s32_1);</td></tr>
<tr><th id="1825">1825</th><td>      vst1_s16(output + index, vqmovn_s32(sum_0));</td></tr>
<tr><th id="1826">1826</th><td>      vst1_s16(output + index + <var>4</var>, vqmovn_s32(sum_1));</td></tr>
<tr><th id="1827">1827</th><td>    }</td></tr>
<tr><th id="1828">1828</th><td>    <b>for</b> (; TFLITE_UNLIKELY(i &lt; n_input); ++i) {</td></tr>
<tr><th id="1829">1829</th><td>      <em>const</em> <em>int</em> index = batch * n_input + i;</td></tr>
<tr><th id="1830">1830</th><td>      int32_t sum = input_1[index] + input_2[index];</td></tr>
<tr><th id="1831">1831</th><td>      <em>const</em> int32 sum_clamped = std::min(int16_max, std::max(int16_min, sum));</td></tr>
<tr><th id="1832">1832</th><td>      output[index] = <b>static_cast</b>&lt;int16_t&gt;(sum_clamped);</td></tr>
<tr><th id="1833">1833</th><td>    }</td></tr>
<tr><th id="1834">1834</th><td>  }</td></tr>
<tr><th id="1835">1835</th><td>}</td></tr>
<tr><th id="1836">1836</th><td></td></tr>
<tr><th id="1837">1837</th><td><em>void</em> NeonCwiseClipping(<em>float</em>* vector, <em>const</em> <em>int</em> v_size,</td></tr>
<tr><th id="1838">1838</th><td>                       <em>const</em> <em>float</em> clipping_value) {</td></tr>
<tr><th id="1839">1839</th><td>  <em>const</em> float32x4_t clipping_value_f32x4 = vmovq_n_f32(clipping_value);</td></tr>
<tr><th id="1840">1840</th><td>  <em>const</em> float32x4_t neg_clipping_value_f32x4 = vmovq_n_f32(-clipping_value);</td></tr>
<tr><th id="1841">1841</th><td></td></tr>
<tr><th id="1842">1842</th><td>  <em>int</em> i = <var>0</var>;</td></tr>
<tr><th id="1843">1843</th><td>  <b>for</b> (; i &lt;= v_size - kFloatValuesPerNeonVector;</td></tr>
<tr><th id="1844">1844</th><td>       i += kFloatValuesPerNeonVector) {</td></tr>
<tr><th id="1845">1845</th><td>    <i>// Load from memory to vector.</i></td></tr>
<tr><th id="1846">1846</th><td>    float32x4_t v_f32x4 = vld1q_f32(vector + i);</td></tr>
<tr><th id="1847">1847</th><td>    <i>// Clip between clipping_value and -clipping_value.</i></td></tr>
<tr><th id="1848">1848</th><td>    v_f32x4 = vminq_f32(clipping_value_f32x4, v_f32x4);</td></tr>
<tr><th id="1849">1849</th><td>    v_f32x4 = vmaxq_f32(neg_clipping_value_f32x4, v_f32x4);</td></tr>
<tr><th id="1850">1850</th><td>    <i>// Save to output.</i></td></tr>
<tr><th id="1851">1851</th><td>    vst1q_f32(vector + i, v_f32x4);</td></tr>
<tr><th id="1852">1852</th><td>  }</td></tr>
<tr><th id="1853">1853</th><td>  <b>for</b> (; TFLITE_UNLIKELY(i &lt; v_size); i++) {</td></tr>
<tr><th id="1854">1854</th><td>    vector[i] = std::max(std::min(clipping_value, vector[i]), -clipping_value);</td></tr>
<tr><th id="1855">1855</th><td>  }</td></tr>
<tr><th id="1856">1856</th><td>}</td></tr>
<tr><th id="1857">1857</th><td></td></tr>
<tr><th id="1858">1858</th><td><em>void</em> NeonCwiseClipping(int16_t* vector, <em>const</em> <em>int</em> v_size,</td></tr>
<tr><th id="1859">1859</th><td>                       <em>const</em> int16_t clipping_value) {</td></tr>
<tr><th id="1860">1860</th><td>  <em>const</em> int16x8_t max_dup = vdupq_n_s16(clipping_value);</td></tr>
<tr><th id="1861">1861</th><td>  <em>const</em> int16x8_t min_dup = vdupq_n_s16(-clipping_value);</td></tr>
<tr><th id="1862">1862</th><td></td></tr>
<tr><th id="1863">1863</th><td>  <em>int</em> i = <var>0</var>;</td></tr>
<tr><th id="1864">1864</th><td>  <b>for</b> (; i &lt;= v_size - kInt16ValuesPerNeonVector * <var>2</var>;</td></tr>
<tr><th id="1865">1865</th><td>       i += kInt16ValuesPerNeonVector * <var>2</var>) {</td></tr>
<tr><th id="1866">1866</th><td>    int16x8_t val_0 = vld1q_s16(vector + i);</td></tr>
<tr><th id="1867">1867</th><td>    int16x8_t val_1 = vld1q_s16(vector + i + kInt16ValuesPerNeonVector);</td></tr>
<tr><th id="1868">1868</th><td>    val_0 = vminq_s16(val_0, max_dup);</td></tr>
<tr><th id="1869">1869</th><td>    val_1 = vminq_s16(val_1, max_dup);</td></tr>
<tr><th id="1870">1870</th><td>    val_0 = vmaxq_s16(val_0, min_dup);</td></tr>
<tr><th id="1871">1871</th><td>    val_1 = vmaxq_s16(val_1, min_dup);</td></tr>
<tr><th id="1872">1872</th><td>    vst1q_s16(vector + i, val_0);</td></tr>
<tr><th id="1873">1873</th><td>    vst1q_s16(vector + i + kInt16ValuesPerNeonVector, val_1);</td></tr>
<tr><th id="1874">1874</th><td>  }</td></tr>
<tr><th id="1875">1875</th><td>  <b>for</b> (; TFLITE_UNLIKELY(i &lt; v_size); i++) {</td></tr>
<tr><th id="1876">1876</th><td>    vector[i] = std::max(std::min(clipping_value, vector[i]),</td></tr>
<tr><th id="1877">1877</th><td>                         <b>static_cast</b>&lt;int16_t&gt;(-clipping_value));</td></tr>
<tr><th id="1878">1878</th><td>  }</td></tr>
<tr><th id="1879">1879</th><td>}</td></tr>
<tr><th id="1880">1880</th><td></td></tr>
<tr><th id="1881">1881</th><td><em>void</em> NeonCwiseClipping(int8_t* vector, <em>const</em> <em>int</em> v_size,</td></tr>
<tr><th id="1882">1882</th><td>                       <em>const</em> int8_t clipping_value) {</td></tr>
<tr><th id="1883">1883</th><td>  <em>const</em> int8x16_t max_dup = vdupq_n_s8(clipping_value);</td></tr>
<tr><th id="1884">1884</th><td>  <em>const</em> int8x16_t min_dup = vdupq_n_s8(-clipping_value);</td></tr>
<tr><th id="1885">1885</th><td></td></tr>
<tr><th id="1886">1886</th><td>  <em>int</em> i = <var>0</var>;</td></tr>
<tr><th id="1887">1887</th><td>  <b>for</b> (; i &lt; v_size - kInt8ValuesPerNeonVector * <var>2</var>;</td></tr>
<tr><th id="1888">1888</th><td>       i += kInt8ValuesPerNeonVector * <var>2</var>) {</td></tr>
<tr><th id="1889">1889</th><td>    int8x16_t val_0 = vld1q_s8(vector + i);</td></tr>
<tr><th id="1890">1890</th><td>    int8x16_t val_1 = vld1q_s8(vector + i + kInt8ValuesPerNeonVector);</td></tr>
<tr><th id="1891">1891</th><td>    val_0 = vminq_s8(val_0, max_dup);</td></tr>
<tr><th id="1892">1892</th><td>    val_1 = vminq_s8(val_1, max_dup);</td></tr>
<tr><th id="1893">1893</th><td>    val_0 = vmaxq_s8(val_0, min_dup);</td></tr>
<tr><th id="1894">1894</th><td>    val_1 = vmaxq_s8(val_1, min_dup);</td></tr>
<tr><th id="1895">1895</th><td>    vst1q_s8(vector + i, val_0);</td></tr>
<tr><th id="1896">1896</th><td>    vst1q_s8(vector + i + kInt8ValuesPerNeonVector, val_1);</td></tr>
<tr><th id="1897">1897</th><td>  }</td></tr>
<tr><th id="1898">1898</th><td>  <b>for</b> (; TFLITE_UNLIKELY(i &lt; v_size); i++) {</td></tr>
<tr><th id="1899">1899</th><td>    vector[i] = std::max(std::min(clipping_value, vector[i]),</td></tr>
<tr><th id="1900">1900</th><td>                         <b>static_cast</b>&lt;int8_t&gt;(-clipping_value));</td></tr>
<tr><th id="1901">1901</th><td>  }</td></tr>
<tr><th id="1902">1902</th><td>}</td></tr>
<tr><th id="1903">1903</th><td></td></tr>
<tr><th id="1904">1904</th><td><em>void</em> NeonSparseMatrixBatchVectorMultiplyAccumulate1x4(</td></tr>
<tr><th id="1905">1905</th><td>    <em>const</em> <em>float</em>* <b>__restrict__</b> matrix, <em>const</em> int32_t* <b>__restrict__</b> segments,</td></tr>
<tr><th id="1906">1906</th><td>    <em>const</em> int32_t* <b>__restrict__</b> indices, <em>int</em> m_rows, <em>int</em> m_cols,</td></tr>
<tr><th id="1907">1907</th><td>    <em>const</em> <em>float</em>* <b>__restrict__</b> vector, <em>int</em> n_batch, <em>float</em>* <b>__restrict__</b> result) {</td></tr>
<tr><th id="1908">1908</th><td>  <b>constexpr</b> <em>int</em> kBlockSize = kFloatValuesPerNeonVector;</td></tr>
<tr><th id="1909">1909</th><td>  TFLITE_DCHECK_EQ(m_cols % kBlockSize, <var>0</var>);</td></tr>
<tr><th id="1910">1910</th><td></td></tr>
<tr><th id="1911">1911</th><td>  <b>for</b> (<em>int</em> batch = <var>0</var>; batch &lt; n_batch; batch++) {</td></tr>
<tr><th id="1912">1912</th><td>    <em>const</em> <em>float</em>* matrix_ptr = matrix;</td></tr>
<tr><th id="1913">1913</th><td>    <b>for</b> (<em>int</em> row = <var>0</var>; row &lt; m_rows; row++) {</td></tr>
<tr><th id="1914">1914</th><td>      float32x4_t acc_32x4 = vmovq_n_f32(<var>0.0</var>);</td></tr>
<tr><th id="1915">1915</th><td>      <em>const</em> <em>float</em>* vector_in_batch = vector + batch * m_cols;</td></tr>
<tr><th id="1916">1916</th><td></td></tr>
<tr><th id="1917">1917</th><td>      <b>for</b> (<em>int</em> i = segments[row]; i &lt; segments[row + <var>1</var>]; i++) {</td></tr>
<tr><th id="1918">1918</th><td>        <em>const</em> <em>int</em> block_start_index = indices[i] * kBlockSize;</td></tr>
<tr><th id="1919">1919</th><td>        <em>const</em> <em>float</em>* vector_block_in_batch_ptr =</td></tr>
<tr><th id="1920">1920</th><td>            vector_in_batch + block_start_index;</td></tr>
<tr><th id="1921">1921</th><td></td></tr>
<tr><th id="1922">1922</th><td>        <i>// Load 4 float values from the vector and matrix row.</i></td></tr>
<tr><th id="1923">1923</th><td>        float32x4_t vector_f32x4 = vld1q_f32(vector_block_in_batch_ptr);</td></tr>
<tr><th id="1924">1924</th><td>        float32x4_t matrix_f32x4 = vld1q_f32(matrix_ptr);</td></tr>
<tr><th id="1925">1925</th><td>        <i>// Multiply the vector and matrix row and add to accumulator.</i></td></tr>
<tr><th id="1926">1926</th><td>        acc_32x4 = vmlaq_f32(acc_32x4, matrix_f32x4, vector_f32x4);</td></tr>
<tr><th id="1927">1927</th><td>        matrix_ptr += kBlockSize;</td></tr>
<tr><th id="1928">1928</th><td>      }</td></tr>
<tr><th id="1929">1929</th><td>      result[batch * m_rows + row] += AccumulateNeonLane(acc_32x4);</td></tr>
<tr><th id="1930">1930</th><td>    }</td></tr>
<tr><th id="1931">1931</th><td>  }</td></tr>
<tr><th id="1932">1932</th><td>}</td></tr>
<tr><th id="1933">1933</th><td></td></tr>
<tr><th id="1934">1934</th><td><em>void</em> NeonSparseMatrixBatchVectorMultiplyAccumulate(</td></tr>
<tr><th id="1935">1935</th><td>    <em>const</em> <em>float</em>* <b>__restrict__</b> matrix, <em>const</em> uint8_t* <b>__restrict__</b> ledger,</td></tr>
<tr><th id="1936">1936</th><td>    <em>int</em> m_rows, <em>int</em> m_cols, <em>const</em> <em>float</em>* <b>__restrict__</b> vector, <em>int</em> n_batch,</td></tr>
<tr><th id="1937">1937</th><td>    <em>float</em>* <b>__restrict__</b> result) {</td></tr>
<tr><th id="1938">1938</th><td>  <b>constexpr</b> <em>int</em> kNeonVectorsPerBlock = <var>4</var>;</td></tr>
<tr><th id="1939">1939</th><td>  <b>constexpr</b> <em>int</em> kBlockSize = kNeonVectorsPerBlock * kFloatValuesPerNeonVector;</td></tr>
<tr><th id="1940">1940</th><td>  TFLITE_DCHECK_EQ(  <i>// NOLINT</i></td></tr>
<tr><th id="1941">1941</th><td>      m_cols % kBlockSize, <var>0</var>);</td></tr>
<tr><th id="1942">1942</th><td></td></tr>
<tr><th id="1943">1943</th><td>  <b>for</b> (<em>int</em> batch = <var>0</var>; batch &lt; n_batch; batch++) {</td></tr>
<tr><th id="1944">1944</th><td>    <em>const</em> <em>float</em>* matrix_ptr = matrix;</td></tr>
<tr><th id="1945">1945</th><td>    <em>const</em> uint8_t* ledger_ptr = ledger;</td></tr>
<tr><th id="1946">1946</th><td>    <b>for</b> (<em>int</em> row = <var>0</var>; row &lt; m_rows; row++) {</td></tr>
<tr><th id="1947">1947</th><td>      <em>int</em> num_nonzero_blocks = *ledger_ptr++;</td></tr>
<tr><th id="1948">1948</th><td>      <b>if</b> (num_nonzero_blocks &gt; <var>0</var>) {</td></tr>
<tr><th id="1949">1949</th><td>        float32x4_t acc_32x4 = vmovq_n_f32(<var>0.0</var>);</td></tr>
<tr><th id="1950">1950</th><td>        <em>const</em> <em>float</em>* vector_in_batch = vector + batch * m_cols;</td></tr>
<tr><th id="1951">1951</th><td></td></tr>
<tr><th id="1952">1952</th><td>        <b>for</b> (<em>int</em> i = <var>0</var>; i &lt; num_nonzero_blocks; i++) {</td></tr>
<tr><th id="1953">1953</th><td>          <em>const</em> <em>int</em> block_start_index = *ledger_ptr++ * kBlockSize;</td></tr>
<tr><th id="1954">1954</th><td>          <em>const</em> <em>float</em>* vector_block_in_batch_ptr =</td></tr>
<tr><th id="1955">1955</th><td>              vector_in_batch + block_start_index;</td></tr>
<tr><th id="1956">1956</th><td></td></tr>
<tr><th id="1957">1957</th><td>          <b>for</b> (<em>int</em> c = <var>0</var>; c &lt; kNeonVectorsPerBlock; c++) {</td></tr>
<tr><th id="1958">1958</th><td>            <i>// Load 4 float values from the vector and matrix row.</i></td></tr>
<tr><th id="1959">1959</th><td>            float32x4_t vector_f32x4 = vld1q_f32(vector_block_in_batch_ptr +</td></tr>
<tr><th id="1960">1960</th><td>                                                 c * kFloatValuesPerNeonVector);</td></tr>
<tr><th id="1961">1961</th><td>            float32x4_t matrix_f32x4 =</td></tr>
<tr><th id="1962">1962</th><td>                vld1q_f32(matrix_ptr + c * kFloatValuesPerNeonVector);</td></tr>
<tr><th id="1963">1963</th><td>            <i>// Multiply the vector and matrix row and add to accumulator.</i></td></tr>
<tr><th id="1964">1964</th><td>            acc_32x4 = vmlaq_f32(acc_32x4, matrix_f32x4, vector_f32x4);</td></tr>
<tr><th id="1965">1965</th><td>          }</td></tr>
<tr><th id="1966">1966</th><td>          matrix_ptr += kBlockSize;</td></tr>
<tr><th id="1967">1967</th><td>        }</td></tr>
<tr><th id="1968">1968</th><td>        result[batch * m_rows + row] += AccumulateNeonLane(acc_32x4);</td></tr>
<tr><th id="1969">1969</th><td>      }</td></tr>
<tr><th id="1970">1970</th><td>    }</td></tr>
<tr><th id="1971">1971</th><td>  }</td></tr>
<tr><th id="1972">1972</th><td>}</td></tr>
<tr><th id="1973">1973</th><td></td></tr>
<tr><th id="1974">1974</th><td><em>void</em> NeonSparseMatrixBatchVectorMultiplyAccumulate(</td></tr>
<tr><th id="1975">1975</th><td>    <em>const</em> int8_t* <b>__restrict__</b> matrix, <em>const</em> uint8_t* ledger, <em>const</em> <em>int</em> m_rows,</td></tr>
<tr><th id="1976">1976</th><td>    <em>const</em> <em>int</em> m_cols, <em>const</em> int8_t* <b>__restrict__</b> vectors,</td></tr>
<tr><th id="1977">1977</th><td>    <em>const</em> <em>float</em>* scaling_factors, <em>int</em> n_batch, <em>float</em>* <b>__restrict__</b> result) {</td></tr>
<tr><th id="1978">1978</th><td><u>#ifdef __aarch64__</u></td></tr>
<tr><th id="1979">1979</th><td>  <b>if</b> (HasSdotInstruction() &amp;&amp; m_cols % <var>16</var> == <var>0</var>) {</td></tr>
<tr><th id="1980">1980</th><td>    DotprodSparseMatrixBatchVectorMultiplyAccumulate(</td></tr>
<tr><th id="1981">1981</th><td>        matrix, ledger, m_rows, m_cols, vectors, scaling_factors, n_batch,</td></tr>
<tr><th id="1982">1982</th><td>        result);</td></tr>
<tr><th id="1983">1983</th><td>    <b>return</b>;</td></tr>
<tr><th id="1984">1984</th><td>  }</td></tr>
<tr><th id="1985">1985</th><td><u>#endif  // __aarch64__</u></td></tr>
<tr><th id="1986">1986</th><td></td></tr>
<tr><th id="1987">1987</th><td>  <b>constexpr</b> <em>int</em> kBlockSize = kInt8ValuesPerNeonVector;</td></tr>
<tr><th id="1988">1988</th><td>  TFLITE_DCHECK_EQ(  <i>// NOLINT</i></td></tr>
<tr><th id="1989">1989</th><td>      m_cols % kBlockSize, <var>0</var>);</td></tr>
<tr><th id="1990">1990</th><td>  <em>void</em>* aligned_vec_free = <b>nullptr</b>;</td></tr>
<tr><th id="1991">1991</th><td>  int8_t* aligned_vec =</td></tr>
<tr><th id="1992">1992</th><td>      (int8_t*)aligned_alloc(kNeonVectorAlignment, m_cols,  <i>// NOLINT</i></td></tr>
<tr><th id="1993">1993</th><td>                             &amp;aligned_vec_free);</td></tr>
<tr><th id="1994">1994</th><td></td></tr>
<tr><th id="1995">1995</th><td>  <b>for</b> (<em>int</em> batch = <var>0</var>; batch &lt; n_batch; ++batch) {</td></tr>
<tr><th id="1996">1996</th><td>    <em>const</em> <em>float</em> batch_scaling_factor = scaling_factors[batch];</td></tr>
<tr><th id="1997">1997</th><td>    <i>// Copy the vector data to an aligned vector.</i></td></tr>
<tr><th id="1998">1998</th><td>    memcpy(aligned_vec, vectors + batch * m_cols, <b>sizeof</b>(int8) * m_cols);</td></tr>
<tr><th id="1999">1999</th><td></td></tr>
<tr><th id="2000">2000</th><td>    <em>const</em> uint8_t* ledger_ptr = ledger;</td></tr>
<tr><th id="2001">2001</th><td>    <em>const</em> int8_t* row_ptr = matrix;</td></tr>
<tr><th id="2002">2002</th><td>    <b>for</b> (<em>int</em> row = <var>0</var>; row &lt; m_rows; ++row) {</td></tr>
<tr><th id="2003">2003</th><td>      <i>// Initialize the dot product sum for the row to 0.</i></td></tr>
<tr><th id="2004">2004</th><td>      int32x4_t dotprod_32x4 = vmovq_n_s32(<var>0</var>);</td></tr>
<tr><th id="2005">2005</th><td>      <em>int</em> num_nonzero_blocks = *ledger_ptr++;</td></tr>
<tr><th id="2006">2006</th><td>      <b>if</b> (num_nonzero_blocks &gt; <var>0</var>) {</td></tr>
<tr><th id="2007">2007</th><td>        <i>// Prefetch the row to cache.</i></td></tr>
<tr><th id="2008">2008</th><td>        __builtin_prefetch(row_ptr, <var>0</var> <i>/* prefetch for read */</i>,</td></tr>
<tr><th id="2009">2009</th><td>                           <var>3</var> <i>/* temporal locality */</i>);</td></tr>
<tr><th id="2010">2010</th><td>        <b>for</b> (<em>int</em> i = <var>0</var>; i &lt; num_nonzero_blocks; i++) {</td></tr>
<tr><th id="2011">2011</th><td>          <em>const</em> <em>int</em> col_index = *ledger_ptr++ * kBlockSize;</td></tr>
<tr><th id="2012">2012</th><td>          <i>// Load 16 8-bit values from the row and vector, each, to operate on.</i></td></tr>
<tr><th id="2013">2013</th><td><i>          // Here the assumption is that each buffer is 4-byte aligned.</i></td></tr>
<tr><th id="2014">2014</th><td><i>          // Otherwise, performance may suffer significantly.</i></td></tr>
<tr><th id="2015">2015</th><td>          TFLITE_DCHECK_EQ(  <i>// NOLINT</i></td></tr>
<tr><th id="2016">2016</th><td>              (uintptr_t)(&amp;row_ptr) &amp; (kNeonVectorAlignment - <var>1</var>), <var>0</var>);</td></tr>
<tr><th id="2017">2017</th><td>          <em>const</em> int8x16_t s1_8x16 =</td></tr>
<tr><th id="2018">2018</th><td>              vld1q_s8((<em>const</em> int8_t*)(aligned_vec + col_index));</td></tr>
<tr><th id="2019">2019</th><td>          <em>const</em> int8x16_t s2_8x16 = vld1q_s8((<em>const</em> int8_t*)(row_ptr));</td></tr>
<tr><th id="2020">2020</th><td>          <i>// Multiply the low bits (i.e. the lower 8 8bit numbers in the</i></td></tr>
<tr><th id="2021">2021</th><td><i>          // registers).</i></td></tr>
<tr><th id="2022">2022</th><td>          int16x8_t prod_16x8 =</td></tr>
<tr><th id="2023">2023</th><td>              vmull_s8(vget_low_s8(s1_8x16), vget_low_s8(s2_8x16));</td></tr>
<tr><th id="2024">2024</th><td>          <i>// Multiply the high bits (i.e. the lower 8 8bit numbers in the</i></td></tr>
<tr><th id="2025">2025</th><td><i>          // registers), and accumulate with the result of the low bits product.</i></td></tr>
<tr><th id="2026">2026</th><td><i>          // The assumption here is that overflow will not happen as we quantize</i></td></tr>
<tr><th id="2027">2027</th><td><i>          // our values to be in the range [-127, 127]. As such the sum of the 2</i></td></tr>
<tr><th id="2028">2028</th><td><i>          // products is always strictly smaller than 15-bits (32767 in absolute</i></td></tr>
<tr><th id="2029">2029</th><td><i>          // value).</i></td></tr>
<tr><th id="2030">2030</th><td>          prod_16x8 =</td></tr>
<tr><th id="2031">2031</th><td>              vmlal_s8(prod_16x8, vget_high_s8(s1_8x16), vget_high_s8(s2_8x16));</td></tr>
<tr><th id="2032">2032</th><td></td></tr>
<tr><th id="2033">2033</th><td>          dotprod_32x4 = vpadalq_s16(dotprod_32x4, prod_16x8);</td></tr>
<tr><th id="2034">2034</th><td>          row_ptr += kBlockSize;</td></tr>
<tr><th id="2035">2035</th><td>        }</td></tr>
<tr><th id="2036">2036</th><td>        <i>// Add the 4 intermediate sum values to get the final dot-prod value for</i></td></tr>
<tr><th id="2037">2037</th><td><i>        // this row.</i></td></tr>
<tr><th id="2038">2038</th><td>        int32_t dotprod = AccumulateNeonLane(dotprod_32x4);</td></tr>
<tr><th id="2039">2039</th><td>        result[batch * m_rows + row] += dotprod * batch_scaling_factor;</td></tr>
<tr><th id="2040">2040</th><td>      }</td></tr>
<tr><th id="2041">2041</th><td>    }  <i>// for row</i></td></tr>
<tr><th id="2042">2042</th><td>  }    <i>// for batch</i></td></tr>
<tr><th id="2043">2043</th><td>  free(aligned_vec_free);</td></tr>
<tr><th id="2044">2044</th><td>}</td></tr>
<tr><th id="2045">2045</th><td></td></tr>
<tr><th id="2046">2046</th><td><em>void</em> NeonSub1Vector(<em>const</em> <em>float</em>* vector, <em>int</em> v_size, <em>float</em>* result) {</td></tr>
<tr><th id="2047">2047</th><td>  <i>// If v_size is not divisible by the vector size, then we need to process the</i></td></tr>
<tr><th id="2048">2048</th><td><i>  // final few elements sequentially. postamble_start shows the start index</i></td></tr>
<tr><th id="2049">2049</th><td><i>  // where this should happen.</i></td></tr>
<tr><th id="2050">2050</th><td>  <em>const</em> <em>int</em> postamble_start =</td></tr>
<tr><th id="2051">2051</th><td>      RoundDownVectors&lt;kFloatValuesPerNeonVector&gt;(v_size);</td></tr>
<tr><th id="2052">2052</th><td></td></tr>
<tr><th id="2053">2053</th><td>  float32x4_t one_f32x4 = vmovq_n_f32(<var>1.0</var>);</td></tr>
<tr><th id="2054">2054</th><td>  <em>int</em> v = <var>0</var>;</td></tr>
<tr><th id="2055">2055</th><td>  <b>for</b> (; v &lt; postamble_start; v += kFloatValuesPerNeonVector) {</td></tr>
<tr><th id="2056">2056</th><td>    <i>// Load 4 float values from the current pointers of the input column and</i></td></tr>
<tr><th id="2057">2057</th><td><i>    // subtract from 1.</i></td></tr>
<tr><th id="2058">2058</th><td>    float32x4_t v_f32x4 = vld1q_f32(vector + v);</td></tr>
<tr><th id="2059">2059</th><td>    float32x4_t result_f32x4 = vsubq_f32(one_f32x4, v_f32x4);</td></tr>
<tr><th id="2060">2060</th><td>    <i>// Save to output.</i></td></tr>
<tr><th id="2061">2061</th><td>    vst1q_f32(result + v, result_f32x4);</td></tr>
<tr><th id="2062">2062</th><td>  }</td></tr>
<tr><th id="2063">2063</th><td>  <b>for</b> (; TFLITE_UNLIKELY(v &lt; v_size); v++) {</td></tr>
<tr><th id="2064">2064</th><td>    result[v] = <var>1.0f</var> - vector[v];</td></tr>
<tr><th id="2065">2065</th><td>  }</td></tr>
<tr><th id="2066">2066</th><td>}</td></tr>
<tr><th id="2067">2067</th><td></td></tr>
<tr><th id="2068">2068</th><td><em>void</em> NeonSub1Vector(<em>const</em> int16_t* vector, <em>int</em> v_size, int16_t* result) {</td></tr>
<tr><th id="2069">2069</th><td>  <em>int</em> postamble_start = RoundDownVectors&lt;kInt16ValuesPerNeonVector&gt;(v_size);</td></tr>
<tr><th id="2070">2070</th><td>  <em>static</em> <em>const</em> int16_t kOne = <var>32767</var>;</td></tr>
<tr><th id="2071">2071</th><td>  <i>// Use xor to replace substract from 1 &lt;&lt; 15 - 1.</i></td></tr>
<tr><th id="2072">2072</th><td><i>  // Local benchmark shows it's slightly faster than pure substract.</i></td></tr>
<tr><th id="2073">2073</th><td>  <em>const</em> int16x8_t one_dup = vdupq_n_s16(kOne);</td></tr>
<tr><th id="2074">2074</th><td>  <em>int</em> i = <var>0</var>;</td></tr>
<tr><th id="2075">2075</th><td>  <b>for</b> (; i &lt; postamble_start; i += kInt16ValuesPerNeonVector) {</td></tr>
<tr><th id="2076">2076</th><td>    <em>const</em> int16x8_t input = vld1q_s16(vector + i);</td></tr>
<tr><th id="2077">2077</th><td>    <em>const</em> int16x8_t sub1_result = veorq_s16(one_dup, input);</td></tr>
<tr><th id="2078">2078</th><td>    vst1q_s16(result + i, sub1_result);</td></tr>
<tr><th id="2079">2079</th><td>  }</td></tr>
<tr><th id="2080">2080</th><td>  <b>for</b> (; TFLITE_UNLIKELY(i &lt; v_size); i++) {</td></tr>
<tr><th id="2081">2081</th><td>    result[i] = kOne ^ vector[i];</td></tr>
<tr><th id="2082">2082</th><td>  }</td></tr>
<tr><th id="2083">2083</th><td>}</td></tr>
<tr><th id="2084">2084</th><td></td></tr>
<tr><th id="2085">2085</th><td><b>namespace</b> {</td></tr>
<tr><th id="2086">2086</th><td></td></tr>
<tr><th id="2087">2087</th><td><u>#ifdef __aarch64__</u></td></tr>
<tr><th id="2088">2088</th><td><b>inline</b> <em>bool</em> IsAllZero(<em>const</em> int8x16_t v_s8x16) {</td></tr>
<tr><th id="2089">2089</th><td>  <em>const</em> uint32_t u32 = vmaxvq_u32(vreinterpretq_u32_s8(v_s8x16));</td></tr>
<tr><th id="2090">2090</th><td>  <b>return</b> !u32;</td></tr>
<tr><th id="2091">2091</th><td>}</td></tr>
<tr><th id="2092">2092</th><td></td></tr>
<tr><th id="2093">2093</th><td><b>inline</b> <em>bool</em> IsAllZero(<em>const</em> float32x4_t v_f32x4) {</td></tr>
<tr><th id="2094">2094</th><td>  <em>const</em> uint32x4_t cmp_result = vceqzq_f32(v_f32x4);</td></tr>
<tr><th id="2095">2095</th><td>  <em>const</em> uint32_t u32 = vminvq_u32(cmp_result);</td></tr>
<tr><th id="2096">2096</th><td>  <b>return</b> u32;</td></tr>
<tr><th id="2097">2097</th><td>}</td></tr>
<tr><th id="2098">2098</th><td><u>#else</u></td></tr>
<tr><th id="2099">2099</th><td><b>inline</b> <em>bool</em> IsAllZero(<em>const</em> uint32x4_t u32x4) {</td></tr>
<tr><th id="2100">2100</th><td>  <em>const</em> uint32x2_t u32x2 = vqadd_u32(vget_high_u32(u32x4), vget_low_u32(u32x4));</td></tr>
<tr><th id="2101">2101</th><td>  <em>const</em> uint64x1_t u64 = vreinterpret_u64_u32(u32x2);</td></tr>
<tr><th id="2102">2102</th><td>  <b>return</b> !vget_lane_u64(u64, <var>0</var>);</td></tr>
<tr><th id="2103">2103</th><td>}</td></tr>
<tr><th id="2104">2104</th><td></td></tr>
<tr><th id="2105">2105</th><td><u>#ifndef __SSE__</u></td></tr>
<tr><th id="2106">2106</th><td><i>// With Intel NEON-2-SSE translator library, this is a redefinition..</i></td></tr>
<tr><th id="2107">2107</th><td><b>inline</b> <em>bool</em> IsAllZero(<em>const</em> int8x16_t v) {</td></tr>
<tr><th id="2108">2108</th><td>  <b>return</b> IsAllZero(vreinterpretq_u32_s8(v));</td></tr>
<tr><th id="2109">2109</th><td>}</td></tr>
<tr><th id="2110">2110</th><td><u>#endif</u></td></tr>
<tr><th id="2111">2111</th><td></td></tr>
<tr><th id="2112">2112</th><td><b>inline</b> <em>bool</em> IsAllZero(<em>const</em> float32x4_t v_f32x4) {</td></tr>
<tr><th id="2113">2113</th><td>  <em>const</em> float32x4_t zero_f32x4 = vmovq_n_f32(<var>0.0f</var>);</td></tr>
<tr><th id="2114">2114</th><td>  <i>// Compare-absolute greater-than, |v| &gt; |0|, equivalently v != 0</i></td></tr>
<tr><th id="2115">2115</th><td>  <em>const</em> uint32x4_t cmp_result = vcagtq_f32(v_f32x4, zero_f32x4);</td></tr>
<tr><th id="2116">2116</th><td>  <b>return</b> IsAllZero(cmp_result);</td></tr>
<tr><th id="2117">2117</th><td>}</td></tr>
<tr><th id="2118">2118</th><td><u>#endif</u></td></tr>
<tr><th id="2119">2119</th><td></td></tr>
<tr><th id="2120">2120</th><td>}  <i>// namespace</i></td></tr>
<tr><th id="2121">2121</th><td></td></tr>
<tr><th id="2122">2122</th><td><em>bool</em> NeonIsZeroVector(<em>const</em> <em>float</em>* vector, <em>int</em> v_size) {</td></tr>
<tr><th id="2123">2123</th><td>  <i>// If v_size is not divisible by the vector size, then we need to process the</i></td></tr>
<tr><th id="2124">2124</th><td><i>  // final few elements sequentially. postamble_start shows the start index</i></td></tr>
<tr><th id="2125">2125</th><td><i>  // where this should happen.</i></td></tr>
<tr><th id="2126">2126</th><td>  <em>const</em> <em>int</em> postamble_start =</td></tr>
<tr><th id="2127">2127</th><td>      RoundDownVectors&lt;kFloatValuesPerNeonVector&gt;(v_size);</td></tr>
<tr><th id="2128">2128</th><td></td></tr>
<tr><th id="2129">2129</th><td>  <em>int</em> v = <var>0</var>;</td></tr>
<tr><th id="2130">2130</th><td>  <b>for</b> (; v &lt; postamble_start; v += kFloatValuesPerNeonVector) {</td></tr>
<tr><th id="2131">2131</th><td>    <em>const</em> float32x4_t v_f32x4 = vld1q_f32(vector + v);</td></tr>
<tr><th id="2132">2132</th><td>    <b>if</b> (!IsAllZero(v_f32x4)) <b>return</b> <b>false</b>;</td></tr>
<tr><th id="2133">2133</th><td>  }</td></tr>
<tr><th id="2134">2134</th><td>  <i>// Postamble loop</i></td></tr>
<tr><th id="2135">2135</th><td>  <b>for</b> (; TFLITE_UNLIKELY(v &lt; v_size); ++v) {</td></tr>
<tr><th id="2136">2136</th><td>    <b>if</b> (vector[v] != <var>0.0</var>) <b>return</b> <b>false</b>;</td></tr>
<tr><th id="2137">2137</th><td>  }</td></tr>
<tr><th id="2138">2138</th><td>  <b>return</b> <b>true</b>;</td></tr>
<tr><th id="2139">2139</th><td>}</td></tr>
<tr><th id="2140">2140</th><td></td></tr>
<tr><th id="2141">2141</th><td><em>bool</em> NeonIsZeroVector(<em>const</em> int8_t* vector, <em>int</em> v_size) {</td></tr>
<tr><th id="2142">2142</th><td>  <i>// If v_size is not divisible by the vector size, then we need to process the</i></td></tr>
<tr><th id="2143">2143</th><td><i>  // final few elements sequentially. postamble_start shows the start index</i></td></tr>
<tr><th id="2144">2144</th><td><i>  // where this should happen.</i></td></tr>
<tr><th id="2145">2145</th><td>  <em>const</em> <em>int</em> postamble_start =</td></tr>
<tr><th id="2146">2146</th><td>      RoundDownVectors&lt;kInt8ValuesPerNeonVector&gt;(v_size);</td></tr>
<tr><th id="2147">2147</th><td></td></tr>
<tr><th id="2148">2148</th><td>  <em>int</em> v = <var>0</var>;</td></tr>
<tr><th id="2149">2149</th><td>  <b>for</b> (; v &lt; postamble_start; v += kInt8ValuesPerNeonVector) {</td></tr>
<tr><th id="2150">2150</th><td>    <em>const</em> int8x16_t v_s8x16 = vld1q_s8(vector + v);</td></tr>
<tr><th id="2151">2151</th><td>    <b>if</b> (!IsAllZero(v_s8x16)) <b>return</b> <b>false</b>;</td></tr>
<tr><th id="2152">2152</th><td>  }</td></tr>
<tr><th id="2153">2153</th><td>  <i>// Postamble loop</i></td></tr>
<tr><th id="2154">2154</th><td>  <b>for</b> (; TFLITE_UNLIKELY(v &lt; v_size); ++v) {</td></tr>
<tr><th id="2155">2155</th><td>    <b>if</b> (vector[v] != <var>0</var>) <b>return</b> <b>false</b>;</td></tr>
<tr><th id="2156">2156</th><td>  }</td></tr>
<tr><th id="2157">2157</th><td>  <b>return</b> <b>true</b>;</td></tr>
<tr><th id="2158">2158</th><td>}</td></tr>
<tr><th id="2159">2159</th><td></td></tr>
<tr><th id="2160">2160</th><td><em>void</em> NeonVectorScalarMultiply(<em>const</em> int8_t* vector, <em>const</em> <em>int</em> v_size,</td></tr>
<tr><th id="2161">2161</th><td>                              <em>const</em> <em>float</em> scale, <em>float</em>* result) {</td></tr>
<tr><th id="2162">2162</th><td>  <i>// Here the assumption is that each buffer is 4-byte aligned.</i></td></tr>
<tr><th id="2163">2163</th><td>  TFLITE_CHECK_EQ((intptr_t)(&amp;vector[<var>0</var>]) &amp; (kNeonVectorAlignment - <var>1</var>), <var>0</var>);</td></tr>
<tr><th id="2164">2164</th><td>  <i>// If v_size is not divisible by kInt8ValuesPerNeonVector, we cannot use the</i></td></tr>
<tr><th id="2165">2165</th><td><i>  // main vectorized loop, and we need to process sequentially. postamble_start</i></td></tr>
<tr><th id="2166">2166</th><td><i>  // shows the start index where this should happen.</i></td></tr>
<tr><th id="2167">2167</th><td>  <em>const</em> <em>int</em> postamble_start =</td></tr>
<tr><th id="2168">2168</th><td>      RoundDownVectors&lt;kInt8ValuesPerNeonVector&gt;(v_size);</td></tr>
<tr><th id="2169">2169</th><td></td></tr>
<tr><th id="2170">2170</th><td>  <i>// Create a vector of 4 floats with the scale value.</i></td></tr>
<tr><th id="2171">2171</th><td>  <em>const</em> float32x4_t scale_f32x4 = vdupq_n_f32(scale);</td></tr>
<tr><th id="2172">2172</th><td>  <em>int</em> v = <var>0</var>;</td></tr>
<tr><th id="2173">2173</th><td>  <b>for</b> (; v &lt; postamble_start; v += kInt8ValuesPerNeonVector) {</td></tr>
<tr><th id="2174">2174</th><td>    <i>// Load int8 values, sixteen at a time.</i></td></tr>
<tr><th id="2175">2175</th><td>    <em>const</em> int8x16_t v_i8x16 = vld1q_s8(vector + v);</td></tr>
<tr><th id="2176">2176</th><td>    <i>// Split it into two components of size eight.</i></td></tr>
<tr><th id="2177">2177</th><td>    <em>const</em> int8x8_t v0_i8x8 = vget_low_s8(v_i8x16);</td></tr>
<tr><th id="2178">2178</th><td>    <em>const</em> int8x8_t v1_i8x8 = vget_high_s8(v_i8x16);</td></tr>
<tr><th id="2179">2179</th><td>    <i>// Convert both components to int16 first.</i></td></tr>
<tr><th id="2180">2180</th><td>    <em>const</em> int16x8_t v0_i16x8 = vmovl_s8(v0_i8x8);</td></tr>
<tr><th id="2181">2181</th><td>    <em>const</em> int16x8_t v1_i16x8 = vmovl_s8(v1_i8x8);</td></tr>
<tr><th id="2182">2182</th><td>    <i>// Split each of them into two components each.</i></td></tr>
<tr><th id="2183">2183</th><td>    <em>const</em> int16x4_t v0_i16x4 = vget_low_s16(v0_i16x8);</td></tr>
<tr><th id="2184">2184</th><td>    <em>const</em> int16x4_t v1_i16x4 = vget_high_s16(v0_i16x8);</td></tr>
<tr><th id="2185">2185</th><td>    <em>const</em> int16x4_t v2_i16x4 = vget_low_s16(v1_i16x8);</td></tr>
<tr><th id="2186">2186</th><td>    <em>const</em> int16x4_t v3_i16x4 = vget_high_s16(v1_i16x8);</td></tr>
<tr><th id="2187">2187</th><td>    <i>// Convert these to int32 and then to float.</i></td></tr>
<tr><th id="2188">2188</th><td>    float32x4_t v0_f32x4 = vcvtq_f32_s32(vmovl_s16(v0_i16x4));</td></tr>
<tr><th id="2189">2189</th><td>    float32x4_t v1_f32x4 = vcvtq_f32_s32(vmovl_s16(v1_i16x4));</td></tr>
<tr><th id="2190">2190</th><td>    float32x4_t v2_f32x4 = vcvtq_f32_s32(vmovl_s16(v2_i16x4));</td></tr>
<tr><th id="2191">2191</th><td>    float32x4_t v3_f32x4 = vcvtq_f32_s32(vmovl_s16(v3_i16x4));</td></tr>
<tr><th id="2192">2192</th><td>    <i>// Vector multiply four floats at a time.</i></td></tr>
<tr><th id="2193">2193</th><td>    v0_f32x4 = vmulq_f32(v0_f32x4, scale_f32x4);</td></tr>
<tr><th id="2194">2194</th><td>    v1_f32x4 = vmulq_f32(v1_f32x4, scale_f32x4);</td></tr>
<tr><th id="2195">2195</th><td>    v2_f32x4 = vmulq_f32(v2_f32x4, scale_f32x4);</td></tr>
<tr><th id="2196">2196</th><td>    v3_f32x4 = vmulq_f32(v3_f32x4, scale_f32x4);</td></tr>
<tr><th id="2197">2197</th><td>    <i>// Store the results.</i></td></tr>
<tr><th id="2198">2198</th><td>    vst1q_f32(result + v, v0_f32x4);</td></tr>
<tr><th id="2199">2199</th><td>    vst1q_f32(result + v + <var>4</var>, v1_f32x4);</td></tr>
<tr><th id="2200">2200</th><td>    vst1q_f32(result + v + <var>8</var>, v2_f32x4);</td></tr>
<tr><th id="2201">2201</th><td>    vst1q_f32(result + v + <var>12</var>, v3_f32x4);</td></tr>
<tr><th id="2202">2202</th><td>  }</td></tr>
<tr><th id="2203">2203</th><td></td></tr>
<tr><th id="2204">2204</th><td>  <b>if</b> (TFLITE_UNLIKELY(v_size - postamble_start &gt;=</td></tr>
<tr><th id="2205">2205</th><td>                      (kInt8ValuesPerNeonVector &gt;&gt; <var>1</var>))) {</td></tr>
<tr><th id="2206">2206</th><td>    <i>// Load eight int8 values, if there is at least eight remaining.</i></td></tr>
<tr><th id="2207">2207</th><td>    <em>const</em> int8x8_t v_i8x8 = vld1_s8(vector + v);</td></tr>
<tr><th id="2208">2208</th><td>    <i>// Convert them to int16 first.</i></td></tr>
<tr><th id="2209">2209</th><td>    <em>const</em> int16x8_t v_i16x8 = vmovl_s8(v_i8x8);</td></tr>
<tr><th id="2210">2210</th><td>    <i>// Split it into two components.</i></td></tr>
<tr><th id="2211">2211</th><td>    <em>const</em> int16x4_t v0_i16x4 = vget_low_s16(v_i16x8);</td></tr>
<tr><th id="2212">2212</th><td>    <em>const</em> int16x4_t v1_i16x4 = vget_high_s16(v_i16x8);</td></tr>
<tr><th id="2213">2213</th><td>    <i>// Convert the components two floats.</i></td></tr>
<tr><th id="2214">2214</th><td>    float32x4_t v0_f32x4 = vcvtq_f32_s32(vmovl_s16(v0_i16x4));</td></tr>
<tr><th id="2215">2215</th><td>    float32x4_t v1_f32x4 = vcvtq_f32_s32(vmovl_s16(v1_i16x4));</td></tr>
<tr><th id="2216">2216</th><td>    <i>// Vector multiply four floats at a time.</i></td></tr>
<tr><th id="2217">2217</th><td>    v0_f32x4 = vmulq_f32(v0_f32x4, scale_f32x4);</td></tr>
<tr><th id="2218">2218</th><td>    v1_f32x4 = vmulq_f32(v1_f32x4, scale_f32x4);</td></tr>
<tr><th id="2219">2219</th><td>    <i>// Store the results.</i></td></tr>
<tr><th id="2220">2220</th><td>    vst1q_f32(result + v, v0_f32x4);</td></tr>
<tr><th id="2221">2221</th><td>    vst1q_f32(result + v + <var>4</var>, v1_f32x4);</td></tr>
<tr><th id="2222">2222</th><td>    v += (kInt8ValuesPerNeonVector &gt;&gt; <var>1</var>);</td></tr>
<tr><th id="2223">2223</th><td>  }</td></tr>
<tr><th id="2224">2224</th><td></td></tr>
<tr><th id="2225">2225</th><td>  <i>// Postamble loop.</i></td></tr>
<tr><th id="2226">2226</th><td>  <b>for</b> (; TFLITE_UNLIKELY(v &lt; v_size); v++) {</td></tr>
<tr><th id="2227">2227</th><td>    result[v] = scale * vector[v];</td></tr>
<tr><th id="2228">2228</th><td>  }</td></tr>
<tr><th id="2229">2229</th><td>}</td></tr>
<tr><th id="2230">2230</th><td></td></tr>
<tr><th id="2231">2231</th><td><i>// TODO(renjieliu): Avoid duplicating the logic.</i></td></tr>
<tr><th id="2232">2232</th><td><i>// Also consider changing the rounding stragey from "ties to away" to</i></td></tr>
<tr><th id="2233">2233</th><td><i>// "ties to even" since vcvtnq_s32_f32 is generally more available.</i></td></tr>
<tr><th id="2234">2234</th><td><b>inline</b> int32x4_t RoundToNearest(<em>const</em> float32x4_t input) {</td></tr>
<tr><th id="2235">2235</th><td><u>#if __ARM_ARCH &gt;= 8</u></td></tr>
<tr><th id="2236">2236</th><td>  <b>return</b> vcvtaq_s32_f32(input);</td></tr>
<tr><th id="2237">2237</th><td><u>#else</u></td></tr>
<tr><th id="2238">2238</th><td>  <em>static</em> <em>const</em> float32x4_t zero_val_dup = vdupq_n_f32(<var>0.0f</var>);</td></tr>
<tr><th id="2239">2239</th><td>  <em>static</em> <em>const</em> float32x4_t point5_val_dup = vdupq_n_f32(<var>0.5f</var>);</td></tr>
<tr><th id="2240">2240</th><td></td></tr>
<tr><th id="2241">2241</th><td>  <em>const</em> int32x4_t mask = vreinterpretq_s32_u32(vcltq_f32(input, zero_val_dup));</td></tr>
<tr><th id="2242">2242</th><td>  <em>const</em> float32x4_t casted_mask = vcvtq_f32_s32(mask);</td></tr>
<tr><th id="2243">2243</th><td>  <em>const</em> float32x4_t round = vaddq_f32(casted_mask, point5_val_dup);</td></tr>
<tr><th id="2244">2244</th><td>  <b>return</b> vcvtq_s32_f32(vaddq_f32(input, round));</td></tr>
<tr><th id="2245">2245</th><td><u>#endif</u></td></tr>
<tr><th id="2246">2246</th><td>}</td></tr>
<tr><th id="2247">2247</th><td></td></tr>
<tr><th id="2248">2248</th><td><i>// Note: this function caps minimum and maximum at zero, unlike the true</i></td></tr>
<tr><th id="2249">2249</th><td><i>// minmax_element. This is intentional.</i></td></tr>
<tr><th id="2250">2250</th><td><b>inline</b> <em>void</em> NeonMinMax(<em>const</em> <em>float</em>* values, <em>const</em> <em>int</em> size, <em>float</em>* min,</td></tr>
<tr><th id="2251">2251</th><td>                       <em>float</em>* max) {</td></tr>
<tr><th id="2252">2252</th><td>  <em>const</em> <em>int</em> postamble_start = RoundDownVectors&lt;kFloatValuesPerNeonVector&gt;(size);</td></tr>
<tr><th id="2253">2253</th><td>  <em>float</em> rmin = <var>0.0f</var>, rmax = <var>0.0f</var>;</td></tr>
<tr><th id="2254">2254</th><td>  <em>int</em> i = <var>0</var>;</td></tr>
<tr><th id="2255">2255</th><td>  <b>if</b> (postamble_start) {</td></tr>
<tr><th id="2256">2256</th><td>    float32x4_t min_f32x4 = vld1q_f32(values);</td></tr>
<tr><th id="2257">2257</th><td>    float32x4_t max_f32x4 = min_f32x4;</td></tr>
<tr><th id="2258">2258</th><td>    <b>for</b> (i = kFloatValuesPerNeonVector; i &lt; postamble_start;</td></tr>
<tr><th id="2259">2259</th><td>         i += kFloatValuesPerNeonVector) {</td></tr>
<tr><th id="2260">2260</th><td>      <em>const</em> float32x4_t value0_f32x4 = vld1q_f32(&amp;values[i]);</td></tr>
<tr><th id="2261">2261</th><td>      min_f32x4 = vminq_f32(min_f32x4, value0_f32x4);</td></tr>
<tr><th id="2262">2262</th><td>      max_f32x4 = vmaxq_f32(max_f32x4, value0_f32x4);</td></tr>
<tr><th id="2263">2263</th><td>    }</td></tr>
<tr><th id="2264">2264</th><td><u>#ifdef __aarch64__</u></td></tr>
<tr><th id="2265">2265</th><td>    rmin = std::min(rmin, vminvq_f32(min_f32x4));</td></tr>
<tr><th id="2266">2266</th><td>    rmax = std::max(rmax, vmaxvq_f32(max_f32x4));</td></tr>
<tr><th id="2267">2267</th><td><u>#else</u></td></tr>
<tr><th id="2268">2268</th><td>    float32x2_t min_f32x2 =</td></tr>
<tr><th id="2269">2269</th><td>        vmin_f32(vget_low_f32(min_f32x4), vget_high_f32(min_f32x4));</td></tr>
<tr><th id="2270">2270</th><td>    float32x2_t max_f32x2 =</td></tr>
<tr><th id="2271">2271</th><td>        vmax_f32(vget_low_f32(max_f32x4), vget_high_f32(max_f32x4));</td></tr>
<tr><th id="2272">2272</th><td>    min_f32x2 = vpmin_f32(min_f32x2, min_f32x2);</td></tr>
<tr><th id="2273">2273</th><td>    max_f32x2 = vpmax_f32(max_f32x2, max_f32x2);</td></tr>
<tr><th id="2274">2274</th><td>    rmin = std::min(rmin, vget_lane_f32(min_f32x2, <var>0</var>));</td></tr>
<tr><th id="2275">2275</th><td>    rmax = std::max(rmax, vget_lane_f32(max_f32x2, <var>0</var>));</td></tr>
<tr><th id="2276">2276</th><td><u>#endif  // __aarch64__</u></td></tr>
<tr><th id="2277">2277</th><td>  }</td></tr>
<tr><th id="2278">2278</th><td>  <b>if</b> (TFLITE_UNLIKELY(i &lt; size)) {</td></tr>
<tr><th id="2279">2279</th><td>    <em>const</em> <em>auto</em> minmax =</td></tr>
<tr><th id="2280">2280</th><td>        std::minmax_element(values + postamble_start, values + size);</td></tr>
<tr><th id="2281">2281</th><td>    rmin = std::min(rmin, *minmax.first);</td></tr>
<tr><th id="2282">2282</th><td>    rmax = std::max(rmax, *minmax.second);</td></tr>
<tr><th id="2283">2283</th><td>  }</td></tr>
<tr><th id="2284">2284</th><td>  *min = rmin;</td></tr>
<tr><th id="2285">2285</th><td>  *max = rmax;</td></tr>
<tr><th id="2286">2286</th><td>}</td></tr>
<tr><th id="2287">2287</th><td></td></tr>
<tr><th id="2288">2288</th><td><em>void</em> NeonSymmetricQuantizeFloats(<em>const</em> <em>float</em>* values, <em>const</em> <em>int</em> size,</td></tr>
<tr><th id="2289">2289</th><td>                                 int8_t* quantized_values, <em>float</em>* min,</td></tr>
<tr><th id="2290">2290</th><td>                                 <em>float</em>* max, <em>float</em>* scaling_factor) {</td></tr>
<tr><th id="2291">2291</th><td>  <i>// TODO(raziel): vectorize min/max calculation.</i></td></tr>
<tr><th id="2292">2292</th><td>  <em>auto</em> minmax = std::minmax_element(values, values + size);</td></tr>
<tr><th id="2293">2293</th><td>  *min = *minmax.first;</td></tr>
<tr><th id="2294">2294</th><td>  *max = *minmax.second;</td></tr>
<tr><th id="2295">2295</th><td>  NeonSymmetricQuantizeFloats(values, size, quantized_values, *min, *max,</td></tr>
<tr><th id="2296">2296</th><td>                              scaling_factor);</td></tr>
<tr><th id="2297">2297</th><td>}</td></tr>
<tr><th id="2298">2298</th><td></td></tr>
<tr><th id="2299">2299</th><td><em>void</em> NeonSymmetricQuantizeFloats(<em>const</em> <em>float</em>* values, <em>const</em> <em>int</em> size,</td></tr>
<tr><th id="2300">2300</th><td>                                 int8_t* quantized_values, <em>float</em> min, <em>float</em> max,</td></tr>
<tr><th id="2301">2301</th><td>                                 <em>float</em>* scaling_factor) {</td></tr>
<tr><th id="2302">2302</th><td>  <b>constexpr</b> <em>int</em> kScale = <var>127</var>;</td></tr>
<tr><th id="2303">2303</th><td>  <em>const</em> <em>float</em> range = std::max(std::abs(min), std::abs(max));</td></tr>
<tr><th id="2304">2304</th><td>  <b>if</b> (range == <var>0</var>) {</td></tr>
<tr><th id="2305">2305</th><td>    memset(quantized_values, <var>0</var>, size * <b>sizeof</b>(int8_t));</td></tr>
<tr><th id="2306">2306</th><td>    *scaling_factor = <var>1</var>;</td></tr>
<tr><th id="2307">2307</th><td>    <b>return</b>;</td></tr>
<tr><th id="2308">2308</th><td>  }</td></tr>
<tr><th id="2309">2309</th><td>  *scaling_factor = range / kScale;</td></tr>
<tr><th id="2310">2310</th><td>  <em>const</em> <em>float</em> scaling_factor_inv = kScale / range;</td></tr>
<tr><th id="2311">2311</th><td></td></tr>
<tr><th id="2312">2312</th><td>  <em>const</em> <em>int</em> postamble_start =</td></tr>
<tr><th id="2313">2313</th><td>      RoundDownVectors&lt;(<var>2</var> * kFloatValuesPerNeonVector)&gt;(size);</td></tr>
<tr><th id="2314">2314</th><td></td></tr>
<tr><th id="2315">2315</th><td>  <i>// Vectorized constants.</i></td></tr>
<tr><th id="2316">2316</th><td>  <em>const</em> float32x4_t q_factor_f32x4 = vmovq_n_f32(scaling_factor_inv);</td></tr>
<tr><th id="2317">2317</th><td>  <em>const</em> int32x4_t scale_i32x4 = vmovq_n_s32(kScale);</td></tr>
<tr><th id="2318">2318</th><td>  <em>const</em> int32x4_t neg_scale_i32x4 = vmovq_n_s32(-kScale);</td></tr>
<tr><th id="2319">2319</th><td></td></tr>
<tr><th id="2320">2320</th><td>  <em>int</em> i = <var>0</var>;</td></tr>
<tr><th id="2321">2321</th><td>  <b>for</b> (; i &lt; postamble_start; i += <var>2</var> * kFloatValuesPerNeonVector) {</td></tr>
<tr><th id="2322">2322</th><td>    <i>// Implements the vectorized version of the following:</i></td></tr>
<tr><th id="2323">2323</th><td><i>    // const int32 quantized_value = static_cast&lt;int32&gt;(</i></td></tr>
<tr><th id="2324">2324</th><td><i>    //    std::round(*scaling_factor * values[i]));</i></td></tr>
<tr><th id="2325">2325</th><td>    float32x4_t value0_f32x4 = vld1q_f32(&amp;values[i]);</td></tr>
<tr><th id="2326">2326</th><td>    float32x4_t value1_f32x4 =</td></tr>
<tr><th id="2327">2327</th><td>        vld1q_f32(&amp;values[i + kFloatValuesPerNeonVector]);</td></tr>
<tr><th id="2328">2328</th><td>    float32x4_t mul0_f32x4 = vmulq_f32(value0_f32x4, q_factor_f32x4);</td></tr>
<tr><th id="2329">2329</th><td>    float32x4_t mul1_f32x4 = vmulq_f32(value1_f32x4, q_factor_f32x4);</td></tr>
<tr><th id="2330">2330</th><td></td></tr>
<tr><th id="2331">2331</th><td>    <em>const</em> int32x4_t f2i0_i32x4 = RoundToNearest(mul0_f32x4);</td></tr>
<tr><th id="2332">2332</th><td>    <em>const</em> int32x4_t f2i1_i32x4 = RoundToNearest(mul1_f32x4);</td></tr>
<tr><th id="2333">2333</th><td></td></tr>
<tr><th id="2334">2334</th><td>    <i>// Implements the vectorized version of the following block:</i></td></tr>
<tr><th id="2335">2335</th><td><i>    //  quantized_values[i] = std::min(kScale, std::max(-kScale,</i></td></tr>
<tr><th id="2336">2336</th><td><i>    //  quantized_value));</i></td></tr>
<tr><th id="2337">2337</th><td>    int32x4_t max0_i32x4 = vmaxq_s32(f2i0_i32x4, neg_scale_i32x4);</td></tr>
<tr><th id="2338">2338</th><td>    int32x4_t max1_i32x4 = vmaxq_s32(f2i1_i32x4, neg_scale_i32x4);</td></tr>
<tr><th id="2339">2339</th><td>    int32x4_t min0_i32x4 = vminq_s32(max0_i32x4, scale_i32x4);</td></tr>
<tr><th id="2340">2340</th><td>    int32x4_t min1_i32x4 = vminq_s32(max1_i32x4, scale_i32x4);</td></tr>
<tr><th id="2341">2341</th><td></td></tr>
<tr><th id="2342">2342</th><td>    int16x4_t min0_16x4 = vmovn_s32(min0_i32x4);</td></tr>
<tr><th id="2343">2343</th><td>    int16x4_t min1_16x4 = vmovn_s32(min1_i32x4);</td></tr>
<tr><th id="2344">2344</th><td></td></tr>
<tr><th id="2345">2345</th><td>    int16x8_t min_16x8 = vcombine_s16(min0_16x4, min1_16x4);</td></tr>
<tr><th id="2346">2346</th><td>    int8x8_t min_s8x8 = vqmovn_s16(min_16x8);</td></tr>
<tr><th id="2347">2347</th><td>    vst1_s8(&amp;quantized_values[i], min_s8x8);</td></tr>
<tr><th id="2348">2348</th><td>  }</td></tr>
<tr><th id="2349">2349</th><td></td></tr>
<tr><th id="2350">2350</th><td>  <b>for</b> (; TFLITE_UNLIKELY(i &lt; size); ++i) {</td></tr>
<tr><th id="2351">2351</th><td>    <em>const</em> int32 quantized_value =</td></tr>
<tr><th id="2352">2352</th><td>        <b>static_cast</b>&lt;int32&gt;(TfLiteRound(scaling_factor_inv * values[i]));</td></tr>
<tr><th id="2353">2353</th><td>    quantized_values[i] = std::min(kScale, std::max(-kScale, quantized_value));</td></tr>
<tr><th id="2354">2354</th><td>  }</td></tr>
<tr><th id="2355">2355</th><td>}</td></tr>
<tr><th id="2356">2356</th><td></td></tr>
<tr><th id="2357">2357</th><td><em>void</em> NeonAsymmetricQuantizeFloats(<em>const</em> <em>float</em>* values, <em>const</em> <em>int</em> size,</td></tr>
<tr><th id="2358">2358</th><td>                                  int8_t* quantized_values,</td></tr>
<tr><th id="2359">2359</th><td>                                  <em>float</em>* scaling_factor, int32_t* offset) {</td></tr>
<tr><th id="2360">2360</th><td>  <em>float</em> rmin, rmax;</td></tr>
<tr><th id="2361">2361</th><td>  NeonMinMax(values, size, &amp;rmin, &amp;rmax);</td></tr>
<tr><th id="2362">2362</th><td></td></tr>
<tr><th id="2363">2363</th><td>  <em>const</em> int32_t kMinScale = -<var>128</var>;</td></tr>
<tr><th id="2364">2364</th><td>  <em>const</em> int32_t kMaxScale = <var>127</var>;</td></tr>
<tr><th id="2365">2365</th><td>  <em>const</em> <em>double</em> qmin_double = kMinScale;</td></tr>
<tr><th id="2366">2366</th><td>  <em>const</em> <em>double</em> qmax_double = kMaxScale;</td></tr>
<tr><th id="2367">2367</th><td>  <b>if</b> (rmin == rmax) {</td></tr>
<tr><th id="2368">2368</th><td>    memset(quantized_values, <var>0</var>, size * <b>sizeof</b>(int8_t));</td></tr>
<tr><th id="2369">2369</th><td>    *scaling_factor = <var>1</var>;</td></tr>
<tr><th id="2370">2370</th><td>    *offset = <var>0</var>;</td></tr>
<tr><th id="2371">2371</th><td>    <b>return</b>;</td></tr>
<tr><th id="2372">2372</th><td>  } <b>else</b> {</td></tr>
<tr><th id="2373">2373</th><td>    <em>const</em> <em>double</em> scale = (rmax - rmin) / (qmax_double - qmin_double);</td></tr>
<tr><th id="2374">2374</th><td>    <em>const</em> <em>double</em> zero_point_from_min = qmin_double - rmin / scale;</td></tr>
<tr><th id="2375">2375</th><td>    <em>const</em> <em>double</em> zero_point_from_max = qmax_double - rmax / scale;</td></tr>
<tr><th id="2376">2376</th><td>    <em>const</em> <em>double</em> zero_point_from_min_error =</td></tr>
<tr><th id="2377">2377</th><td>        std::abs(qmin_double) + std::abs(rmin / scale);</td></tr>
<tr><th id="2378">2378</th><td>    <em>const</em> <em>double</em> zero_point_from_max_error =</td></tr>
<tr><th id="2379">2379</th><td>        std::abs(qmax_double) + std::abs(rmax / scale);</td></tr>
<tr><th id="2380">2380</th><td>    <em>const</em> <em>double</em> zero_point_double =</td></tr>
<tr><th id="2381">2381</th><td>        zero_point_from_min_error &lt; zero_point_from_max_error</td></tr>
<tr><th id="2382">2382</th><td>            ? zero_point_from_min</td></tr>
<tr><th id="2383">2383</th><td>            : zero_point_from_max;</td></tr>
<tr><th id="2384">2384</th><td>    int8 nudged_zero_point = <var>0</var>;</td></tr>
<tr><th id="2385">2385</th><td>    <b>if</b> (zero_point_double &lt;= qmin_double) {</td></tr>
<tr><th id="2386">2386</th><td>      nudged_zero_point = kMinScale;</td></tr>
<tr><th id="2387">2387</th><td>    } <b>else</b> <b>if</b> (zero_point_double &gt;= qmax_double) {</td></tr>
<tr><th id="2388">2388</th><td>      nudged_zero_point = kMaxScale;</td></tr>
<tr><th id="2389">2389</th><td>    } <b>else</b> {</td></tr>
<tr><th id="2390">2390</th><td>      nudged_zero_point = <b>static_cast</b>&lt;int8&gt;(round(zero_point_double));</td></tr>
<tr><th id="2391">2391</th><td>    }</td></tr>
<tr><th id="2392">2392</th><td>    *scaling_factor = scale;</td></tr>
<tr><th id="2393">2393</th><td>    *offset = nudged_zero_point;</td></tr>
<tr><th id="2394">2394</th><td>  }</td></tr>
<tr><th id="2395">2395</th><td></td></tr>
<tr><th id="2396">2396</th><td>  <em>const</em> <em>int</em> postamble_start =</td></tr>
<tr><th id="2397">2397</th><td>      RoundDownVectors&lt;(<var>2</var> * kFloatValuesPerNeonVector)&gt;(size);</td></tr>
<tr><th id="2398">2398</th><td>  <em>const</em> <em>float</em> scaling_factor_inv =</td></tr>
<tr><th id="2399">2399</th><td>      *scaling_factor == <var>0</var> ? <var>0</var> : <var>1.0</var> / *scaling_factor;</td></tr>
<tr><th id="2400">2400</th><td>  <em>const</em> float32x4_t q_factor_f32x4 = vmovq_n_f32(scaling_factor_inv);</td></tr>
<tr><th id="2401">2401</th><td>  <em>const</em> int32x4_t scale_i32x4 = vmovq_n_s32(kMaxScale);</td></tr>
<tr><th id="2402">2402</th><td>  <em>const</em> int32x4_t neg_scale_i32x4 = vmovq_n_s32(kMinScale);</td></tr>
<tr><th id="2403">2403</th><td>  <em>const</em> int32x4_t offset_i32x4 = vmovq_n_s32(*offset);</td></tr>
<tr><th id="2404">2404</th><td></td></tr>
<tr><th id="2405">2405</th><td>  <em>int</em> i = <var>0</var>;</td></tr>
<tr><th id="2406">2406</th><td>  <b>for</b> (; i &lt; postamble_start; i += <var>2</var> * kFloatValuesPerNeonVector) {</td></tr>
<tr><th id="2407">2407</th><td>    float32x4_t value0_f32x4 = vld1q_f32(&amp;values[i]);</td></tr>
<tr><th id="2408">2408</th><td>    float32x4_t value1_f32x4 =</td></tr>
<tr><th id="2409">2409</th><td>        vld1q_f32(&amp;values[i + kFloatValuesPerNeonVector]);</td></tr>
<tr><th id="2410">2410</th><td>    float32x4_t mul0_f32x4 = vmulq_f32(value0_f32x4, q_factor_f32x4);</td></tr>
<tr><th id="2411">2411</th><td>    float32x4_t mul1_f32x4 = vmulq_f32(value1_f32x4, q_factor_f32x4);</td></tr>
<tr><th id="2412">2412</th><td></td></tr>
<tr><th id="2413">2413</th><td>    <em>const</em> int32x4_t f2i0_i32x4 = RoundToNearest(mul0_f32x4);</td></tr>
<tr><th id="2414">2414</th><td>    <em>const</em> int32x4_t f2i1_i32x4 = RoundToNearest(mul1_f32x4);</td></tr>
<tr><th id="2415">2415</th><td></td></tr>
<tr><th id="2416">2416</th><td>    <i>// Add offset</i></td></tr>
<tr><th id="2417">2417</th><td>    int32x4_t q0_i32x4 = vaddq_s32(f2i0_i32x4, offset_i32x4);</td></tr>
<tr><th id="2418">2418</th><td>    int32x4_t q1_i32x4 = vaddq_s32(f2i1_i32x4, offset_i32x4);</td></tr>
<tr><th id="2419">2419</th><td></td></tr>
<tr><th id="2420">2420</th><td>    int32x4_t max0_i32x4 = vmaxq_s32(q0_i32x4, neg_scale_i32x4);</td></tr>
<tr><th id="2421">2421</th><td>    int32x4_t max1_i32x4 = vmaxq_s32(q1_i32x4, neg_scale_i32x4);</td></tr>
<tr><th id="2422">2422</th><td>    int32x4_t min0_i32x4 = vminq_s32(max0_i32x4, scale_i32x4);</td></tr>
<tr><th id="2423">2423</th><td>    int32x4_t min1_i32x4 = vminq_s32(max1_i32x4, scale_i32x4);</td></tr>
<tr><th id="2424">2424</th><td></td></tr>
<tr><th id="2425">2425</th><td>    int16x4_t min0_16x4 = vmovn_s32(min0_i32x4);</td></tr>
<tr><th id="2426">2426</th><td>    int16x4_t min1_16x4 = vmovn_s32(min1_i32x4);</td></tr>
<tr><th id="2427">2427</th><td></td></tr>
<tr><th id="2428">2428</th><td>    int16x8_t min_16x8 = vcombine_s16(min0_16x4, min1_16x4);</td></tr>
<tr><th id="2429">2429</th><td>    int8x8_t min_s8x8 = vqmovn_s16(min_16x8);</td></tr>
<tr><th id="2430">2430</th><td>    vst1_s8(&amp;quantized_values[i], min_s8x8);</td></tr>
<tr><th id="2431">2431</th><td>  }</td></tr>
<tr><th id="2432">2432</th><td></td></tr>
<tr><th id="2433">2433</th><td>  <b>for</b> (; TFLITE_UNLIKELY(i &lt; size); ++i) {</td></tr>
<tr><th id="2434">2434</th><td>    <em>const</em> int32 quantized_value = <b>static_cast</b>&lt;int32&gt;(</td></tr>
<tr><th id="2435">2435</th><td>        *offset + TfLiteRound(scaling_factor_inv * values[i]));</td></tr>
<tr><th id="2436">2436</th><td>    quantized_values[i] =</td></tr>
<tr><th id="2437">2437</th><td>        std::min(kMaxScale, std::max(kMinScale, quantized_value));</td></tr>
<tr><th id="2438">2438</th><td>  }</td></tr>
<tr><th id="2439">2439</th><td>}</td></tr>
<tr><th id="2440">2440</th><td></td></tr>
<tr><th id="2441">2441</th><td><em>float</em> NeonVectorVectorDotProduct(<em>const</em> <em>float</em>* vector1, <em>const</em> <em>float</em>* vector2,</td></tr>
<tr><th id="2442">2442</th><td>                                 <em>int</em> v_size) {</td></tr>
<tr><th id="2443">2443</th><td>  <i>// If v_size is not divisible by the vector size, then we need to process the</i></td></tr>
<tr><th id="2444">2444</th><td><i>  // final few elements sequentially. postamble_start shows the start index</i></td></tr>
<tr><th id="2445">2445</th><td><i>  // where this should happen.</i></td></tr>
<tr><th id="2446">2446</th><td>  <em>const</em> <em>int</em> postamble_start =</td></tr>
<tr><th id="2447">2447</th><td>      RoundDownVectors&lt;kFloatValuesPerNeonVector&gt;(v_size);</td></tr>
<tr><th id="2448">2448</th><td>  float32x4_t acc_32x4 = vmovq_n_f32(<var>0.0</var>);</td></tr>
<tr><th id="2449">2449</th><td>  <em>int</em> v = <var>0</var>;</td></tr>
<tr><th id="2450">2450</th><td>  <b>for</b> (; v &lt; postamble_start; v += kFloatValuesPerNeonVector) {</td></tr>
<tr><th id="2451">2451</th><td>    <i>// Load 4 float values from vector1 and vector2 and accumulator.</i></td></tr>
<tr><th id="2452">2452</th><td>    float32x4_t v1_f32x4 = vld1q_f32(vector1 + v);</td></tr>
<tr><th id="2453">2453</th><td>    float32x4_t v2_f32x4 = vld1q_f32(vector2 + v);</td></tr>
<tr><th id="2454">2454</th><td>    <i>// Vector multiply-accumulate 4 float</i></td></tr>
<tr><th id="2455">2455</th><td>    acc_32x4 = vmlaq_f32(acc_32x4, v1_f32x4, v2_f32x4);</td></tr>
<tr><th id="2456">2456</th><td>  }</td></tr>
<tr><th id="2457">2457</th><td>  <em>float</em> result = AccumulateNeonLane(acc_32x4);</td></tr>
<tr><th id="2458">2458</th><td>  <i>// Postamble loop.</i></td></tr>
<tr><th id="2459">2459</th><td>  <b>for</b> (; TFLITE_UNLIKELY(v &lt; v_size); v++) {</td></tr>
<tr><th id="2460">2460</th><td>    result += vector1[v] * vector2[v];</td></tr>
<tr><th id="2461">2461</th><td>  }</td></tr>
<tr><th id="2462">2462</th><td>  <b>return</b> result;</td></tr>
<tr><th id="2463">2463</th><td>}</td></tr>
<tr><th id="2464">2464</th><td></td></tr>
<tr><th id="2465">2465</th><td><em>void</em> NeonReductionSumVector(<em>const</em> <em>float</em>* input_vector, <em>float</em>* output_vector,</td></tr>
<tr><th id="2466">2466</th><td>                            <em>int</em> output_size, <em>int</em> reduction_size) {</td></tr>
<tr><th id="2467">2467</th><td>  <b>for</b> (<em>int</em> o = <var>0</var>; o &lt; output_size; o++) {</td></tr>
<tr><th id="2468">2468</th><td>    <i>// If v_size is not divisible by the vector size, then we need to process</i></td></tr>
<tr><th id="2469">2469</th><td><i>    // the final few elements sequentially. postamble_start shows the start</i></td></tr>
<tr><th id="2470">2470</th><td><i>    // index where this should happen.</i></td></tr>
<tr><th id="2471">2471</th><td>    <em>const</em> <em>int</em> postamble_start =</td></tr>
<tr><th id="2472">2472</th><td>        RoundDownVectors&lt;kFloatValuesPerNeonVector&gt;(reduction_size);</td></tr>
<tr><th id="2473">2473</th><td>    float32x4_t sum_f32x4 = vmovq_n_f32(<var>0.0</var>);</td></tr>
<tr><th id="2474">2474</th><td>    <em>int</em> r = <var>0</var>;</td></tr>
<tr><th id="2475">2475</th><td>    <b>for</b> (; r &lt; postamble_start; r += kFloatValuesPerNeonVector) {</td></tr>
<tr><th id="2476">2476</th><td>      float32x4_t v1_f32x4 = vld1q_f32(input_vector + r);</td></tr>
<tr><th id="2477">2477</th><td>      sum_f32x4 = vaddq_f32(sum_f32x4, v1_f32x4);</td></tr>
<tr><th id="2478">2478</th><td>    }</td></tr>
<tr><th id="2479">2479</th><td>    <em>float</em> sum = AccumulateNeonLane(sum_f32x4);</td></tr>
<tr><th id="2480">2480</th><td>    <i>// Postamble loop.</i></td></tr>
<tr><th id="2481">2481</th><td>    <b>for</b> (; TFLITE_UNLIKELY(r &lt; reduction_size); r++) {</td></tr>
<tr><th id="2482">2482</th><td>      sum += input_vector[r];</td></tr>
<tr><th id="2483">2483</th><td>    }</td></tr>
<tr><th id="2484">2484</th><td>    output_vector[o] = sum;</td></tr>
<tr><th id="2485">2485</th><td>    input_vector += reduction_size;</td></tr>
<tr><th id="2486">2486</th><td>  }</td></tr>
<tr><th id="2487">2487</th><td>}</td></tr>
<tr><th id="2488">2488</th><td></td></tr>
<tr><th id="2489">2489</th><td><em>void</em> NeonReductionSumVector(<em>const</em> int8_t* input_vector, int32_t* output_vector,</td></tr>
<tr><th id="2490">2490</th><td>                            <em>const</em> <em>int</em> output_size, <em>const</em> <em>int</em> reduction_size) {</td></tr>
<tr><th id="2491">2491</th><td>  <em>const</em> <em>int</em> postamble_half_start =</td></tr>
<tr><th id="2492">2492</th><td>      RoundDownVectors&lt;kInt8ValuesPerNeonVector&gt;(reduction_size);</td></tr>
<tr><th id="2493">2493</th><td>  <em>const</em> <em>int</em> postamble_start =</td></tr>
<tr><th id="2494">2494</th><td>      RoundDownVectors&lt;(kInt8ValuesPerNeonVector / <var>2</var>)&gt;(reduction_size);</td></tr>
<tr><th id="2495">2495</th><td>  <b>for</b> (<em>int</em> o = <var>0</var>; o &lt; output_size; ++o) {</td></tr>
<tr><th id="2496">2496</th><td>    int32x4_t sum_32x4 = vmovq_n_s32(<var>0</var>);</td></tr>
<tr><th id="2497">2497</th><td>    <em>int</em> r = <var>0</var>;</td></tr>
<tr><th id="2498">2498</th><td>    <b>for</b> (; r &lt; postamble_half_start; r += kInt8ValuesPerNeonVector) {</td></tr>
<tr><th id="2499">2499</th><td>      <em>const</em> int8x16_t s2_8x16 = vld1q_s8(input_vector + r);</td></tr>
<tr><th id="2500">2500</th><td>      sum_32x4 = vpadalq_s16(sum_32x4, vpaddlq_s8(s2_8x16));</td></tr>
<tr><th id="2501">2501</th><td>    }</td></tr>
<tr><th id="2502">2502</th><td>    <b>if</b> (TFLITE_UNLIKELY(r &lt; postamble_start)) {</td></tr>
<tr><th id="2503">2503</th><td>      <em>const</em> int8x8_t s2_8x8 = vld1_s8(input_vector + r);</td></tr>
<tr><th id="2504">2504</th><td>      sum_32x4 = vpadalq_s16(sum_32x4, vmovl_s8(s2_8x8));</td></tr>
<tr><th id="2505">2505</th><td>      r += (kInt8ValuesPerNeonVector &gt;&gt; <var>1</var>);</td></tr>
<tr><th id="2506">2506</th><td>    }</td></tr>
<tr><th id="2507">2507</th><td>    int32_t sum = AccumulateNeonLane(sum_32x4);</td></tr>
<tr><th id="2508">2508</th><td>    <b>for</b> (; TFLITE_UNLIKELY(r &lt; reduction_size); ++r) {</td></tr>
<tr><th id="2509">2509</th><td>      sum += input_vector[r];</td></tr>
<tr><th id="2510">2510</th><td>    }</td></tr>
<tr><th id="2511">2511</th><td>    output_vector[o] = sum;</td></tr>
<tr><th id="2512">2512</th><td>    input_vector += reduction_size;</td></tr>
<tr><th id="2513">2513</th><td>  }</td></tr>
<tr><th id="2514">2514</th><td>}</td></tr>
<tr><th id="2515">2515</th><td></td></tr>
<tr><th id="2516">2516</th><td><em>void</em> NeonVectorBatchVectorCwiseProductAccumulate(</td></tr>
<tr><th id="2517">2517</th><td>    <em>const</em> int16_t* vector, <em>int</em> v_size, <em>const</em> int16_t* batch_vector, <em>int</em> n_batch,</td></tr>
<tr><th id="2518">2518</th><td>    int32_t multiplier, <em>int</em> shift, int16_t* result) {</td></tr>
<tr><th id="2519">2519</th><td>  int32x4_t min_value_vector = vdupq_n_s32(-<var>32768</var>);</td></tr>
<tr><th id="2520">2520</th><td>  int32x4_t max_value_vector = vdupq_n_s32(<var>32767</var>);</td></tr>
<tr><th id="2521">2521</th><td></td></tr>
<tr><th id="2522">2522</th><td>  <b>for</b> (<em>int</em> b = <var>0</var>; b &lt; n_batch; b++) {</td></tr>
<tr><th id="2523">2523</th><td>    <em>int</em> v = <var>0</var>;</td></tr>
<tr><th id="2524">2524</th><td>    <b>for</b> (; v &lt;= v_size - <var>16</var>; v += <var>16</var>) {</td></tr>
<tr><th id="2525">2525</th><td>      int32x4x4_t prod;</td></tr>
<tr><th id="2526">2526</th><td>      prod.val[<var>0</var>] = vmull_s16(vld1_s16(vector + v), vld1_s16(batch_vector));</td></tr>
<tr><th id="2527">2527</th><td>      prod.val[<var>1</var>] =</td></tr>
<tr><th id="2528">2528</th><td>          vmull_s16(vld1_s16(vector + v + <var>4</var>), vld1_s16(batch_vector + <var>4</var>));</td></tr>
<tr><th id="2529">2529</th><td>      prod.val[<var>2</var>] =</td></tr>
<tr><th id="2530">2530</th><td>          vmull_s16(vld1_s16(vector + v + <var>8</var>), vld1_s16(batch_vector + <var>8</var>));</td></tr>
<tr><th id="2531">2531</th><td>      prod.val[<var>3</var>] =</td></tr>
<tr><th id="2532">2532</th><td>          vmull_s16(vld1_s16(vector + v + <var>12</var>), vld1_s16(batch_vector + <var>12</var>));</td></tr>
<tr><th id="2533">2533</th><td>      batch_vector += <var>16</var>;</td></tr>
<tr><th id="2534">2534</th><td></td></tr>
<tr><th id="2535">2535</th><td>      prod = MultiplyByQuantizedMultiplier4Rows(prod, multiplier, shift);</td></tr>
<tr><th id="2536">2536</th><td></td></tr>
<tr><th id="2537">2537</th><td>      int16x4x4_t results;</td></tr>
<tr><th id="2538">2538</th><td>      results.val[<var>0</var>] = vld1_s16(result);</td></tr>
<tr><th id="2539">2539</th><td>      results.val[<var>1</var>] = vld1_s16(result + <var>4</var>);</td></tr>
<tr><th id="2540">2540</th><td>      results.val[<var>2</var>] = vld1_s16(result + <var>8</var>);</td></tr>
<tr><th id="2541">2541</th><td>      results.val[<var>3</var>] = vld1_s16(result + <var>12</var>);</td></tr>
<tr><th id="2542">2542</th><td></td></tr>
<tr><th id="2543">2543</th><td>      prod.val[<var>0</var>] = vaddq_s32(prod.val[<var>0</var>], vmovl_s16(results.val[<var>0</var>]));</td></tr>
<tr><th id="2544">2544</th><td>      prod.val[<var>1</var>] = vaddq_s32(prod.val[<var>1</var>], vmovl_s16(results.val[<var>1</var>]));</td></tr>
<tr><th id="2545">2545</th><td>      prod.val[<var>2</var>] = vaddq_s32(prod.val[<var>2</var>], vmovl_s16(results.val[<var>2</var>]));</td></tr>
<tr><th id="2546">2546</th><td>      prod.val[<var>3</var>] = vaddq_s32(prod.val[<var>3</var>], vmovl_s16(results.val[<var>3</var>]));</td></tr>
<tr><th id="2547">2547</th><td></td></tr>
<tr><th id="2548">2548</th><td>      prod.val[<var>0</var>] = vmaxq_s32(prod.val[<var>0</var>], min_value_vector);</td></tr>
<tr><th id="2549">2549</th><td>      prod.val[<var>1</var>] = vmaxq_s32(prod.val[<var>1</var>], min_value_vector);</td></tr>
<tr><th id="2550">2550</th><td>      prod.val[<var>2</var>] = vmaxq_s32(prod.val[<var>2</var>], min_value_vector);</td></tr>
<tr><th id="2551">2551</th><td>      prod.val[<var>3</var>] = vmaxq_s32(prod.val[<var>3</var>], min_value_vector);</td></tr>
<tr><th id="2552">2552</th><td></td></tr>
<tr><th id="2553">2553</th><td>      prod.val[<var>0</var>] = vminq_s32(prod.val[<var>0</var>], max_value_vector);</td></tr>
<tr><th id="2554">2554</th><td>      prod.val[<var>1</var>] = vminq_s32(prod.val[<var>1</var>], max_value_vector);</td></tr>
<tr><th id="2555">2555</th><td>      prod.val[<var>2</var>] = vminq_s32(prod.val[<var>2</var>], max_value_vector);</td></tr>
<tr><th id="2556">2556</th><td>      prod.val[<var>3</var>] = vminq_s32(prod.val[<var>3</var>], max_value_vector);</td></tr>
<tr><th id="2557">2557</th><td></td></tr>
<tr><th id="2558">2558</th><td>      vst1_s16(result, vmovn_s32(prod.val[<var>0</var>]));</td></tr>
<tr><th id="2559">2559</th><td>      vst1_s16(result + <var>4</var>, vmovn_s32(prod.val[<var>1</var>]));</td></tr>
<tr><th id="2560">2560</th><td>      vst1_s16(result + <var>8</var>, vmovn_s32(prod.val[<var>2</var>]));</td></tr>
<tr><th id="2561">2561</th><td>      vst1_s16(result + <var>12</var>, vmovn_s32(prod.val[<var>3</var>]));</td></tr>
<tr><th id="2562">2562</th><td></td></tr>
<tr><th id="2563">2563</th><td>      result += <var>16</var>;</td></tr>
<tr><th id="2564">2564</th><td>    }</td></tr>
<tr><th id="2565">2565</th><td></td></tr>
<tr><th id="2566">2566</th><td>    <b>for</b> (; TFLITE_UNLIKELY(v &lt; v_size); v++) {</td></tr>
<tr><th id="2567">2567</th><td>      int32_t prod = vector[v] * *batch_vector++;</td></tr>
<tr><th id="2568">2568</th><td>      prod = MultiplyByQuantizedMultiplier(prod, multiplier, shift);</td></tr>
<tr><th id="2569">2569</th><td>      int32_t output = prod + *result;</td></tr>
<tr><th id="2570">2570</th><td>      output = std::max(std::min(<var>32767</var>, output), -<var>32768</var>);</td></tr>
<tr><th id="2571">2571</th><td>      *result++ = output;</td></tr>
<tr><th id="2572">2572</th><td>    }</td></tr>
<tr><th id="2573">2573</th><td>  }</td></tr>
<tr><th id="2574">2574</th><td>}</td></tr>
<tr><th id="2575">2575</th><td></td></tr>
<tr><th id="2576">2576</th><td><em>void</em> NeonMeanStddevNormalization(<em>const</em> <em>float</em>* <b>__restrict__</b> input_vector,</td></tr>
<tr><th id="2577">2577</th><td>                                 <em>float</em>* <b>__restrict__</b> output_vector, <em>int</em> v_size,</td></tr>
<tr><th id="2578">2578</th><td>                                 <em>int</em> n_batch) {</td></tr>
<tr><th id="2579">2579</th><td>  <b>constexpr</b> <em>int</em> kBlockSize = kFloatValuesPerNeonVector * <var>4</var>;</td></tr>
<tr><th id="2580">2580</th><td></td></tr>
<tr><th id="2581">2581</th><td>  <b>for</b> (<em>int</em> batch = <var>0</var>; batch &lt; n_batch; ++batch) {</td></tr>
<tr><th id="2582">2582</th><td>    <i>// Calculate sum</i></td></tr>
<tr><th id="2583">2583</th><td>    float32x4_t sum_f32x4_0 = vdupq_n_f32(<var>0.0f</var>);</td></tr>
<tr><th id="2584">2584</th><td>    float32x4_t sum_f32x4_1 = vdupq_n_f32(<var>0.0f</var>);</td></tr>
<tr><th id="2585">2585</th><td>    float32x4_t sum_f32x4_2 = vdupq_n_f32(<var>0.0f</var>);</td></tr>
<tr><th id="2586">2586</th><td>    float32x4_t sum_f32x4_3 = vdupq_n_f32(<var>0.0f</var>);</td></tr>
<tr><th id="2587">2587</th><td>    <em>int</em> i = <var>0</var>;</td></tr>
<tr><th id="2588">2588</th><td>    <b>for</b> (; i &lt;= v_size - kBlockSize; i += kBlockSize) {</td></tr>
<tr><th id="2589">2589</th><td>      <em>const</em> float32x4_t input_f32x4_0 =</td></tr>
<tr><th id="2590">2590</th><td>          vld1q_f32(input_vector + i + <var>0</var> * kFloatValuesPerNeonVector);</td></tr>
<tr><th id="2591">2591</th><td>      <em>const</em> float32x4_t input_f32x4_1 =</td></tr>
<tr><th id="2592">2592</th><td>          vld1q_f32(input_vector + i + <var>1</var> * kFloatValuesPerNeonVector);</td></tr>
<tr><th id="2593">2593</th><td>      <em>const</em> float32x4_t input_f32x4_2 =</td></tr>
<tr><th id="2594">2594</th><td>          vld1q_f32(input_vector + i + <var>2</var> * kFloatValuesPerNeonVector);</td></tr>
<tr><th id="2595">2595</th><td>      <em>const</em> float32x4_t input_f32x4_3 =</td></tr>
<tr><th id="2596">2596</th><td>          vld1q_f32(input_vector + i + <var>3</var> * kFloatValuesPerNeonVector);</td></tr>
<tr><th id="2597">2597</th><td>      sum_f32x4_0 = vaddq_f32(sum_f32x4_0, input_f32x4_0);</td></tr>
<tr><th id="2598">2598</th><td>      sum_f32x4_1 = vaddq_f32(sum_f32x4_1, input_f32x4_1);</td></tr>
<tr><th id="2599">2599</th><td>      sum_f32x4_2 = vaddq_f32(sum_f32x4_2, input_f32x4_2);</td></tr>
<tr><th id="2600">2600</th><td>      sum_f32x4_3 = vaddq_f32(sum_f32x4_3, input_f32x4_3);</td></tr>
<tr><th id="2601">2601</th><td>    }</td></tr>
<tr><th id="2602">2602</th><td>    sum_f32x4_0 = vaddq_f32(sum_f32x4_0, sum_f32x4_2);</td></tr>
<tr><th id="2603">2603</th><td>    sum_f32x4_1 = vaddq_f32(sum_f32x4_1, sum_f32x4_3);</td></tr>
<tr><th id="2604">2604</th><td>    sum_f32x4_0 = vaddq_f32(sum_f32x4_0, sum_f32x4_1);</td></tr>
<tr><th id="2605">2605</th><td>    <em>float</em> sum = AccumulateNeonLane(sum_f32x4_0);</td></tr>
<tr><th id="2606">2606</th><td>    <b>for</b> (; TFLITE_UNLIKELY(i &lt; v_size); ++i) {</td></tr>
<tr><th id="2607">2607</th><td>      sum += input_vector[i];</td></tr>
<tr><th id="2608">2608</th><td>    }</td></tr>
<tr><th id="2609">2609</th><td>    <i>// Calculate mean</i></td></tr>
<tr><th id="2610">2610</th><td>    <em>const</em> <em>float</em> mean = sum / v_size;</td></tr>
<tr><th id="2611">2611</th><td>    <em>const</em> float32x4_t mean_f32x4 = vdupq_n_f32(mean);</td></tr>
<tr><th id="2612">2612</th><td>    <i>// Calculate sum of squared differences</i></td></tr>
<tr><th id="2613">2613</th><td>    float32x4_t sum_diff_sq_f32x4_0 = vdupq_n_f32(<var>0.0f</var>);</td></tr>
<tr><th id="2614">2614</th><td>    float32x4_t sum_diff_sq_f32x4_1 = vdupq_n_f32(<var>0.0f</var>);</td></tr>
<tr><th id="2615">2615</th><td>    float32x4_t sum_diff_sq_f32x4_2 = vdupq_n_f32(<var>0.0f</var>);</td></tr>
<tr><th id="2616">2616</th><td>    float32x4_t sum_diff_sq_f32x4_3 = vdupq_n_f32(<var>0.0f</var>);</td></tr>
<tr><th id="2617">2617</th><td>    i = <var>0</var>;</td></tr>
<tr><th id="2618">2618</th><td>    <b>for</b> (; i &lt;= v_size - kBlockSize; i += kBlockSize) {</td></tr>
<tr><th id="2619">2619</th><td>      <em>const</em> float32x4_t input_f32x4_0 =</td></tr>
<tr><th id="2620">2620</th><td>          vld1q_f32(input_vector + i + <var>0</var> * kFloatValuesPerNeonVector);</td></tr>
<tr><th id="2621">2621</th><td>      <em>const</em> float32x4_t input_f32x4_1 =</td></tr>
<tr><th id="2622">2622</th><td>          vld1q_f32(input_vector + i + <var>1</var> * kFloatValuesPerNeonVector);</td></tr>
<tr><th id="2623">2623</th><td>      <em>const</em> float32x4_t input_f32x4_2 =</td></tr>
<tr><th id="2624">2624</th><td>          vld1q_f32(input_vector + i + <var>2</var> * kFloatValuesPerNeonVector);</td></tr>
<tr><th id="2625">2625</th><td>      <em>const</em> float32x4_t input_f32x4_3 =</td></tr>
<tr><th id="2626">2626</th><td>          vld1q_f32(input_vector + i + <var>3</var> * kFloatValuesPerNeonVector);</td></tr>
<tr><th id="2627">2627</th><td>      <em>const</em> float32x4_t diff_f32x4_0 = vsubq_f32(input_f32x4_0, mean_f32x4);</td></tr>
<tr><th id="2628">2628</th><td>      <em>const</em> float32x4_t diff_f32x4_1 = vsubq_f32(input_f32x4_1, mean_f32x4);</td></tr>
<tr><th id="2629">2629</th><td>      <em>const</em> float32x4_t diff_f32x4_2 = vsubq_f32(input_f32x4_2, mean_f32x4);</td></tr>
<tr><th id="2630">2630</th><td>      <em>const</em> float32x4_t diff_f32x4_3 = vsubq_f32(input_f32x4_3, mean_f32x4);</td></tr>
<tr><th id="2631">2631</th><td>      sum_diff_sq_f32x4_0 =</td></tr>
<tr><th id="2632">2632</th><td>          vmlaq_f32(sum_diff_sq_f32x4_0, diff_f32x4_0, diff_f32x4_0);</td></tr>
<tr><th id="2633">2633</th><td>      sum_diff_sq_f32x4_1 =</td></tr>
<tr><th id="2634">2634</th><td>          vmlaq_f32(sum_diff_sq_f32x4_1, diff_f32x4_1, diff_f32x4_1);</td></tr>
<tr><th id="2635">2635</th><td>      sum_diff_sq_f32x4_2 =</td></tr>
<tr><th id="2636">2636</th><td>          vmlaq_f32(sum_diff_sq_f32x4_2, diff_f32x4_2, diff_f32x4_2);</td></tr>
<tr><th id="2637">2637</th><td>      sum_diff_sq_f32x4_3 =</td></tr>
<tr><th id="2638">2638</th><td>          vmlaq_f32(sum_diff_sq_f32x4_3, diff_f32x4_3, diff_f32x4_3);</td></tr>
<tr><th id="2639">2639</th><td>    }</td></tr>
<tr><th id="2640">2640</th><td>    sum_diff_sq_f32x4_0 = vaddq_f32(sum_diff_sq_f32x4_0, sum_diff_sq_f32x4_2);</td></tr>
<tr><th id="2641">2641</th><td>    sum_diff_sq_f32x4_1 = vaddq_f32(sum_diff_sq_f32x4_1, sum_diff_sq_f32x4_3);</td></tr>
<tr><th id="2642">2642</th><td>    sum_diff_sq_f32x4_0 = vaddq_f32(sum_diff_sq_f32x4_0, sum_diff_sq_f32x4_1);</td></tr>
<tr><th id="2643">2643</th><td>    <em>float</em> sum_diff_sq = AccumulateNeonLane(sum_diff_sq_f32x4_0);</td></tr>
<tr><th id="2644">2644</th><td>    <b>for</b> (; TFLITE_UNLIKELY(i &lt; v_size); ++i) {</td></tr>
<tr><th id="2645">2645</th><td>      <em>const</em> <em>float</em> diff = input_vector[i] - mean;</td></tr>
<tr><th id="2646">2646</th><td>      sum_diff_sq += diff * diff;</td></tr>
<tr><th id="2647">2647</th><td>    }</td></tr>
<tr><th id="2648">2648</th><td>    <i>// Calculate 1/stddev</i></td></tr>
<tr><th id="2649">2649</th><td>    <em>const</em> <em>float</em> variance = sum_diff_sq / v_size;</td></tr>
<tr><th id="2650">2650</th><td>    <b>constexpr</b> <em>float</em> kNormalizationConstant = <var>1e-8f</var>;</td></tr>
<tr><th id="2651">2651</th><td>    <em>const</em> <em>float</em> stddev_inv =</td></tr>
<tr><th id="2652">2652</th><td>        <var>1.0f</var> / std::sqrt(variance + kNormalizationConstant);</td></tr>
<tr><th id="2653">2653</th><td>    <i>// Do the normalization</i></td></tr>
<tr><th id="2654">2654</th><td>    i = <var>0</var>;</td></tr>
<tr><th id="2655">2655</th><td>    <b>for</b> (; i &lt;= v_size - kBlockSize; i += kBlockSize) {</td></tr>
<tr><th id="2656">2656</th><td>      <em>const</em> float32x4_t input_f32x4_0 =</td></tr>
<tr><th id="2657">2657</th><td>          vld1q_f32(input_vector + i + <var>0</var> * kFloatValuesPerNeonVector);</td></tr>
<tr><th id="2658">2658</th><td>      <em>const</em> float32x4_t input_f32x4_1 =</td></tr>
<tr><th id="2659">2659</th><td>          vld1q_f32(input_vector + i + <var>1</var> * kFloatValuesPerNeonVector);</td></tr>
<tr><th id="2660">2660</th><td>      <em>const</em> float32x4_t input_f32x4_2 =</td></tr>
<tr><th id="2661">2661</th><td>          vld1q_f32(input_vector + i + <var>2</var> * kFloatValuesPerNeonVector);</td></tr>
<tr><th id="2662">2662</th><td>      <em>const</em> float32x4_t input_f32x4_3 =</td></tr>
<tr><th id="2663">2663</th><td>          vld1q_f32(input_vector + i + <var>3</var> * kFloatValuesPerNeonVector);</td></tr>
<tr><th id="2664">2664</th><td>      <em>const</em> float32x4_t tmp_0 = vsubq_f32(input_f32x4_0, mean_f32x4);</td></tr>
<tr><th id="2665">2665</th><td>      <em>const</em> float32x4_t tmp_1 = vsubq_f32(input_f32x4_1, mean_f32x4);</td></tr>
<tr><th id="2666">2666</th><td>      <em>const</em> float32x4_t tmp_2 = vsubq_f32(input_f32x4_2, mean_f32x4);</td></tr>
<tr><th id="2667">2667</th><td>      <em>const</em> float32x4_t tmp_3 = vsubq_f32(input_f32x4_3, mean_f32x4);</td></tr>
<tr><th id="2668">2668</th><td>      <em>const</em> float32x4_t output_f32x4_0 = vmulq_n_f32(tmp_0, stddev_inv);</td></tr>
<tr><th id="2669">2669</th><td>      <em>const</em> float32x4_t output_f32x4_1 = vmulq_n_f32(tmp_1, stddev_inv);</td></tr>
<tr><th id="2670">2670</th><td>      <em>const</em> float32x4_t output_f32x4_2 = vmulq_n_f32(tmp_2, stddev_inv);</td></tr>
<tr><th id="2671">2671</th><td>      <em>const</em> float32x4_t output_f32x4_3 = vmulq_n_f32(tmp_3, stddev_inv);</td></tr>
<tr><th id="2672">2672</th><td>      vst1q_f32(output_vector + i + <var>0</var> * kFloatValuesPerNeonVector,</td></tr>
<tr><th id="2673">2673</th><td>                output_f32x4_0);</td></tr>
<tr><th id="2674">2674</th><td>      vst1q_f32(output_vector + i + <var>1</var> * kFloatValuesPerNeonVector,</td></tr>
<tr><th id="2675">2675</th><td>                output_f32x4_1);</td></tr>
<tr><th id="2676">2676</th><td>      vst1q_f32(output_vector + i + <var>2</var> * kFloatValuesPerNeonVector,</td></tr>
<tr><th id="2677">2677</th><td>                output_f32x4_2);</td></tr>
<tr><th id="2678">2678</th><td>      vst1q_f32(output_vector + i + <var>3</var> * kFloatValuesPerNeonVector,</td></tr>
<tr><th id="2679">2679</th><td>                output_f32x4_3);</td></tr>
<tr><th id="2680">2680</th><td>    }</td></tr>
<tr><th id="2681">2681</th><td>    <b>for</b> (; TFLITE_UNLIKELY(i &lt; v_size); ++i) {</td></tr>
<tr><th id="2682">2682</th><td>      output_vector[i] = (input_vector[i] - mean) * stddev_inv;</td></tr>
<tr><th id="2683">2683</th><td>    }</td></tr>
<tr><th id="2684">2684</th><td>    <i>// Advance to next batch</i></td></tr>
<tr><th id="2685">2685</th><td>    input_vector += v_size;</td></tr>
<tr><th id="2686">2686</th><td>    output_vector += v_size;</td></tr>
<tr><th id="2687">2687</th><td>  }</td></tr>
<tr><th id="2688">2688</th><td>}</td></tr>
<tr><th id="2689">2689</th><td></td></tr>
<tr><th id="2690">2690</th><td>}  <i>// namespace tensor_utils</i></td></tr>
<tr><th id="2691">2691</th><td>}  <i>// namespace tflite</i></td></tr>
<tr><th id="2692">2692</th><td></td></tr>
<tr><th id="2693">2693</th><td><u>#<span data-ppcond="36">endif</span>  // USE_NEON</u></td></tr>
<tr><th id="2694">2694</th><td></td></tr>
</table><hr/><p id='footer'>
Generated on <em>2021-Aug-05</em> from project halide revision <em>v12.0.1</em>